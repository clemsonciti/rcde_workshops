

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Training Techniques &#8212; Research Computing and Data Workshop</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'pytorch_advanced/05-training_techniques';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Containerization on Palmetto (under development)" href="../containers/00-index.html" />
    <link rel="prev" title="Pytorch Lightning" href="04-pytorch_lightning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Research Computing and Data Workshops
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introductory Sequence</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../intro_linux/00-index.html">Introduction to Linux</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/00a-outline.html">Workshop Outline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/01-introduction.html">What is Linux?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/01a-shell.html">Shell Specifics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/02-accessing-palmetto.html">Accessing the Palmetto Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/03-file-system.html">Navigating Files and Directories</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/04-working_with_files_and_directories.html">Working With Files and Directories</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/04a-file-permissions.html">File Permissions and Atrributes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/05-pipes.html">Pipes and Redirection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/06-find.html">Finding Things</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/07-utilities.html">Utilities and Useful Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_linux/08-conclusion.html">Workshop Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../intro_palmetto/00-index.html">Introduction to Palmetto</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro_palmetto/01-introduction.html">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_palmetto/02-accessing-palmetto.html">Accessing the Palmetto Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_palmetto/03-palmetto_structure.html">The structure of the Palmetto Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_palmetto/04-storage.html">Storage on Palmetto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_palmetto/05-interactive.html">Running an interactive job on Palmetto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_palmetto/06-file-transfer.html">Transferring files to and from Palmetto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_palmetto/07-open-od.html">Web-based access to the Palmetto Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_palmetto/08-batch.html">Running a batch job</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">R</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../r_programming/00-index.html">Introduction to R</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/01-Introduction.html">Introduction to R</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/02-Basic-R.html">Basics of R</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/03-Data-Structures.html">Data Structures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/04-Matrix.html">Vectors, Matrices, Lists and Data Frames</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/05-Control-Structure.html">Control Structure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/06-Functions.html">Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/07-Parallel-Computing.html">Parallel Computing in R</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/08-Basic-Plotting.html">Basic plotting with R</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/09-Plotting-with-ggplot.html">Ploting with ggplot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_programming/10-R-in-Palmetto.html">R in Palmetto</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../r_machine_learning/00-index.html">Machine Learning using R</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/01-Introduction.html">Introduction to Machine Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/02-Caret-Preprocessing.html">Introduction to Caret</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/03-Caret-Data-Partition.html">Data Partition with caret</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/04-Caret-Evaluation-Metrics.html">Evaluation Metrics with caret</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/05-Training_Regression.html">Training Machine Learning model using Regression Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/06-Decision_boundaries.html">Classification with decision boundaries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/07-KNN.html">Nearest Neighbours Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/08-Training_Tree.html">Training Machine Learning model using Tree-based model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/09-Training_Ensemble.html">Training Machine Learning model using Ensemble approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/10-Unsupervised-Learning.html">Unsupervised Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../r_machine_learning/11-Neural-Network.html">Neural Network</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Python</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../python_programming/00-index.html">Introduction to Python Programming</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_programming/01-IntroToPython-I.html">Introduction to Python I</a></li>







<li class="toctree-l2"><a class="reference internal" href="../python_programming/02-IntroToPython-II.html">Introduction to Python II</a></li>

<li class="toctree-l2"><a class="reference internal" href="../python_programming/03-IntroToPython-III.html">Introduction to Python III</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_programming/04-IntroToPython-IV.html">Introduction to Python IV</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../python_sklearn/00-index.html">Machine Learning using Python</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_sklearn/01-TLDR.html">TL;DR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_sklearn/02-preparing-data.html">Preparing data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_sklearn/03-partitioning-data.html">Data partition: training and testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_sklearn/04-regression-example.html">Model Pipelines in Sklearn</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../python_deep_learning/00-index.html">Deep Learning in Python</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_deep_learning/01-Introduction.html">Introduction to Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_deep_learning/02-Deep-Learning-Framework.html">Deep Learning Library Framework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_deep_learning/03-Neural-Network.html">Recap on ANN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_deep_learning/04-Intro-to-Keras.html">Introduction to Keras</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_deep_learning/05-Keras-Regression.html">Training Deep Learning Regression model with Keras</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_deep_learning/06-Keras-Classification.html">Training Deep Learning Classification model with Keras</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_deep_learning/07-Convolution-Neural-Network.html">Convolution Neural Network for image classification</a></li>

<li class="toctree-l2"><a class="reference internal" href="../python_deep_learning/08-Recurrent-neural-networks.html">Recurrent Neural Network for Timeseries forecasting</a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../python_big_data/00-index.html">Big Data Analytics in Python</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../python_big_data/01-introduction.html">Introduction to Apache Spark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_big_data/02-cluster.html">Launching the Spark cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python_big_data/03-notebooks.html">1. Where are the notebooks</a></li>


</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/00-index.html">Deep Learning in Pytorch</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/00-quickstart.html">PyTorch Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/01-pytorch_basics.html">PyTorch Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/02-pytorch_gpu_support.html">Pytorch GPU support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/03-regression_and_classification.html">Regression and Classification with Fully Connected Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/04-high-dimensional-data.html">High Dimensional Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/05-datasets.html">Datasets and data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/06-modules.html">Building the network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/07-cnn_emnist.html">Computer Vision and Convolutional Neural Networks</a></li>







</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="00-index.html">Advanced Deep Learning in Pytorch</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01-emnist_baseline.html">EMNIST Baseline</a></li>
<li class="toctree-l2"><a class="reference internal" href="02-import_custom_scripts.html">Move reused code into python script files</a></li>
<li class="toctree-l2"><a class="reference internal" href="03-finetune_pretrained_models.html">Model fine-tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="04-pytorch_lightning.html">Pytorch Lightning</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Training Techniques</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Palmetto Usage</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../containers/00-index.html">Containerization on Palmetto (under development)</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../containers/01-introduction.html">Introduction to CloudLab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../containers/02-dockers.html">Docker Containers on CloudLab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../containers/03-apptainers.html">Singularity/Apptainers on Palmetto</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../advanced_scheduling/00-index.html">Advanced Scheduling (under development)</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced_scheduling/01-introduction.html">1. What is Spark?</a></li>







</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Development Life Cycle</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../intro_git_gitlab/00-index.html">Introduction to Version Control with Git and GitLab</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro_git_gitlab/01-version-control.html">Version Control Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_git_gitlab/02-git-workflow.html">Git Version Control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_git_gitlab/03-git-commands.html">Git Commands</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_git_gitlab/04-install-git.html">Installing Git</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_git_gitlab/05-example.html">Practice With a Local Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_git_gitlab/06-gitlab.html">GitLab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_git_gitlab/07-collaboration-conflicts.html">Collaboration and Conflicts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro_git_gitlab/09-more-resources.html">More Resources</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/pytorch_advanced/05-training_techniques.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Training Techniques</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#settings">Settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#emnist-dataset">EMNIST Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baseline">Baseline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-profiling">Model profiling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logging-with-weights-and-biases">Logging with Weights and Biases</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-mixed-precision-amp">Automatic mixed precision (AMP)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#amp-in-pytorch-ligtning">AMP in Pytorch Ligtning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-checkpointing">Model checkpointing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-model-from-a-local-checkpoint">Creating the model from a local checkpoint</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">Early Stopping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu">Multi-gpu</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-node-multi-gpu">Single-node, multi-gpu</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-multi-gpu">Multi-node, multi-gpu</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="training-techniques">
<h1>Training Techniques<a class="headerlink" href="#training-techniques" title="Permalink to this heading">#</a></h1>
<p>The <a class="reference external" href="https://www.pytorchlightning.ai/index.html">Pytorch Ligtning</a> <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class implements many advanced features to improve training speed, convergence, reproducibility, etc. In this notebook, we apply a number of these features to our EMNIST dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">ResNet18_Weights</span>
</pre></div>
</div>
</div>
</div>
<section id="settings">
<h2>Settings<a class="headerlink" href="#settings" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;/scratch/</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;USER&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">/data&quot;</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;/scratch/</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;USER&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">/model.pt&quot;</span>

<span class="c1"># Model and Training</span>
<span class="n">epochs</span><span class="o">=</span><span class="mi">2</span> <span class="c1"># number of training epochs</span>
<span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span> <span class="c1">#input batch size for training (default: 64)</span>
<span class="n">test_batch_size</span><span class="o">=</span><span class="mi">1000</span> <span class="c1">#input batch size for testing (default: 1000)</span>
<span class="n">num_workers</span><span class="o">=</span><span class="mi">10</span> <span class="c1"># parallel data loading to speed things up</span>
<span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span> <span class="c1">#learning rate (default: 0.1)</span>
<span class="n">gamma</span><span class="o">=</span><span class="mf">0.7</span> <span class="c1">#Learning rate step gamma (default: 0.7)</span>
<span class="n">seed</span><span class="o">=</span><span class="mi">42</span> <span class="c1">#random seed (default: 42)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="emnist-dataset">
<h2>EMNIST Dataset<a class="headerlink" href="#emnist-dataset" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">data</span>

<span class="c1"># transforms (we may wish to experiment with these so leave as inputs)</span>
<span class="n">train_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
<span class="p">])</span>
<span class="n">test_transforms</span> <span class="o">=</span> <span class="n">train_transforms</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_train_dataloader</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">train_transforms</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get_test_dataloader</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">test_transforms</span><span class="p">,</span> <span class="n">test_batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="p">)</span>

<span class="c1"># save a test batch for later testing</span>
<span class="n">image_gen</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
<span class="n">test_img</span><span class="p">,</span> <span class="n">test_trg</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">image_gen</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training dataset:&quot;</span><span class="p">,</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing dataset:&quot;</span><span class="p">,</span> <span class="n">test_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training dataset: Dataset EMNIST
    Number of datapoints: 112800
    Root location: /scratch/dane2/data
    Split: Train
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.1307,), std=(0.3081,))
           )
Testing dataset: Dataset EMNIST
    Number of datapoints: 18800
    Root location: /scratch/dane2/data
    Split: Test
    StandardTransform
Transform: Compose(
               ToTensor()
               Normalize(mean=(0.1307,), std=(0.3081,))
           )
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># test batch</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="baseline">
<h2>Baseline<a class="headerlink" href="#baseline" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># init the classifier</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Classifier</span><span class="p">()</span> <span class="c1">#models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)</span>

<span class="c1"># init the lazy layers</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="c1"># create the ligtning model</span>
<span class="c1"># Note: since the last notebook, we moved the LitModel logic into utils.models</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LitModel</span><span class="p">(</span><span class="n">pt_model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightning.pytorch</span> <span class="kn">import</span> <span class="n">loggers</span> <span class="k">as</span> <span class="n">pl_loggers</span>
<span class="kn">from</span> <span class="nn">lightning.pytorch</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="c1"># a logger to save results</span>
<span class="n">csv_logger</span> <span class="o">=</span> <span class="n">pl_loggers</span><span class="o">.</span><span class="n">CSVLogger</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="s2">&quot;logs/&quot;</span><span class="p">)</span>

<span class="c1"># the trainer class has about a million arguments. For now, the defaults will suffice.</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">csv_logger</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type               | Params
-------------------------------------------------
0 | model     | Classifier         | 23.1 K
1 | train_acc | MulticlassAccuracy | 0     
2 | test_acc  | MulticlassAccuracy | 0     
-------------------------------------------------
23.1 K    Trainable params
0         Non-trainable params
23.1 K    Total params
0.093     Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "73b4f7ad1d494bdfbd46f8434e2a4588", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`Trainer.fit` stopped: `max_epochs=2` reached.
</pre></div>
</div>
</div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">logs/lightning_logs</span></code> for results.</p>
</section>
<section id="model-profiling">
<h2>Model profiling<a class="headerlink" href="#model-profiling" title="Permalink to this heading">#</a></h2>
<p>Profiling tools show you how much time each part of your training code is taking. This can help you identify areas where your program should be optimized in order to speed things up.</p>
<p><a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">Pytorch has a built in profiler</a>. We can easily turn this on in Lightning by setting the <code class="docutils literal notranslate"><span class="pre">profiler</span></code> argument in the trainer. Note that the Pytorch profiler forces synchronous cuda execution. That makes things take longer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># init the classifier</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Classifier</span><span class="p">()</span> <span class="c1">#models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)</span>

<span class="c1"># init the lazy layers</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="c1"># create the ligtning model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LitModel</span><span class="p">(</span><span class="n">pt_model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.profilers</span> <span class="kn">import</span> <span class="n">PyTorchProfiler</span>
<span class="n">profiler</span> <span class="o">=</span> <span class="n">PyTorchProfiler</span><span class="p">()</span>

<span class="c1"># just need to set the profiler argument</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">csv_logger</span><span class="p">,</span> <span class="n">profiler</span><span class="o">=</span><span class="s1">&#39;simple&#39;</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type               | Params
-------------------------------------------------
0 | model     | Classifier         | 23.1 K
1 | train_acc | MulticlassAccuracy | 0     
2 | test_acc  | MulticlassAccuracy | 0     
-------------------------------------------------
23.1 K    Trainable params
0         Non-trainable params
23.1 K    Total params
0.093     Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "0cfb766397d54b98a46738b25ac0164b", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`Trainer.fit` stopped: `max_epochs=2` reached.
FIT Profiler Report

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|  Action                                                                                                                                                         	|  Mean duration (s)	|  Num calls      	|  Total time (s) 	|  Percentage %   	|
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|  Total                                                                                                                                                          	|  -              	|  64189          	|  14.797         	|  100 %          	|
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|  run_training_epoch                                                                                                                                             	|  6.672          	|  2              	|  13.344         	|  90.182         	|
|  run_training_batch                                                                                                                                             	|  0.0041982      	|  1764           	|  7.4056         	|  50.049         	|
|  [LightningModule]LitModel.optimizer_step                                                                                                                       	|  0.0041312      	|  1764           	|  7.2875         	|  49.251         	|
|  [Strategy]SingleDeviceStrategy.training_step                                                                                                                   	|  0.0023097      	|  1764           	|  4.0742         	|  27.535         	|
|  [Callback]TQDMProgressBar.on_train_batch_end                                                                                                                   	|  0.0010392      	|  1764           	|  1.8332         	|  12.389         	|
|  [Strategy]SingleDeviceStrategy.backward                                                                                                                        	|  0.00097618     	|  1764           	|  1.722          	|  11.638         	|
|  [_EvaluationLoop].val_next                                                                                                                                     	|  0.016218       	|  41             	|  0.66495        	|  4.4939         	|
|  [_TrainingEpochLoop].train_dataloader_next                                                                                                                     	|  0.00020906     	|  1764           	|  0.36878        	|  2.4923         	|
|  [Strategy]SingleDeviceStrategy.batch_to_device                                                                                                                 	|  0.00016747     	|  1804           	|  0.30211        	|  2.0417         	|
|  [LightningModule]LitModel.transfer_batch_to_device                                                                                                             	|  0.0001305      	|  1804           	|  0.23541        	|  1.591          	|
|  [Strategy]SingleDeviceStrategy.validation_step                                                                                                                 	|  0.0034089      	|  40             	|  0.13635        	|  0.92152        	|
|  [LightningModule]LitModel.optimizer_zero_grad                                                                                                                  	|  7.4632e-05     	|  1764           	|  0.13165        	|  0.88972        	|
|  [Callback]TQDMProgressBar.on_validation_start                                                                                                                  	|  0.040434       	|  3              	|  0.1213         	|  0.81979        	|
|  [LightningModule]LitModel.configure_gradient_clipping                                                                                                          	|  1.8325e-05     	|  1764           	|  0.032326       	|  0.21846        	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_train_batch_end       	|  1.7976e-05     	|  1764           	|  0.031709       	|  0.2143         	|
|  [Callback]TQDMProgressBar.on_validation_batch_end                                                                                                              	|  0.00062314     	|  40             	|  0.024926       	|  0.16845        	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_train_epoch_end       	|  0.01069        	|  2              	|  0.02138        	|  0.14449        	|
|  [Callback]TQDMProgressBar.on_train_start                                                                                                                       	|  0.012711       	|  1              	|  0.012711       	|  0.085902       	|
|  [Callback]TQDMProgressBar.on_sanity_check_start                                                                                                                	|  0.011467       	|  1              	|  0.011467       	|  0.077495       	|
|  [Callback]ModelSummary.on_train_batch_end                                                                                                                      	|  3.5871e-06     	|  1764           	|  0.0063276      	|  0.042764       	|
|  [Callback]TQDMProgressBar.on_before_zero_grad                                                                                                                  	|  3.0775e-06     	|  1764           	|  0.0054287      	|  0.036688       	|
|  [Callback]TQDMProgressBar.on_validation_batch_start                                                                                                            	|  0.000128       	|  40             	|  0.0051199      	|  0.034601       	|
|  [Callback]TQDMProgressBar.on_train_batch_start                                                                                                                 	|  2.6424e-06     	|  1764           	|  0.0046612      	|  0.031502       	|
|  [Callback]TQDMProgressBar.on_after_backward                                                                                                                    	|  2.566e-06      	|  1764           	|  0.0045265      	|  0.030591       	|
|  [Callback]TQDMProgressBar.on_before_backward                                                                                                                   	|  2.4907e-06     	|  1764           	|  0.0043937      	|  0.029694       	|
|  [Callback]TQDMProgressBar.on_before_optimizer_step                                                                                                             	|  1.8815e-06     	|  1764           	|  0.0033191      	|  0.022431       	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_after_backward        	|  1.6706e-06     	|  1764           	|  0.0029469      	|  0.019916       	|
|  [LightningModule]LitModel.on_before_batch_transfer                                                                                                             	|  1.5896e-06     	|  1804           	|  0.0028677      	|  0.01938        	|
|  [Callback]ModelSummary.on_before_zero_grad                                                                                                                     	|  1.5617e-06     	|  1764           	|  0.0027549      	|  0.018618       	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_before_zero_grad      	|  1.5176e-06     	|  1764           	|  0.002677       	|  0.018092       	|
|  [Callback]ModelSummary.on_train_batch_start                                                                                                                    	|  1.4731e-06     	|  1764           	|  0.0025985      	|  0.017561       	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_train_batch_start     	|  1.4309e-06     	|  1764           	|  0.0025241      	|  0.017058       	|
|  [Callback]TQDMProgressBar.on_validation_end                                                                                                                    	|  0.00081106     	|  3              	|  0.0024332      	|  0.016444       	|
|  [Callback]ModelSummary.on_before_backward                                                                                                                      	|  1.3715e-06     	|  1764           	|  0.0024193      	|  0.01635        	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_before_backward       	|  1.3586e-06     	|  1764           	|  0.0023965      	|  0.016196       	|
|  [Callback]ModelSummary.on_after_backward                                                                                                                       	|  1.2958e-06     	|  1764           	|  0.0022858      	|  0.015448       	|
|  [LightningModule]LitModel.on_after_batch_transfer                                                                                                              	|  1.2316e-06     	|  1804           	|  0.0022217      	|  0.015015       	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_before_optimizer_step 	|  1.2388e-06     	|  1764           	|  0.0021852      	|  0.014768       	|
|  [Callback]ModelSummary.on_before_optimizer_step                                                                                                                	|  1.2288e-06     	|  1764           	|  0.0021675      	|  0.014649       	|
|  [LightningModule]LitModel.on_train_batch_end                                                                                                                   	|  1.1941e-06     	|  1764           	|  0.0021064      	|  0.014236       	|
|  [Callback]TQDMProgressBar.on_train_epoch_start                                                                                                                 	|  0.00097899     	|  2              	|  0.001958       	|  0.013233       	|
|  [LightningModule]LitModel.on_train_batch_start                                                                                                                 	|  1.0119e-06     	|  1764           	|  0.0017849      	|  0.012063       	|
|  [LightningModule]LitModel.on_before_zero_grad                                                                                                                  	|  9.9306e-07     	|  1764           	|  0.0017518      	|  0.011839       	|
|  [LightningModule]LitModel.on_after_backward                                                                                                                    	|  9.5494e-07     	|  1764           	|  0.0016845      	|  0.011384       	|
|  [LightningModule]LitModel.on_before_backward                                                                                                                   	|  9.1773e-07     	|  1764           	|  0.0016189      	|  0.010941       	|
|  [LightningModule]LitModel.on_before_optimizer_step                                                                                                             	|  8.0452e-07     	|  1764           	|  0.0014192      	|  0.0095912      	|
|  [Strategy]SingleDeviceStrategy.on_train_batch_start                                                                                                            	|  8.0104e-07     	|  1764           	|  0.001413       	|  0.0095497      	|
|  [Callback]ModelSummary.on_fit_start                                                                                                                            	|  0.001351       	|  1              	|  0.001351       	|  0.0091307      	|
|  [LightningModule]LitModel.on_validation_model_eval                                                                                                             	|  0.00039541     	|  3              	|  0.0011862      	|  0.0080169      	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.setup                    	|  0.00079733     	|  1              	|  0.00079733     	|  0.0053885      	|
|  [Callback]TQDMProgressBar.on_train_epoch_end                                                                                                                   	|  0.00034932     	|  2              	|  0.00069864     	|  0.0047216      	|
|  [Callback]TQDMProgressBar.on_train_end                                                                                                                         	|  0.00055111     	|  1              	|  0.00055111     	|  0.0037245      	|
|  [LightningModule]LitModel.on_validation_model_train                                                                                                            	|  0.00011774     	|  3              	|  0.00035323     	|  0.0023872      	|
|  [LightningModule]LitModel.configure_optimizers                                                                                                                 	|  0.00014666     	|  1              	|  0.00014666     	|  0.00099116     	|
|  [Callback]ModelSummary.on_validation_batch_end                                                                                                                 	|  3.5676e-06     	|  40             	|  0.0001427      	|  0.00096444     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_validation_end        	|  4.3696e-05     	|  3              	|  0.00013109     	|  0.00088594     	|
|  [LightningModule]LitModel.lr_scheduler_step                                                                                                                    	|  5.1686e-05     	|  2              	|  0.00010337     	|  0.00069861     	|
|  [Callback]ModelSummary.on_validation_batch_start                                                                                                               	|  1.9563e-06     	|  40             	|  7.8253e-05     	|  0.00052886     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_validation_batch_end  	|  1.7672e-06     	|  40             	|  7.0687e-05     	|  0.00047772     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_validation_batch_start	|  1.6458e-06     	|  40             	|  6.5831e-05     	|  0.00044491     	|
|  [LightningModule]LitModel.on_validation_batch_end                                                                                                              	|  1.1852e-06     	|  40             	|  4.7408e-05     	|  0.0003204      	|
|  [LightningModule]LitModel.on_validation_batch_start                                                                                                            	|  9.9554e-07     	|  40             	|  3.9821e-05     	|  0.00026912     	|
|  [Strategy]SingleDeviceStrategy.on_validation_start                                                                                                             	|  7.5934e-06     	|  3              	|  2.278e-05      	|  0.00015395     	|
|  [Callback]ModelSummary.on_validation_start                                                                                                                     	|  7.4717e-06     	|  3              	|  2.2415e-05     	|  0.00015149     	|
|  [Callback]ModelSummary.on_validation_end                                                                                                                       	|  3.2149e-06     	|  3              	|  9.6448e-06     	|  6.5182e-05     	|
|  [Callback]TQDMProgressBar.on_validation_epoch_end                                                                                                              	|  2.7524e-06     	|  3              	|  8.2571e-06     	|  5.5804e-05     	|
|  [LightningModule]LitModel.on_train_epoch_end                                                                                                                   	|  3.9898e-06     	|  2              	|  7.9796e-06     	|  5.3928e-05     	|
|  [Callback]ModelSummary.on_train_epoch_end                                                                                                                      	|  3.2689e-06     	|  2              	|  6.5379e-06     	|  4.4185e-05     	|
|  [Callback]ModelSummary.on_train_epoch_start                                                                                                                    	|  3.2475e-06     	|  2              	|  6.495e-06      	|  4.3895e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_validation_start      	|  2.0663e-06     	|  3              	|  6.1989e-06     	|  4.1894e-05     	|
|  [Callback]ModelSummary.on_train_start                                                                                                                          	|  5.8357e-06     	|  1              	|  5.8357e-06     	|  3.9439e-05     	|
|  [Callback]TQDMProgressBar.on_save_checkpoint                                                                                                                   	|  2.9039e-06     	|  2              	|  5.8077e-06     	|  3.925e-05      	|
|  [Callback]TQDMProgressBar.on_validation_epoch_start                                                                                                            	|  1.7403e-06     	|  3              	|  5.221e-06      	|  3.5285e-05     	|
|  [Callback]TQDMProgressBar.setup                                                                                                                                	|  4.6473e-06     	|  1              	|  4.6473e-06     	|  3.1408e-05     	|
|  [Callback]ModelSummary.on_validation_epoch_end                                                                                                                 	|  1.5199e-06     	|  3              	|  4.5598e-06     	|  3.0816e-05     	|
|  [LightningModule]LitModel.on_validation_end                                                                                                                    	|  1.4435e-06     	|  3              	|  4.3306e-06     	|  2.9268e-05     	|
|  [Callback]TQDMProgressBar.on_fit_end                                                                                                                           	|  4.1407e-06     	|  1              	|  4.1407e-06     	|  2.7984e-05     	|
|  [Callback]TQDMProgressBar.on_sanity_check_end                                                                                                                  	|  4.122e-06      	|  1              	|  4.122e-06      	|  2.7858e-05     	|
|  [LightningModule]LitModel.on_train_epoch_start                                                                                                                 	|  2.0051e-06     	|  2              	|  4.0103e-06     	|  2.7102e-05     	|
|  [LightningModule]LitModel.on_validation_start                                                                                                                  	|  1.3088e-06     	|  3              	|  3.9265e-06     	|  2.6536e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_validation_epoch_end  	|  1.302e-06      	|  3              	|  3.906e-06      	|  2.6398e-05     	|
|  [Callback]ModelSummary.on_validation_epoch_start                                                                                                               	|  1.2126e-06     	|  3              	|  3.6377e-06     	|  2.4585e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_validation_epoch_start	|  1.1958e-06     	|  3              	|  3.5875e-06     	|  2.4245e-05     	|
|  [LightningModule]LitModel.on_validation_epoch_end                                                                                                              	|  1.1759e-06     	|  3              	|  3.5278e-06     	|  2.3842e-05     	|
|  [Callback]ModelSummary.on_save_checkpoint                                                                                                                      	|  1.7025e-06     	|  2              	|  3.4049e-06     	|  2.3011e-05     	|
|  [Callback]ModelSummary.on_sanity_check_start                                                                                                                   	|  3.295e-06      	|  1              	|  3.295e-06      	|  2.2269e-05     	|
|  [Callback]ModelSummary.on_train_end                                                                                                                            	|  3.295e-06      	|  1              	|  3.295e-06      	|  2.2269e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_train_start           	|  3.2205e-06     	|  1              	|  3.2205e-06     	|  2.1765e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_fit_start             	|  3.1982e-06     	|  1              	|  3.1982e-06     	|  2.1614e-05     	|
|  [Strategy]SingleDeviceStrategy.on_train_start                                                                                                                  	|  3.0417e-06     	|  1              	|  3.0417e-06     	|  2.0557e-05     	|
|  [Strategy]SingleDeviceStrategy.on_validation_end                                                                                                               	|  9.7789e-07     	|  3              	|  2.9337e-06     	|  1.9826e-05     	|
|  [LightningModule]LitModel.on_validation_epoch_start                                                                                                            	|  9.6609e-07     	|  3              	|  2.8983e-06     	|  1.9587e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_train_epoch_start     	|  1.4193e-06     	|  2              	|  2.8387e-06     	|  1.9184e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_save_checkpoint       	|  1.3905e-06     	|  2              	|  2.7809e-06     	|  1.8794e-05     	|
|  [LightningModule]LitModel.on_train_start                                                                                                                       	|  2.5611e-06     	|  1              	|  2.5611e-06     	|  1.7309e-05     	|
|  [LightningModule]LitModel.configure_callbacks                                                                                                                  	|  2.5574e-06     	|  1              	|  2.5574e-06     	|  1.7284e-05     	|
|  [Callback]TQDMProgressBar.on_fit_start                                                                                                                         	|  2.5127e-06     	|  1              	|  2.5127e-06     	|  1.6982e-05     	|
|  [Callback]TQDMProgressBar.teardown                                                                                                                             	|  2.4326e-06     	|  1              	|  2.4326e-06     	|  1.644e-05      	|
|  [LightningModule]LitModel.on_save_checkpoint                                                                                                                   	|  1.0366e-06     	|  2              	|  2.0731e-06     	|  1.4011e-05     	|
|  [LightningModule]LitModel.setup                                                                                                                                	|  2.0284e-06     	|  1              	|  2.0284e-06     	|  1.3709e-05     	|
|  [Callback]ModelSummary.on_sanity_check_end                                                                                                                     	|  2.0135e-06     	|  1              	|  2.0135e-06     	|  1.3608e-05     	|
|  [Callback]ModelSummary.setup                                                                                                                                   	|  1.7472e-06     	|  1              	|  1.7472e-06     	|  1.1808e-05     	|
|  [LightningModule]LitModel.prepare_data                                                                                                                         	|  1.6913e-06     	|  1              	|  1.6913e-06     	|  1.143e-05      	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.teardown                 	|  1.628e-06      	|  1              	|  1.628e-06      	|  1.1002e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_sanity_check_start    	|  1.6037e-06     	|  1              	|  1.6037e-06     	|  1.0838e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_fit_end               	|  1.5199e-06     	|  1              	|  1.5199e-06     	|  1.0272e-05     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_train_end             	|  1.492e-06      	|  1              	|  1.492e-06      	|  1.0083e-05     	|
|  [LightningModule]LitModel.on_fit_end                                                                                                                           	|  1.4734e-06     	|  1              	|  1.4734e-06     	|  9.9573e-06     	|
|  [Callback]ModelSummary.on_fit_end                                                                                                                              	|  1.4585e-06     	|  1              	|  1.4585e-06     	|  9.8566e-06     	|
|  [LightningModule]LitModel.configure_sharded_model                                                                                                              	|  1.4193e-06     	|  1              	|  1.4193e-06     	|  9.5922e-06     	|
|  [Strategy]SingleDeviceStrategy.on_train_end                                                                                                                    	|  1.4193e-06     	|  1              	|  1.4193e-06     	|  9.5922e-06     	|
|  [LightningModule]LitModel.on_train_end                                                                                                                         	|  1.4156e-06     	|  1              	|  1.4156e-06     	|  9.5671e-06     	|
|  [Callback]ModelCheckpoint{&#39;monitor&#39;: None, &#39;mode&#39;: &#39;min&#39;, &#39;every_n_train_steps&#39;: 0, &#39;every_n_epochs&#39;: 1, &#39;train_time_interval&#39;: None}.on_sanity_check_end      	|  1.3392e-06     	|  1              	|  1.3392e-06     	|  9.0509e-06     	|
|  [LightningModule]LitModel.teardown                                                                                                                             	|  1.3392e-06     	|  1              	|  1.3392e-06     	|  9.0509e-06     	|
|  [Callback]ModelSummary.teardown                                                                                                                                	|  1.3318e-06     	|  1              	|  1.3318e-06     	|  9.0006e-06     	|
|  [LightningModule]LitModel.on_fit_start                                                                                                                         	|  1.302e-06      	|  1              	|  1.302e-06      	|  8.7992e-06     	|
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
</pre></div>
</div>
</div>
</div>
<p>Lightning supports <a class="reference external" href="https://pytorch-lightning.readthedocs.io/en/1.6.2/advanced/profiler.html">several other performance profilers</a>.</p>
</section>
<section id="logging-with-weights-and-biases">
<h2>Logging with Weights and Biases<a class="headerlink" href="#logging-with-weights-and-biases" title="Permalink to this heading">#</a></h2>
<p>Logging is simply the act of recording data throughout model training and evaluation that can be used to make decisions about model development. Earlier, we used the CSV logger to record experimental results. <a class="reference external" href="https://wandb.ai/site">Weights and Biases (WandB)</a> is an online platform for logging training experiments. It provides a range of data collection and visualization tools to help you understand how your training is going. WandB is free for academic use cases. Its also very easy to integrate with Pytorch Lightning.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># init the classifier</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Classifier</span><span class="p">()</span> <span class="c1">#models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)</span>

<span class="c1"># init the lazy layers</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="c1"># create the ligtning model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LitModel</span><span class="p">(</span><span class="n">pt_model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_lightning.loggers</span> <span class="kn">import</span> <span class="n">WandbLogger</span>
<span class="n">wandb_logger</span> <span class="o">=</span> <span class="n">WandbLogger</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;initial_run&#39;</span><span class="p">)</span>

<span class="c1"># just need to set the profiler argument</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">wandb_logger</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

<span class="c1"># indicate that the run has finished</span>
<span class="n">wandb_logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.
<span class=" -Color -Color-Bold -Color-Bold-Blue">wandb</span>: Currently logged in as: <span class=" -Color -Color-Yellow">dhudsmith</span> (<span class=" -Color -Color-Yellow">wficai-fast</span>). Use <span class=" -Color -Color-Bold">`wandb login --relogin`</span> to force relogin
</pre></div>
</div>
<div class="output text_html">Tracking run with wandb version 0.15.4</div><div class="output text_html">Run data is saved locally in <code>./wandb/run-20230621_221708-uugnumm8</code></div><div class="output text_html">Syncing run <strong><a href='https://wandb.ai/wficai-fast/lightning_logs/runs/uugnumm8' target="_blank">initial_run</a></strong> to <a href='https://wandb.ai/wficai-fast/lightning_logs' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/run' target="_blank">docs</a>)<br/></div><div class="output text_html"> View project at <a href='https://wandb.ai/wficai-fast/lightning_logs' target="_blank">https://wandb.ai/wficai-fast/lightning_logs</a></div><div class="output text_html"> View run at <a href='https://wandb.ai/wficai-fast/lightning_logs/runs/uugnumm8' target="_blank">https://wandb.ai/wficai-fast/lightning_logs/runs/uugnumm8</a></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type               | Params
-------------------------------------------------
0 | model     | Classifier         | 23.1 K
1 | train_acc | MulticlassAccuracy | 0     
2 | test_acc  | MulticlassAccuracy | 0     
-------------------------------------------------
23.1 K    Trainable params
0         Non-trainable params
23.1 K    Total params
0.093     Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "b394a541fa2344158a576c020dfc58f0", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`Trainer.fit` stopped: `max_epochs=2` reached.
</pre></div>
</div>
<div class="output text_html">Waiting for W&B process to finish... <strong style="color:green">(success).</strong></div><script type="application/vnd.jupyter.widget-view+json">{"model_id": "30f7f964b84b482494e575febd77c136", "version_major": 2, "version_minor": 0}</script><div class="output text_html"><style>
    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}
    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }
    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }
    </style>
<div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br/><table class="wandb"><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc_epoch</td><td></td></tr><tr><td>val_acc_step</td><td></td></tr><tr><td>val_loss_epoch</td><td></td></tr><tr><td>val_loss_step</td><td></td></tr></table><br/></div><div class="wandb-col"><h3>Run summary:</h3><br/><table class="wandb"><tr><td>epoch</td><td>1</td></tr><tr><td>train_acc</td><td>0.85938</td></tr><tr><td>train_loss</td><td>0.46852</td></tr><tr><td>trainer/global_step</td><td>1763</td></tr><tr><td>val_acc_epoch</td><td>0.79718</td></tr><tr><td>val_acc_step</td><td>0.8125</td></tr><tr><td>val_loss_epoch</td><td>0.67434</td></tr><tr><td>val_loss_step</td><td>0.60827</td></tr></table><br/></div></div></div><div class="output text_html"> View run <strong style="color:#cdcd00">initial_run</strong> at: <a href='https://wandb.ai/wficai-fast/lightning_logs/runs/uugnumm8' target="_blank">https://wandb.ai/wficai-fast/lightning_logs/runs/uugnumm8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)</div><div class="output text_html">Find logs at: <code>./wandb/run-20230621_221708-uugnumm8/logs</code></div></div>
</div>
</section>
<section id="automatic-mixed-precision-amp">
<h2>Automatic mixed precision (AMP)<a class="headerlink" href="#automatic-mixed-precision-amp" title="Permalink to this heading">#</a></h2>
<p>By default, PyTorch uses 32-bit floating point numbers. These means that each element of a tensor, takes up 32 bits / 4 Bytes of memory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.float32
</pre></div>
</div>
</div>
</div>
<p>32-bit floats can store about 8 digits of precision. This level of precision may not be necessary for many of the computations performed by the neural network. Pytorch supports several other floating point formats, that we can make use of. For instance, we can allocate 16-bit tensors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">dtype</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.float16
</pre></div>
</div>
</div>
</div>
<p>16-bit floats take up half the memory of 32 bit, so this may allow us to train larger models on the same GPU hardware. In addition, modern GPU architectures can perform some calculations more efficiently with 16-bit numbers.</p>
<p>Unfortunately, it turns out that its usually not a good idea to convert all aspects of our computation into 16-bit floats. The research community has come up with good approaches to mixing 32-bit and 16-bit computation to get the benefits of using lower-precision without hurting model convergence. Manually setting all of this up is a headache, so Pytorch supports Automatic Mixed Precision to perform the conversion automatically under the hood.</p>
<p>To use this functionality, you use the <code class="docutils literal notranslate"><span class="pre">autocast</span></code> context manager:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creates model and optimizer in default precision</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">...</span><span class="p">)</span>

<span class="c1"># Creates a GradScaler once at the beginning of training.</span>
<span class="c1"># this improves convergence by preventing underflow</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Runs the forward pass with autocasting.</span>
        <span class="k">with</span> <span class="n">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="c1"># Scales loss.  Calls backward() on scaled loss to create scaled gradients.</span>
        <span class="c1"># Backward passes under autocast are not recommended.</span>
        <span class="c1"># Backward ops run in the same dtype autocast chose for corresponding forward ops.</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># scaler.step() first unscales the gradients of the optimizer&#39;s assigned params.</span>
        <span class="c1"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,</span>
        <span class="c1"># otherwise, optimizer.step() is skipped.</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="c1"># Updates the scale for next iteration.</span>
        <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</pre></div>
</div>
<section id="amp-in-pytorch-ligtning">
<h3>AMP in Pytorch Ligtning<a class="headerlink" href="#amp-in-pytorch-ligtning" title="Permalink to this heading">#</a></h3>
<p>Fortunately for us, it is extremely easy to implement AMP now that we have our model set up in Pytorch Lightning</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># init the classifier</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Classifier</span><span class="p">()</span> <span class="c1">#models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)</span>

<span class="c1"># init the lazy layers</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="c1"># create the ligtning model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LitModel</span><span class="p">(</span><span class="n">pt_model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wandb_logger</span> <span class="o">=</span> <span class="n">WandbLogger</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;AMP run&#39;</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">wandb_logger</span><span class="p">,</span> 
                  <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span> <span class="c1">#&lt;-- this</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

<span class="n">wandb_logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "7e7b2500a93446519091648765f7418a", "version_major": 2, "version_minor": 0}</script><div class="output text_html">Tracking run with wandb version 0.15.4</div><div class="output text_html">Run data is saved locally in <code>./wandb/run-20230621_221732-4r3rplac</code></div><div class="output text_html">Syncing run <strong><a href='https://wandb.ai/wficai-fast/lightning_logs/runs/4r3rplac' target="_blank">AMP run</a></strong> to <a href='https://wandb.ai/wficai-fast/lightning_logs' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/run' target="_blank">docs</a>)<br/></div><div class="output text_html"> View project at <a href='https://wandb.ai/wficai-fast/lightning_logs' target="_blank">https://wandb.ai/wficai-fast/lightning_logs</a></div><div class="output text_html"> View run at <a href='https://wandb.ai/wficai-fast/lightning_logs/runs/4r3rplac' target="_blank">https://wandb.ai/wficai-fast/lightning_logs/runs/4r3rplac</a></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type               | Params
-------------------------------------------------
0 | model     | Classifier         | 23.1 K
1 | train_acc | MulticlassAccuracy | 0     
2 | test_acc  | MulticlassAccuracy | 0     
-------------------------------------------------
23.1 K    Trainable params
0         Non-trainable params
23.1 K    Total params
0.093     Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "ee0e4599ab1e468c839db7f4e4d9fa3e", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`Trainer.fit` stopped: `max_epochs=2` reached.
</pre></div>
</div>
<div class="output text_html">Waiting for W&B process to finish... <strong style="color:green">(success).</strong></div><script type="application/vnd.jupyter.widget-view+json">{"model_id": "59547b0878dd4e72829c77bb6aa91915", "version_major": 2, "version_minor": 0}</script><div class="output text_html"><style>
    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}
    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }
    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }
    </style>
<div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br/><table class="wandb"><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc_epoch</td><td></td></tr><tr><td>val_acc_step</td><td></td></tr><tr><td>val_loss_epoch</td><td></td></tr><tr><td>val_loss_step</td><td></td></tr></table><br/></div><div class="wandb-col"><h3>Run summary:</h3><br/><table class="wandb"><tr><td>epoch</td><td>1</td></tr><tr><td>train_acc</td><td>0.79688</td></tr><tr><td>train_loss</td><td>0.62448</td></tr><tr><td>trainer/global_step</td><td>1763</td></tr><tr><td>val_acc_epoch</td><td>0.77883</td></tr><tr><td>val_acc_step</td><td>0.7775</td></tr><tr><td>val_loss_epoch</td><td>0.75217</td></tr><tr><td>val_loss_step</td><td>0.71923</td></tr></table><br/></div></div></div><div class="output text_html"> View run <strong style="color:#cdcd00">AMP run</strong> at: <a href='https://wandb.ai/wficai-fast/lightning_logs/runs/4r3rplac' target="_blank">https://wandb.ai/wficai-fast/lightning_logs/runs/4r3rplac</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)</div><div class="output text_html">Find logs at: <code>./wandb/run-20230621_221732-4r3rplac/logs</code></div></div>
</div>
</section>
</section>
<section id="model-checkpointing">
<h2>Model checkpointing<a class="headerlink" href="#model-checkpointing" title="Permalink to this heading">#</a></h2>
<p>Simply put, checkpointing is the process of saving a model periodically based on a metric that you monitor. If the metric has improved, save the model. In addition to saving the model weights, we need to save the hyperparameters. We do this by calling <code class="docutils literal notranslate"><span class="pre">self.save_hyperparameters()</span></code> in the initializer for the lightning model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pytorch_model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>Unlike the previous options, we need to use a Callback method to set up checkpointing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># init the classifier</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Classifier</span><span class="p">()</span> <span class="c1">#models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)</span>

<span class="c1"># init the lazy layers</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
     
<span class="c1"># create the ligtning model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LitModel</span><span class="p">(</span><span class="n">pt_model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># need to import the checkpoint callback object</span>
<span class="kn">from</span> <span class="nn">lightning.pytorch.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s2">&quot;checkpoints/&quot;</span><span class="p">,</span> <span class="n">save_top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">)</span>

<span class="c1"># we add log_model=&#39;all&#39; to save models on wandb</span>
<span class="n">wandb_logger</span> <span class="o">=</span> <span class="n">WandbLogger</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Run with Checkpointing&#39;</span><span class="p">,</span> <span class="n">log_model</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">)</span>

<span class="c1"># need to pass the checkpoint callback</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">wandb_logger</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">checkpoint_callback</span><span class="p">])</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

<span class="n">wandb_logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "3e99b440b2954780b30853d94b04fe24", "version_major": 2, "version_minor": 0}</script><div class="output text_html">Tracking run with wandb version 0.15.4</div><div class="output text_html">Run data is saved locally in <code>./wandb/run-20230621_221757-tgzrctm2</code></div><div class="output text_html">Syncing run <strong><a href='https://wandb.ai/wficai-fast/lightning_logs/runs/tgzrctm2' target="_blank">Run with Checkpointing</a></strong> to <a href='https://wandb.ai/wficai-fast/lightning_logs' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/run' target="_blank">docs</a>)<br/></div><div class="output text_html"> View project at <a href='https://wandb.ai/wficai-fast/lightning_logs' target="_blank">https://wandb.ai/wficai-fast/lightning_logs</a></div><div class="output text_html"> View run at <a href='https://wandb.ai/wficai-fast/lightning_logs/runs/tgzrctm2' target="_blank">https://wandb.ai/wficai-fast/lightning_logs/runs/tgzrctm2</a></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type               | Params
-------------------------------------------------
0 | model     | Classifier         | 23.1 K
1 | train_acc | MulticlassAccuracy | 0     
2 | test_acc  | MulticlassAccuracy | 0     
-------------------------------------------------
23.1 K    Trainable params
0         Non-trainable params
23.1 K    Total params
0.093     Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "1e527d8f6408470188594fc06ea5dfac", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>`Trainer.fit` stopped: `max_epochs=2` reached.
</pre></div>
</div>
<div class="output text_html">Waiting for W&B process to finish... <strong style="color:green">(success).</strong></div><script type="application/vnd.jupyter.widget-view+json">{"model_id": "ec73dd6aa67a44c39dd9c4cae7921855", "version_major": 2, "version_minor": 0}</script><div class="output text_html"><style>
    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}
    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }
    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }
    </style>
<div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br/><table class="wandb"><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc_epoch</td><td></td></tr><tr><td>val_acc_step</td><td></td></tr><tr><td>val_loss_epoch</td><td></td></tr><tr><td>val_loss_step</td><td></td></tr></table><br/></div><div class="wandb-col"><h3>Run summary:</h3><br/><table class="wandb"><tr><td>epoch</td><td>1</td></tr><tr><td>train_acc</td><td>0.77344</td></tr><tr><td>train_loss</td><td>0.85265</td></tr><tr><td>trainer/global_step</td><td>1763</td></tr><tr><td>val_acc_epoch</td><td>0.78878</td></tr><tr><td>val_acc_step</td><td>0.79125</td></tr><tr><td>val_loss_epoch</td><td>0.70496</td></tr><tr><td>val_loss_step</td><td>0.64834</td></tr></table><br/></div></div></div><div class="output text_html"> View run <strong style="color:#cdcd00">Run with Checkpointing</strong> at: <a href='https://wandb.ai/wficai-fast/lightning_logs/runs/tgzrctm2' target="_blank">https://wandb.ai/wficai-fast/lightning_logs/runs/tgzrctm2</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)</div><div class="output text_html">Find logs at: <code>./wandb/run-20230621_221757-tgzrctm2/logs</code></div></div>
</div>
<section id="creating-the-model-from-a-local-checkpoint">
<h3>Creating the model from a local checkpoint<a class="headerlink" href="#creating-the-model-from-a-local-checkpoint" title="Permalink to this heading">#</a></h3>
<p>To load a model from a checkpoint, we use the <code class="docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code>, passing in the path.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LitModel</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;checkpoints/epoch=1-step=1764.ckpt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># when creating a trainer just for validation, we don&#39;t need to fuss over the arguments.</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "1da46ffec63548f3b2300af2d56d9d39", "version_major": 2, "version_minor": 0}</script><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
<span style="font-weight: bold">      Validate metric      </span><span style="font-weight: bold">       DataLoader 0        </span>

<span style="color: #008080; text-decoration-color: #008080">       val_acc_epoch       </span><span style="color: #800080; text-decoration-color: #800080">    0.7675532102584839     </span>
<span style="color: #008080; text-decoration-color: #008080">      val_loss_epoch       </span><span style="color: #800080; text-decoration-color: #800080">    0.7949550747871399     </span>

</pre>
</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;val_loss_epoch&#39;: 0.7949550747871399, &#39;val_acc_epoch&#39;: 0.7675532102584839}]
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="early-stopping">
<h2>Early Stopping<a class="headerlink" href="#early-stopping" title="Permalink to this heading">#</a></h2>
<p>Early stopping, as the name implies, is a technique for stopping training early if a monitored metric is not improving. This can save lots of time and compute resources. We set this up in Lightning using a callback.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># init the classifier</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Classifier</span><span class="p">()</span> <span class="c1">#models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)</span>

<span class="c1"># init the lazy layers</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">pt_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="c1"># create the ligtning model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">LitModel</span><span class="p">(</span><span class="n">pt_model</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># need to import the early stop callback object</span>
<span class="kn">from</span> <span class="nn">lightning.pytorch.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span>
<span class="n">earlystop_callback</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">min_delta</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s2">&quot;checkpoints/&quot;</span><span class="p">,</span> <span class="n">save_top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">)</span>
<span class="n">wandb_logger</span> <span class="o">=</span> <span class="n">WandbLogger</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Early stopping run&#39;</span><span class="p">)</span>


<span class="c1"># need to pass the checkpoint callback</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">wandb_logger</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> 
                  <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">earlystop_callback</span><span class="p">,</span> <span class="n">checkpoint_callback</span><span class="p">])</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

<span class="n">wandb_logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">finish</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">Tracking run with wandb version 0.15.4</div><div class="output text_html">Run data is saved locally in <code>./wandb/run-20230621_221825-s41wgsxm</code></div><div class="output text_html">Syncing run <strong><a href='https://wandb.ai/wficai-fast/lightning_logs/runs/s41wgsxm' target="_blank">Early stopping run</a></strong> to <a href='https://wandb.ai/wficai-fast/lightning_logs' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/run' target="_blank">docs</a>)<br/></div><div class="output text_html"> View project at <a href='https://wandb.ai/wficai-fast/lightning_logs' target="_blank">https://wandb.ai/wficai-fast/lightning_logs</a></div><div class="output text_html"> View run at <a href='https://wandb.ai/wficai-fast/lightning_logs/runs/s41wgsxm' target="_blank">https://wandb.ai/wficai-fast/lightning_logs/runs/s41wgsxm</a></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type               | Params
-------------------------------------------------
0 | model     | Classifier         | 23.1 K
1 | train_acc | MulticlassAccuracy | 0     
2 | test_acc  | MulticlassAccuracy | 0     
-------------------------------------------------
23.1 K    Trainable params
0         Non-trainable params
23.1 K    Total params
0.093     Total estimated model params size (MB)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "3de30ef27fa54a8c8cc0ac5f874ae20a", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "", "version_major": 2, "version_minor": 0}</script><div class="output text_html">Waiting for W&B process to finish... <strong style="color:green">(success).</strong></div><script type="application/vnd.jupyter.widget-view+json">{"model_id": "9341bea7a76f4fc3af1281023dc89fba", "version_major": 2, "version_minor": 0}</script><div class="output text_html"><style>
    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}
    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }
    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }
    </style>
<div class="wandb-row"><div class="wandb-col"><h3>Run history:</h3><br/><table class="wandb"><tr><td>epoch</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>trainer/global_step</td><td></td></tr><tr><td>val_acc_epoch</td><td></td></tr><tr><td>val_acc_step</td><td></td></tr><tr><td>val_loss_epoch</td><td></td></tr><tr><td>val_loss_step</td><td></td></tr></table><br/></div><div class="wandb-col"><h3>Run summary:</h3><br/><table class="wandb"><tr><td>epoch</td><td>10</td></tr><tr><td>train_acc</td><td>0.82031</td></tr><tr><td>train_loss</td><td>0.72232</td></tr><tr><td>trainer/global_step</td><td>9701</td></tr><tr><td>val_acc_epoch</td><td>0.82207</td></tr><tr><td>val_acc_step</td><td>0.825</td></tr><tr><td>val_loss_epoch</td><td>0.58447</td></tr><tr><td>val_loss_step</td><td>0.54862</td></tr></table><br/></div></div></div><div class="output text_html"> View run <strong style="color:#cdcd00">Early stopping run</strong> at: <a href='https://wandb.ai/wficai-fast/lightning_logs/runs/s41wgsxm' target="_blank">https://wandb.ai/wficai-fast/lightning_logs/runs/s41wgsxm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)</div><div class="output text_html">Find logs at: <code>./wandb/run-20230621_221825-s41wgsxm/logs</code></div></div>
</div>
</section>
<section id="multi-gpu">
<h2>Multi-gpu<a class="headerlink" href="#multi-gpu" title="Permalink to this heading">#</a></h2>
<section id="single-node-multi-gpu">
<h3>Single-node, multi-gpu<a class="headerlink" href="#single-node-multi-gpu" title="Permalink to this heading">#</a></h3>
<p>Pytorch Lightning makes this very easy. In fact, Lightning will automatically use all available gpus by default. However, it is tricky to get this working in Jupyter notebooks. To demonstrate, we have create a script that you can download <a class="reference external" href="https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_advanced/multi_gpu.py">here</a>.</p>
</section>
<section id="multi-node-multi-gpu">
<h3>Multi-node, multi-gpu<a class="headerlink" href="#multi-node-multi-gpu" title="Permalink to this heading">#</a></h3>
<p>This is a bit trickier because it involves setting up communication across nodes. We have an example of how to set this up in our Palmetto Examples repository <a class="reference external" href="https://github.com/clemsonciti/palmetto-examples/tree/master/PyTorch/PBS/distributed_data_parallel">here</a>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./pytorch_advanced"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="04-pytorch_lightning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Pytorch Lightning</p>
      </div>
    </a>
    <a class="right-next"
       href="../containers/00-index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Containerization on Palmetto (under development)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#settings">Settings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#emnist-dataset">EMNIST Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baseline">Baseline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-profiling">Model profiling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logging-with-weights-and-biases">Logging with Weights and Biases</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-mixed-precision-amp">Automatic mixed precision (AMP)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#amp-in-pytorch-ligtning">AMP in Pytorch Ligtning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-checkpointing">Model checkpointing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-the-model-from-a-local-checkpoint">Creating the model from a local checkpoint</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">Early Stopping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu">Multi-gpu</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#single-node-multi-gpu">Single-node, multi-gpu</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-multi-gpu">Multi-node, multi-gpu</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Linh Ngo
</p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"></a> <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Research Computing and Data Workshops</span> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.
</div>
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>