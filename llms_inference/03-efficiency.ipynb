{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching, multi-gpu, and multi-node for large data and large models\n",
    "\n",
    "We've seen how to inference LLMs with a high degree of control over the model inputs and outputs. The goal of this last notebook is to discussion measures to scale up the inference process to large data and large models.\n",
    "\n",
    "There are three primary tools we will use:\n",
    "1. Batching\n",
    "2. Multi-GPU inference\n",
    "3. Multi-node inference\n",
    "\n",
    "We'll discuss each of these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Batching is the process of processing multiple inputs at once. This is a common technique in deep learning, as it allows the model to process multiple inputs in parallel. The `transformers` library has built-in support for batching, and we can use it to speed up inference with minimal code changes.\n",
    "\n",
    "First, we'll load a large number of pieces of text that we want to process using an LLM. Then, we'll process them in batches and compare the time it takes to process them in batches versus one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of texts from the 20 newsgroups dataset\n",
    "# Each text is a post from a newsgroup\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='test')['data'][:64]\n",
    "print(f'Number of documents: {len(docs)}')\n",
    "for i, doc in enumerate(docs[:3]):\n",
    "    print(f'\\n\\nDOCUMENT {i+1}:\\n{doc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want some piece of information about each of these newsgroup posts, and what we want cannot be easily extracted in an automated way using traditional NLP techniques. An LLM might be a good choice for such a task.\n",
    "\n",
    "For example, we might want a one-sentence summary of each post. We can craft a prompt that asks the model to generate such a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"auto\")\n",
    "\n",
    "device = model.device\n",
    "\n",
    "system_prompt = \"The user will supply a post from an online newsgroup. Summarize the post in a single, very short sentence.\"\n",
    "\n",
    "# Define a function that will generate summaries for a batch of posts\n",
    "def generate_summaries(texts, batch_size=8):\n",
    "    results = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    with tqdm(total=total_batches, desc=\"Processing batches\", leave=True, bar_format=\"{l_bar}{bar} | {n_fmt}/{total_fmt}\") as pbar:\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_messages = [[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": text}] for text in batch]\n",
    "                        \n",
    "            # Tokenize the messages using chat template\n",
    "            model_inputs = tokenizer.apply_chat_template(\n",
    "                batch_messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                return_dict=True,\n",
    "            ).to(device)\n",
    "\n",
    "            # Run model to get logits and generated output\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **model_inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    return_dict_in_generate=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            prompt_length = model_inputs[\"input_ids\"].shape[1]\n",
    "            generated_sequences = outputs.sequences[:, prompt_length:]\n",
    "            decoded_outputs = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n",
    "            results.extend(decoded_outputs)\n",
    "\n",
    "            pbar.update(1)\n",
    "    return results\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate summaries for the documents\n",
    "summaries = generate_summaries(docs, batch_size=1)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total time taken: {end - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "import torch\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU inference\n",
    "\n",
    "Thankfully, `transformers` makes multi-gpu inference easy.\n",
    "\n",
    "Note that there are multiple kinds of ways you might want to use multiple GPUs. Note that there are different kinds of paralellism one might want to use. For example, if you just want to speed up your LLM inference, and your model can fit on a single GPU, you can use *data parallelism*.\n",
    "\n",
    "If your model is too large to fit on a single GPU, you can use *model parallelism*, in which the different GPUs each hold a different part of the model. Luckily, `transformers` makes it easy to use model parallelism, via setting `device_map`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "device = model.device\n",
    "\n",
    "system_prompt = \"The user will supply a post from an online newsgroup. Summarize the post in a single, very short sentence.\"\n",
    "\n",
    "# Generate summaries for the documents\n",
    "summaries = generate_summaries(docs, batch_size=8)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total time taken: {end - start:.2f} seconds\")\n",
    "\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "import torch\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-node inference\n",
    "\n",
    "What if you have multiple nodes available, and want to use them all to speed up your inference? There are a variety of sorts of *parallelism* that are possible with multi-node inference.\n",
    "\n",
    "For example, you can use *data parallelism*, in which you split the data across the nodes, and each node processes a different part of the data. You can also use *model parallelism*, in which you split the model across the nodes, and each node processes a different part of the model. The former is for speeding up inference, and the latter is for when you have a model that's too large to fit on a single node.\n",
    "\n",
    "We will implement data parallelism. The code is in the scripts `inference.slurm`, `helper_inference.sh`, and `inference.py`. These three files work together to enable distributed inference:\n",
    "\n",
    "- `inference.slurm`: The SLURM job submission script that requests and configures computing resources (in this example, 2 nodes, each with 1 V100 GPU, 12GB memory, etc.)\n",
    "\n",
    "- `helper_inference.sh`: A shell script that sets up the environment and launches the distributed training using `torchrun`. It handles environment modules, activates the python environment, and configures the distributed setup parameters.\n",
    "\n",
    "- `inference.py`: The main Python script that performs the actual inference. It:\n",
    "  - Initializes distributed processing across nodes\n",
    "  - Loads the model and tokenizer\n",
    "  - Splits the input prompts across available nodes\n",
    "  - Processes prompts in parallel\n",
    "  - Gathers results back to the main node\n",
    "  - Saves the combined output\n",
    "\n",
    "The workflow is to use `sbatch` to submit `inference.slurm`, which calls `helper_inference.sh` on each node, which then launches `inference.py` in a coordinated way across all nodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
