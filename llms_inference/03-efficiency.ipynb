{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batching, multi-gpu, and multi-node for large data and large models\n",
    "\n",
    "We've seen how to inference LLMs with a high degree of control over the model inputs and outputs. The goal of this last notebook is to discussion measures to scale up the inference process to large data and large models.\n",
    "\n",
    "There are three primary tools we will use:\n",
    "1. Batching\n",
    "2. Multi-GPU inference\n",
    "3. Multi-node inference\n",
    "\n",
    "We'll discuss each of these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Batching is the process of processing multiple inputs at once. This is a common technique in deep learning, as it allows the model to process multiple inputs in parallel. The `transformers` library has built-in support for batching, and we can use it to speed up inference with minimal code changes.\n",
    "\n",
    "First, we'll load a large number of pieces of text that we want to process using an LLM. Then, we'll process them in batches and compare the time it takes to process them in batches versus one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of texts from the 20 newsgroups dataset\n",
    "# Each text is a post from a newsgroup\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='test', data_home='/project/rcde/cehrett/running_llms_workshop/data')['data'][:64]\n",
    "print(f'Number of documents: {len(docs)}')\n",
    "for i, doc in enumerate(docs[:3]):\n",
    "    print(f'\\n\\nDOCUMENT {i+1}:\\n{doc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want some piece of information about each of these newsgroup posts, and what we want cannot be easily extracted in an automated way using traditional NLP techniques. An LLM might be a good choice for such a task.\n",
    "\n",
    "For example, we might want a one-sentence summary of each post. We can craft a prompt that asks the model to generate such a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", device_map=\"auto\")\n",
    "\n",
    "device = model.device\n",
    "\n",
    "system_prompt = \"The user will supply a post from an online newsgroup. Summarize the post in a single, very short sentence.\"\n",
    "\n",
    "# Define a function that will generate summaries for a batch of posts\n",
    "def generate_summaries(texts, batch_size):\n",
    "    results = []\n",
    "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    with tqdm(total=total_batches, desc=\"Processing batches\", leave=True, bar_format=\"{l_bar}{bar} | {n_fmt}/{total_fmt}\") as pbar:\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            batch_messages = [[{\"role\": \"system\", \"content\": system_prompt}, \n",
    "                               {\"role\": \"user\", \"content\": text}] for text in batch]\n",
    "                        \n",
    "            # Tokenize the messages using chat template\n",
    "            model_inputs = tokenizer.apply_chat_template(\n",
    "                batch_messages,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                return_dict=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(device)\n",
    "\n",
    "            # Run model to get logits and generated output\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                torch.set_grad_enabled(False)\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "                outputs = model.generate(\n",
    "                    **model_inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    return_dict_in_generate=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode output\n",
    "            prompt_length = model_inputs[\"input_ids\"].shape[1]\n",
    "            generated_sequences = outputs.sequences[:, prompt_length:]\n",
    "            decoded_outputs = tokenizer.batch_decode(generated_sequences, skip_special_tokens=True)\n",
    "            results.extend(decoded_outputs)\n",
    "\n",
    "            pbar.update(1)\n",
    "    return results\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Generate summaries for the documents\n",
    "summaries = generate_summaries(docs, batch_size=16)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total time taken: {end - start:.2f} seconds\")\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why vLLM Is Faster\n",
    "\n",
    "vLLM achieves significantly higher throughput than `transformers` for three main reasons:\n",
    "\n",
    "1. **PagedAttention (optimized KV-cache management)**  \n",
    "   vLLM stores and reuses key–value attention cache in a paging system instead of repeatedly reallocating large tensors. This drastically reduces memory movement and avoids fragmentation.\n",
    "\n",
    "2. **Continuous batching**  \n",
    "   New requests are dynamically added to running batches without waiting for all sequences in the batch to finish. This eliminates “straggler” slowdown and keeps the GPU saturated.\n",
    "\n",
    "3. **CUDA graph + compilation optimizations**  \n",
    "   vLLM captures decoding operations into CUDA graphs, reducing Python overhead and kernel-launch latency. It also fuses kernels and uses optimized scheduling.\n",
    "\n",
    "Together, these features keep the GPU >90% utilized during generation, giving vLLM its characteristic 2–5× speedup over standard `transformers` inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\"Qwen/Qwen3-4B-Instruct-2507\", gpu_memory_utilization=0.70)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def vllm_generate_summaries(texts, batch_size):\n",
    "    system_prompt = \"Summarize the post in one very short sentence.\"\n",
    "\n",
    "    # Build prompts (vLLM expects a single string per item)\n",
    "    prompts = [\n",
    "        f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{text}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\"\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        outputs = llm.generate(batch, sampling_params)\n",
    "        results.extend([o.outputs[0].text for o in outputs])\n",
    "    \n",
    "    return results\n",
    "\n",
    "start = time.time()\n",
    "summaries = vllm_generate_summaries(docs, batch_size=64)\n",
    "end = time.time()\n",
    "print(f\"Total time taken: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    question=(\n",
    "        \"What considerations affect the choice of batch size? \"\n",
    "        \"What downsides are there to setting an enormous batch size? \"\n",
    "        \"(Unsure? Try out different batch sizes and see!)\"\n",
    "    ),\n",
    "    question_id=\"nb3-01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "import torch\n",
    "del model\n",
    "del llm\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU inference\n",
    "\n",
    "Thankfully, `transformers` makes multi-gpu inference easy.\n",
    "\n",
    "Note that there are multiple kinds of ways you might want to use multiple GPUs. Note that there are different kinds of paralellism one might want to use. For example, if you just want to speed up your LLM inference, and your model can fit on a single GPU, you can use *data parallelism*.\n",
    "\n",
    "If your model is too large to fit on a single GPU, you can use *model parallelism*, in which the different GPUs each hold a different part of the model. Luckily, `transformers` makes it easy to use model parallelism, via setting `device_map`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\" # \"meta-llama/Llama-3.3-70B-Instruct\" \n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left') \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", dtype=torch.bfloat16)\n",
    "\n",
    "device = model.device\n",
    "\n",
    "system_prompt = \"The user will supply a post from an online newsgroup. Summarize the post in a single, very short sentence.\"\n",
    "\n",
    "# Generate summaries for the documents\n",
    "summaries = generate_summaries(docs, batch_size=8)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total time taken: {end - start:.2f} seconds\")\n",
    "\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "import torch\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-node inference\n",
    "\n",
    "What if you have multiple nodes available, and want to use them all to speed up your inference? There are a variety of sorts of *parallelism* that are possible with multi-node inference.\n",
    "\n",
    "For example, you can use *data parallelism*, in which you split the data across the nodes, and each node processes a different part of the data. You can also use *model parallelism*, in which you split the model across the nodes, and each node processes a different part of the model. The former is for speeding up inference, and the latter is for when you have a model that's too large to fit on a single node.\n",
    "\n",
    "We will implement data parallelism. The code is in the scripts `multinode_infer.slurm` and `multinode_infer.py`. These files work together to enable distributed inference:\n",
    "\n",
    "- `multinode_infer.slurm`: The SLURM job submission script that requests and configures computing resources (in this example, 2 nodes, each with 2 A100 GPU, 32GB memory, etc.)\n",
    "\n",
    "- `multinode_infer.py`: The main Python script that performs the actual inference. \n",
    "\n",
    "The workflow is to use `sbatch` to submit `multinode_infer.slurm`, which calls `multinode_infer.py` in a coordinated way across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    question=(\n",
    "        \"Thank you for participating in today's workshop! Please record here any thoughts you have about how this workshop or others like it could be of greater use to you in the future.\"\n",
    "    ),\n",
    "    question_id=\"nb3-02\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
