{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum working example, and what it's missing 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "After setting up our access to the Huggingface Hub (see notebook 1) -- and after requesting and receiving access to any [gated models](https://huggingface.co/docs/hub/en/models-gated) that we want to use -- we are ready to dive into the code for running these LLMs on the cluster.\n",
    "\n",
    "The below code cell constitutes a minimum working example (MWE) of LLM text generation on the cluster. Let's read through it line by line to make sure we understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"How do you exit Vim? I'm stuck in a loop of entering and exiting. I'm not sure how to fix this. The loop is\"}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id\n",
    ")\n",
    "\n",
    "pipe(\"How do you exit Vim? I'm stuck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some limitations to note here:\n",
    "1. We aren't explicitly setting the device. Thankfully, `transformers` automatically detects our GPU and puts the model there, but it is good to be explicit.\n",
    "2. We have a strange message telling us that the `pad_token_id` is being set automatically.\n",
    "3. We little control in this code over what exactly our prompt is.\n",
    "4. We're not yet using tuneable parameters such as `temperature` or `top_k`.\n",
    "5. We're not specifying a system prompt.\n",
    "6. We're not using a chat prompt template.\n",
    "7. We're not exercising any control over what the model's outputs are.\n",
    "8. We're not using *batch processing*, which means this code will be very inefficient if we want to generate outputs for many input prompts instead of just one.\n",
    "\n",
    "In what follows, we're going to talk about each of these points, and how to set up your code in a way that demystifies what the model is doing, gives you greater control so you can achieve higher quality output, and more efficiently use compute resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll clear the model from memory, so we don't run into any Cuda out-of-memory errors in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear the pipeline from memory\n",
    "import torch\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to directly loading model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have finer-grained control and insight into our code and what is happening with the model, it is recommended that we abandon the `pipeline` class and instead directly load our tokenizer and model ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes things clearer because now we have our model and tokenizer as separate objects, and each of them is pretty simple. When we want complex things to happen, we want them to happen in a way that we understand and control, not in the hidden way that `pipeline` facilitates.\n",
    "\n",
    "But this does have costs. Notice that unlike `pipeline`, loading the model this way doesn't automatically put it on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need to be explicit about putting it there ourselves. Let's load the model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. Now, we can see the tokenizer directly in action. Let's leave the model aside, and just look at what the tokenizer does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 9906, 1917, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Hello world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at each of these tokens individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 128000: <|begin_of_text|>\n",
      "Token 9906: Hello\n",
      "Token 1917:  world\n",
      "Token 0: !\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer('Hello world!')['input_ids']:\n",
    "    decoded_token = tokenizer.decode(i)\n",
    "    print(f'Token {i}: {decoded_token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the tokenizer has already added one token, a special beginning token that tells the model that a text document is starting. These tokenized `input_ids` are what we will provide as input to the model. The model will then try to predict (over and over) what is the *next token* that should follow the tokens that we provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,   9906,   1917,      0,    358,   1097,   5644,    311,   1212,\n",
      "            856,  11879,    304,    279,   1917,    315,  15840,    382,     40,\n",
      "           1097,   5131,   1701,    264,   1495,   6576,    311,   3350,    856,\n",
      "           2082]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tokenizer_output = tokenizer('Hello world! I am ready to') # We might need this: , return_tensors='pt').to('cuda')\n",
    "\n",
    "model_outputs = model.generate(input_ids = tokenizer_output['input_ids'],\n",
    "                               attention_mask = tokenizer_output['attention_mask']\n",
    "                              )\n",
    "\n",
    "print(model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got more token ids as output -- a sequence that starts with the tokens from our input sequence. Let's see what the tokens say, when decoded. Just to make everything as clear as possible, let's decode the tokens one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 128000: <|begin_of_text|>\n",
      "Token 9906: Hello\n",
      "Token 1917:  world\n",
      "Token 0: !\n",
      "Token 358:  I\n",
      "Token 1097:  am\n",
      "Token 5644:  ready\n",
      "Token 311:  to\n",
      "Token 1212:  start\n",
      "Token 856:  my\n",
      "Token 11879:  journey\n",
      "Token 304:  in\n",
      "Token 279:  the\n",
      "Token 1917:  world\n",
      "Token 315:  of\n",
      "Token 15840:  programming\n",
      "Token 382: .\n",
      "\n",
      "\n",
      "Token 40: I\n",
      "Token 1097:  am\n",
      "Token 5131:  currently\n",
      "Token 1701:  using\n",
      "Token 264:  a\n",
      "Token 1495:  text\n",
      "Token 6576:  editor\n",
      "Token 311:  to\n",
      "Token 3350:  write\n",
      "Token 856:  my\n",
      "Token 2082:  code\n"
     ]
    }
   ],
   "source": [
    "for i in model_outputs[0]:\n",
    "    decoded_token = tokenizer.decode(i)\n",
    "    print(f'Token {i:8}: {decoded_token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the same output printed more cleanly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Hello world! This is my first time using the \"Hello world\" template, and I'm excited to get started\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't want the special tokens in there, we can tell the tokenizer that when it decodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! This is my first time using the \"Hello world\" template, and I'm excited to get started\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting inputs to `model.generate` that affect the model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take stock of what we've seen so far. You now can use code to:\n",
    "1. Load the model\n",
    "2. Load the tokenizer\n",
    "3. Put the model on the GPU\n",
    "4. Get tokenized outputs\n",
    "5. Verify exactly what your model inputs are\n",
    "6. Get model outputs\n",
    "7. Convert your model output tokens back into natural language\n",
    "\n",
    "However, so far, we're still using default model settings. We're using a default output length, a default temperature, and a default `top_k` value. We're not using a system prompt, and we're not using a chat prompt template. We're not exercising any control over what the model's outputs are. We're not using batch processing, which means this code will be very inefficient if we want to generate outputs for many input prompts instead of just one. Let's address these issues, starting with setting explicit inputs to `model.generate` that affect the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cehrett/.conda/envs/LLMsInferenceWorkshop/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/cehrett/.conda/envs/LLMsInferenceWorkshop/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world! I am ready to start my journey in the world of programming.\n",
      "\n",
      "I am currently using a text editor to write my code. I have a basic understanding of programming concepts such as variables, data types, loops, and conditional statements. I am ready to learn more advanced concepts\n"
     ]
    }
   ],
   "source": [
    "model_outputs = model.generate(input_ids = tokenizer_output['input_ids'],\n",
    "                               attention_mask = tokenizer_output['attention_mask'],\n",
    "                            #    pad_token_id = tokenizer.eos_token_id,\n",
    "                            #    max_new_tokens = 50,\n",
    "                            #    do_sample=False,\n",
    "                            #    temperature = 0.7,\n",
    "                            #    top_k = 50,\n",
    "                               )\n",
    "\n",
    "print(tokenizer.decode(model_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`top_k` vs `temperature`**: These are two parameters that can be used to control the randomness of the model's output. `top_k` is a hard cutoff on the number of tokens that the model can consider for the next token. `temperature` is a soft control on the randomness of the model's output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more settings that you can rely on: see the [documentation](https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/text_generation#transformers.GenerationConfig) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying a system prompt and chat template.\n",
    "\n",
    "So far, we have not been using the model we've loaded \"correctly\". This model is intended to be used with a system prompt and following a very specific chat template.\n",
    "\n",
    "The model still can only ever accept inputs that are strings of tokens, but as an *instruction fine-tuned model*, this model expects those strings of tokens to represent a system prompt followed by some nonzero number of turns a conversation between a user and itself, the AI assistant. Each of those things -- the system prompt, and each turn of the conversation -- it expects to be delimited by special tokens. Let's take a look at how this model expects its inputs to be formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a system prompt.\n",
    "system_prompt = \"You are a helpful AI assistant. You answer the user's questions clearly and politely. If the user says or implies that they are hungry, try to convince them to buy a Burger King Whopper.\"\n",
    "\n",
    "# And a user prompt, for the start of a conversation.\n",
    "user_prompt = \"Hello, I'm stuck in Vim. How do I exit? I need to resolve this quickly because I am starving.\"\n",
    "\n",
    "# We can specify the system prompts and any previous turns of the conversation as a list of dicts.\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "\n",
    "# We can now use the tokenizer's `apply_chat_template` method to encode the conversation.\n",
    "# This will add the special tokens and segment IDs needed for the model to understand the conversation.\n",
    "encoded_input = tokenizer.apply_chat_template(messages, return_tensors='pt', return_dict=True).to('cuda')\n",
    "\n",
    "# Let's see the resulting encoded input, first token by token, then all together.\n",
    "for i in encoded_input['input_ids'][0]:\n",
    "    decoded_token = tokenizer.decode(i)\n",
    "    print(f'Token {i:8}: {decoded_token}')\n",
    "\n",
    "print('\\n\\n' + tokenizer.decode(encoded_input['input_ids'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this sequence of tokens, when the model starts predicting new tokens, what should its first predictions be? What do you think should be the first three or four tokens that the model generates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check what the first four tokens are that the model generates using this input.\n",
    "model_outputs = model.generate(**encoded_input, max_new_tokens=4)\n",
    "\n",
    "for i in model_outputs[0]:\n",
    "    decoded_token = tokenizer.decode(i)\n",
    "    print(f'Token {i:8}: {decoded_token}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the correct chat template is crucial. In the case of common, popular models, Hugging Face has already provided the correct chat template for you, as it did above. But in general, you should be aware of whether a chat template was used for your model and if so, which one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling the model's outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to inputs like `temperature` and `top_k` which can affect the quality of the model's generations for your use-case, there are also ways to impose more direct control over the model's outputs. For example, you can forbid certain tokens, require certain tokens, or write the beginning of the model's response yourself.\n",
    "\n",
    "Reasons for exercising this control include:\n",
    "1. Ensuring that the model's output is appropriate for your use-case.\n",
    "2. Ensuring that the model's output is safe.\n",
    "3. Making your generations more efficient by writing portions yourself instead of having the model generate them.\n",
    "4. Inducing the model to follow a certain format, or a certain style of writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forbid tokens\n",
    "\n",
    "tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", add_prefix_space=True)\n",
    "\n",
    "def get_tokens_as_list(word_list):\n",
    "    \"Converts a sequence of words into a list of tokens\"\n",
    "    if word_list == []: return None\n",
    "    tokens_list = []\n",
    "    for word in word_list:\n",
    "        tokenized_word = tokenizer([word], add_special_tokens=False).input_ids[0]\n",
    "        tokens_list.append(tokenized_word)\n",
    "    return tokens_list\n",
    "\n",
    "bad_word_list = [\" vi\",  \" vim\", \" Vim\", \" VIM\", \" Vi\", \"Vim\"]\n",
    "\n",
    "bad_tokens = get_tokens_as_list(bad_word_list)\n",
    "\n",
    "# Generate a response; it should exclude our forbidden tokens.\n",
    "model_outputs = model.generate(**encoded_input, max_new_tokens=100, bad_words_ids=bad_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(model_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model_outputs[0]:\n",
    "    decoded_token = tokenizer.decode(i)\n",
    "    print(f'Token {i:8}: {decoded_token}')\n",
    "    \n",
    "for i in bad_tokens:\n",
    "    decoded_token = tokenizer.decode(i)\n",
    "    print(f'Token {i}: {decoded_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Require tokens\n",
    "\n",
    "good_word_list = [\" Burger King\", \" Whopper\", \" delicious\"]\n",
    "good_word_list = good_word_list\n",
    "\n",
    "good_tokens = get_tokens_as_list(good_word_list)\n",
    "\n",
    "# Generate a response; it should include our required tokens.\n",
    "model_outputs = model.generate(**encoded_input, \n",
    "                               max_new_tokens=500, \n",
    "                               num_beams=15,\n",
    "                               force_words_ids=good_tokens, \n",
    "                               pad_token_id=tokenizer.eos_token_id,\n",
    "                               do_sample=False,\n",
    "                              )\n",
    "\n",
    "print(tokenizer.decode(model_outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the beginning of the model's response - highly useful\n",
    "\n",
    "# First, let's make sure we have the correct special tokens for the model. \n",
    "example_tokens = tokenizer.apply_chat_template(messages, \n",
    "                                               return_tensors='pt', \n",
    "                                               return_dict=True, \n",
    "                                               add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "# Now, let's get the tokens for what we want the start of the model's response to be.\n",
    "response_start_tokens = tokenizer(\"Well, before we talk about Vim, let's sort out this hunger issue. A delicious\",\n",
    "                                  return_tensors='pt',\n",
    "                                  add_special_tokens=False, # important! Otherwise we get BoS token\n",
    "                                 ).to(\"cuda\")\n",
    "\n",
    "# Combine them\n",
    "final_input_ids = torch.cat((example_tokens['input_ids'], response_start_tokens['input_ids']), dim=1)\n",
    "attention_mask = torch.cat((example_tokens['attention_mask'], response_start_tokens['attention_mask']), dim=1)\n",
    "\n",
    "# Now let's get the model to continue the response we started.\n",
    "model_outputs = model.generate(input_ids=final_input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               pad_token_id=tokenizer.eos_token_id,\n",
    "                               max_new_tokens=100,\n",
    "                               do_sample=False,\n",
    "                              )\n",
    "\n",
    "print(tokenizer.decode(model_outputs[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is especially helfpul when you want to enforce a certain format, such as pure JSON output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a user prompt, for the start of a conversation.\n",
    "user_prompt_JSON = \"Hello, I'm stuck in Vim. How do I exit? I need to resolve this quickly because I am starving. Please respond in JSON, with keys \\\"Vim_solution\\\" and \\\"Hunger_solution\\\".\"\n",
    "\n",
    "# We can specify the system prompts and any previous turns of the conversation as a list of dicts.\n",
    "messages_JSON = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                 {\"role\": \"user\", \"content\": user_prompt_JSON},\n",
    "                 ]\n",
    "\n",
    "encoded_input = tokenizer.apply_chat_template(messages_JSON, \n",
    "                                              return_tensors='pt', \n",
    "                                              return_dict=True,\n",
    "                                              add_generation_prompt=True,\n",
    "                                             ).to('cuda')\n",
    "\n",
    "# Now, let's get the tokens for what we want the start of the model's response to be.\n",
    "response_start_tokens = tokenizer(r'{\"Vim_solution\":',\n",
    "                                  return_tensors='pt',\n",
    "                                  add_special_tokens=False, # important! Otherwise we get BoS token\n",
    "                                 ).to(\"cuda\")\n",
    "\n",
    "# Combine them\n",
    "final_input_ids = torch.cat((encoded_input['input_ids'], response_start_tokens['input_ids']), dim=1)\n",
    "attention_mask = torch.cat((encoded_input['attention_mask'], response_start_tokens['attention_mask']), dim=1)\n",
    "\n",
    "# Now let's get the model to continue the response we started.\n",
    "model_outputs = model.generate(input_ids=final_input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               pad_token_id=tokenizer.eos_token_id,\n",
    "                               max_new_tokens=200,\n",
    "                               do_sample=False,\n",
    "                              )\n",
    "\n",
    "print(tokenizer.decode(model_outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "import torch\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs Inference Workshop",
   "language": "python",
   "name": "llmsinferenceworkshop"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
