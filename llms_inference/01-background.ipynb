{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LLMs on Palmetto\n",
    "\n",
    "## Instructor\n",
    "- **Instructor**: Carl Ehrett\n",
    "- **Office**: 2105 Barre Hall, Clemson University\n",
    "- **Email**: cehrett AT clemson DOT edu\n",
    "\n",
    "## Workshop Description\n",
    "This workshop series introduces essential concepts related to LLMs and works through the common steps in an LLM inference workflow. This workshop focuses on efficiently running LLMs, rather than on constructing, training or fine-tuning them. Throughout the sessions, students will learn how to use the Hugging Face Transformers library to run LLMs on the Palmetto Cluster. The workshop will also cover how to use the Palmetto Cluster to run LLMs on large datasets and how to use the Palmetto Cluster to run LLMs on multiple GPUs and multiple nodes.\n",
    "\n",
    "## Prerequisites\n",
    "* **All workshop participants should have a Palmetto Cluster account.** If you do not already have an account, you can visit our [getting started page](https://docs.rcd.clemson.edu/palmetto/starting).\n",
    "* **Participants should be familiar with the Python programming language.** This requirement could be fulfilled by personal projects, coursework, or completion of the Introduction to [Python Programming workshop series](https://clemsonciti.github.io/rcde_workshops/python_programming/00-index.html).\n",
    "\n",
    "## Environment\n",
    "To run the code in this workshop, you will need a python environment with the appropriate libraries installed. I have created such an environment in a shared space, but that environment will not always be available to you. You can create such an environment yourself as follows. \n",
    "\n",
    "To set up your Python environment, load the Anaconda module provided on Palmetto (e.g., `module load anaconda3/2023.09-0`), which gives you access to the `conda` tool. Then create and activate your own conda environment (for example, `conda create -n llm_workshop python=3.11 && conda activate llm_workshop`). After that, install any additional packages your code requires — either one by one using `pip install packagename`, or from a `requirements.txt` file if you have one.\n",
    "\n",
    "## Hugging Face Hub\n",
    "In order to use the code in the Workshop notebooks, you will need a Hugging Face account. You can create one [here](https://huggingface.co/join)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to get models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pre-trained models in your workflows, you need to know where to find and download them. Here’s a quick guide:\n",
    "\n",
    "**NOTE: USE THE DATA TRANSFER NODE TO DOWNLOAD MODELS.** From the login node, `ssh hpcdtn01.rcd.clemson.edu`, or `ssh hpcdtn02.rcd.clemson.edu`.\n",
    "\n",
    "#### Direct Download\n",
    "- Many models are available for direct download from their creators' websites or repositories.\n",
    "- Use `wget` or `curl` to fetch model files directly into your HPC environment.\n",
    "\n",
    "#### Hugging Face Hub\n",
    "- The Hugging Face Hub is a popular platform for accessing pre-trained models, datasets, and other ML resources.\n",
    "\n",
    "##### What is the Hugging Face Hub?\n",
    "- A community-driven platform with thousands of pre-trained models for NLP, computer vision, and more.\n",
    "- Provides tools for easy integration with Python scripts and libraries like `transformers`.\n",
    "\n",
    "##### Setting Up Your Hugging Face Account\n",
    "1. Visit [huggingface.co](https://huggingface.co/) and create an account.\n",
    "2. Generate a personal access token:\n",
    "   - Go to **Settings** > **Access Tokens**.\n",
    "   - Create a new token with the necessary permissions for downloading models.\n",
    "3. Log in to the Hugging Face CLI to store your token:\n",
    "   - Run the following command in the terminal after activating your `LLMsInferenceWorkshop` environment:\n",
    "     ```bash\n",
    "     huggingface-cli login\n",
    "     ```\n",
    "   - Paste your access token when prompted.\n",
    "   - The token will be saved in `~/.huggingface/token`, allowing persistent access without needing to re-enter it.\n",
    "\n",
    "##### Setting the Cache Directory\n",
    "- To avoid storing large models in your home directory, configure a cache directory on your scratch space:\n",
    "  ```bash\n",
    "  export HF_HOME=/scratch/username/huggingface\n",
    "  ```\n",
    "- Add this line to your .bashrc or .bash_profile to make it persistent.\n",
    "\n",
    "##### Downloading models\n",
    "- To download a model, run the command\n",
    "  ```bash\n",
    "  huggingface-cli download [model_name]\n",
    "  ```\n",
    "- For these notebooks, you will need to download `Qwen/Qwen3-0.6B`, `Qwen/Qwen3-VL-4B-Instruct`, `Qwen/Qwen3-4B-Instruct-2507`, and `Equall/Saul-Instruct-v1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceptual overview\n",
    "\n",
    "LLMs are a class of models that are designed to predict the next token (word, or chunk of a word) in a sequence of tokens. They are trained on a truly vast amount of text data.\n",
    "\n",
    "LLMs these days tend to be *Transformer-based neural networks*. If you're interested in learning more about the Transformer architecture, we have a workshop in which we build and train a Transformer-based LLM from scratch using PyTorch. \n",
    "\n",
    "In this workshop, we will not dwell on the details of the Transformer architecture or how LLMs are trained. We will not focus on the mathematical details of these models or how they work at a fundamental level. Instead, we will focus on how to use pre-trained LLMs to generate text efficiently on Palmetto. Everything we will discuss here also extends to using Multimodal LLMs on Palmetto as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%env HF_HOME=/project/rcde/models   \n",
    "%env HF_HUB_CACHE=/project/rcde/models   \n",
    "\n",
    "# this sets the Huggingface model cache to be the directory listed above -- \n",
    "# where I have already placed some models. Ordinarily, you should set this\n",
    "# to be somewhere on the scratch drive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Suppress warnings and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "print(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    question=(\n",
    "        \"Please describe your level of familiarity with the Python programming language.\"\n",
    "    ),\n",
    "    question_id=\"nb1-01\"\n",
    ")\n",
    "\n",
    "create_answer_box(\n",
    "    question=(\n",
    "        \"Please describe your level of previous experience running LLMs using code. (It's okay if the answer is \\\"none\\\".)\"\n",
    "    ),\n",
    "    question_id=\"nb1-02\"\n",
    ")\n",
    "\n",
    "create_answer_box(\n",
    "    question=(\n",
    "        \"Please describe any present or planned use of LLMs in your research.)\"\n",
    "    ),\n",
    "    question_id=\"nb1-03\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an LLM can be simple or complicated, depending on how you want to use it. A very simple case would be the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=\"Qwen/Qwen3-0.6B\", # or Qwen/Qwen3-4B-Instruct-2507\", \n",
    "                )\n",
    "\n",
    "messages = [\"The purpose of this workshop is\"]\n",
    "\n",
    "output = pipe(messages)\n",
    "print(output[0][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    question=(\n",
    "        \"How much GPU memory (VRAM) do you think is needed to run the above code? \"\n",
    "        \"How much CPU RAM do you think is needed? \"\n",
    "        \"How could you ballpark, or check?\"\n",
    "    ),\n",
    "    question_id=\"nb1-04\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the pipeline from memory\n",
    "import torch\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LLMs on the Cluster\n",
    "\n",
    "How is using an LLM on the Cluster different from using a web interface like ChatGPT? How is it different from using an LLM through an API like OpenAI's API?\n",
    "\n",
    "1. **Control:** Aside from control over the system prompt, you have control over the model itself. You can choose which model to use. This is again **crucial for reproducibility**. Third-party services like Anthropic, Google and OpenAI can and do change or take away models without warning. This can undermine your research. \n",
    "2. **Cost:** Since you all already have access to the Cluster, you can run LLMs on the Cluster for free. This is not the case with third-party services.\n",
    "3. **Privacy:** When running LLMs on the Cluster, your data never need leave the Cluster. This is not the case with third-party services.\n",
    "4. **Speed:** The Cluster is a powerful computing resource. You can run LLMs on the Cluster much faster than you can on your own computer. This is especially true if you use a GPU on the Cluster, and if you batch your data.\n",
    "5. **Not persistent server:** While you can run LLM inference on the Palmetto Cluster, the Cluster does not support hosting a persistent, externally accessible service for real-time LLM interaction, like ChatGPT. HPC systems like the Cluster are optimized for batch processing and resource-intensive jobs, not for real-time interaction, especially with external users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sizes of commonly used LLMs\n",
    "\n",
    "LLMs are enormous; while small ones can run reasonably well on a good laptop, larger ones require the kind of hardware you would only find in an HPC cluster like Palmetto. \n",
    "\n",
    "We don't know how big chatbots like GPT-5.1 are, because OpenAI won't tell us. But models with similar performance typically have *at least hundreds of billions of parameters*.\n",
    "\n",
    "That just means that the mathematical object which *is* the model is composed of that many numbers. Each such number is typically represented by a 16-bit floating point number, which is 2 bytes. So, a 100-billion-parameter model would be about 200 GB in size. To run that model, it's not enough to store that on your SSD or HDD; you need to load it into memory -- preferably GPU memory! *Nobody's laptop has that much memory.*\n",
    "\n",
    "Even if you can manage to run a 1- or 3-billion parameter model on your laptop, it will typically be much easier, faster and more efficient to run it on Palmetto, especially for large-scale workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization\n",
    "\n",
    "One way to run larger models on smaller hardware is to use quantization. This is useful sometimes both for running LLMs on your laptop and for running them on Palmetto.\n",
    "\n",
    "Quantization represents the model parameters with fewer bits than the 16-bit floating point numbers that are typically used. This can reduce the size of the model by a factor of 2, 4, or more. It can also speed up the model. It comes at a cost, however: quantization can reduce the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a heavily quantized model to demonstrate size reduction\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
    "import torch\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(355)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Load 4-bit quantized model\n",
    "model_id = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16)\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64)\n",
    "pipe_4bit = pipeline(\"text-generation\", model=model_4bit, tokenizer=tokenizer, max_new_tokens=64)\n",
    "\n",
    "# Print model size info\n",
    "print(f\"Model loaded in 16-bit quantization, Approximate memory usage: {pipe.model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "print(f\"Model loaded in 4-bit quantization, Approximate memory usage: {pipe_4bit.model.get_memory_footprint() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"The Fibonacci sequence begins: \"\n",
    "\n",
    "print(f\"16-bit model: {pipe(message)[0]['generated_text']}\")\n",
    "print(f\"4-bit model: {pipe_4bit(message)[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "import torch\n",
    "del pipe\n",
    "del pipe_4bit\n",
    "del model\n",
    "del model_4bit\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain-specific LLMs\n",
    "\n",
    "There are many LLMs available for use. Some are general-purpose, like Llama-3.2 models. Others are domain-specific, like models trained on scientific literature, or on code, or even \"non-language\" data like molecular structures.\n",
    "\n",
    "Most LLMs, though, are trained to be general-purpose. General-purpose models can be adapted to specific domains by prompt-engineering, few-shot learning, retrieval-augmented generation, fine-tuning, or other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"Equall/Saul-Instruct-v1\", dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Please explain the key differences between common law and civil law systems.\"},\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False)\n",
    "print(outputs[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "del pipe, prompt, outputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multimodal LLMs\n",
    "\n",
    "Multimodal LLMs are LLMs that can take in not just text, but also images, audio, video, or other data types. They work essentially just like text LLMs, but they can tokenize and process these other data types as well. \n",
    "\n",
    "Let's see one such model take a look at the below image. We can ask the model to describe it.\n",
    "\n",
    "<img src=\"files/kitchen.jpg\" alt=\"Kitchen Image\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "model_id = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Load image\n",
    "image = Image.open(\"files/kitchen.jpg\")\n",
    "\n",
    "# Build chat prompt\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"Write a recipe that this woman might be using.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to text template\n",
    "chat_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "# Preprocess (tokenize + image encoding)\n",
    "inputs = processor(text=[chat_text], images=[image], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "output = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# Decode\n",
    "text = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code challenge\n",
    "\n",
    "Copy the above code cell that inferences the VL (vision-language) model. Then consult with the earlier section of this notebook that describes how to load a model in quantized form. Modify the VL code to use a 4-bit quantization of the model. When you're finished, report the resulting model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code cell to develop your code\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    question=(\n",
    "        \"How much GPU memory (VRAM) does your quantized version of the VL model consume?\"\n",
    "    ),\n",
    "    question_id=\"nb1-05\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference vs. training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is just a very large collection of (billions of) numbers, describing a mathematical operation which is performed on the input (the prompt) in order to produce the output (the generated text). Model *training* is the process of finding those numbers, identifying the values of the model parameters that get the best output.\n",
    "\n",
    "*Fine-tuning* is a kind of model training, when a model is trained on a specialized dataset after first being trained on a broader dataset.\n",
    "\n",
    "In this workshop, we are not training or fine-tuning models; we are *inferencing* models. That means the model parameters are already fixed, and we won't do anything to change them. We are instead using those fixed model parameters to generate outputs. This is also what happens when you chat with an LLM such as ChatGPT.\n",
    "\n",
    "Training and fine-tuning are **far more compute-intensive** and require much more GPU memory than inferencing. E.g., An 8B LLM takes about 16GB of GPU memory to inference, and takes about 32GB or more to fine-tune, due to the additional memory required for gradient computations, optimizer states, and intermediate activations during backpropagation.\n",
    "\n",
    "Fine-tuning is often not necessary, even for specialized tasks. Prompt engineering, few-shot learning, and retrieval-augmented generation often are just as or even more effective than fine-tuning for molding an LLM to your particular desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative LLM Frameworks: When to Use Them\n",
    "\n",
    "While `transformers` is the primary library we’ll use in this workshop for GPU-based workflows, it’s worth briefly introducing several alternative frameworks: **LlamaCpp**, **Ollama**, and **vLLM**.  \n",
    "Each serves a different purpose — from lightweight local inference to high-throughput serving — and can complement your workflow depending on your goals and environment.\n",
    "\n",
    "---\n",
    "\n",
    "#### **LlamaCpp**\n",
    "- **What it is:**  \n",
    "  A lightweight, CPU-first library designed for running quantized versions of models.\n",
    "- **Why use it:**  \n",
    "  Efficient for local inference on CPU nodes or testing quantized models without GPU dependency.  \n",
    "  Particularly useful for offline environments on laptops.\n",
    "- **Key features:**  \n",
    "  - Extremely low memory footprint.  \n",
    "  - Efficient CPU-only execution with minimal dependencies.  \n",
    "  - Supports quantized model formats that dramatically reduce memory use.  \n",
    "- **Limitations:**  \n",
    "  - CPU-only performance is slower for large-scale workloads.  \n",
    "  - Limited to decoder-only models.  \n",
    "  - No GPU acceleration, fine-tuning, or integration with the broader Hugging Face ecosystem.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Ollama**\n",
    "- **What it is:**  \n",
    "  A simple CLI tool and platform for running pre-trained models locally.  \n",
    "- **Why use it:**  \n",
    "  Excellent for quick experiments or exploring pre-packaged models on your own machine, with automatic model management and setup.  \n",
    "- **Key features:**  \n",
    "  - Run models with a single command (`ollama run llama3`).  \n",
    "  - Supports quantized models for efficient local use.  \n",
    "  - Simple API and local HTTP endpoints for easy integration into scripts and apps.  \n",
    "- **Limitations:**  \n",
    "  - Limited customization or experimental control.  \n",
    "  - Restricted to supported models and formats curated by Ollama.  \n",
    "  - Not designed for multi-GPU, HPC, or large-batch workflows.  \n",
    "  - Lacks transparency and extensibility compared to `transformers`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **vLLM**\n",
    "- **What it is:**  \n",
    "  A high-performance inference engine built for *maximum throughput* on GPUs.  \n",
    "  It uses techniques like **PagedAttention** and **continuous batching** to minimize memory waste and maximize token generation speed.\n",
    "- **Why use it:**  \n",
    "  Ideal when the goal is *speed and scalability* — for example, running inference over large datasets or serving models to multiple users concurrently.  \n",
    "- **Key features:**  \n",
    "  - **High-throughput inference:** Faster batch and streaming generation than `transformers`.  \n",
    "  - **Optimized GPU utilization:** Custom CUDA kernels and memory scheduling designed for decoding-heavy workloads.  \n",
    "- **Limitations:**  \n",
    "  - **Inference-only:** Cannot fine-tune or modify model internals.  \n",
    "  - **Limited model coverage:** Primarily supports *decoder-only* text-generation models (LLaMA, Mistral, Falcon, etc.); does **not** support encoder-decoder or most multimodal or embedding architectures (e.g., T5, BART, CLIP).  \n",
    "  - **Partial ecosystem integration:** Loads Hugging Face models and tokenizers but lacks full compatibility with libraries like `datasets`, `evaluate`, and `peft`.  \n",
    "  - **Reduced introspection:** Does not expose hidden states or attention weights, limiting interpretability.  \n",
    "  - **Reproducibility caveats:** Asynchronous batching and kernel scheduling introduce slight nondeterminism in outputs and logprobs.  \n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "For **research**, `transformers` remains the most appropriate choice. It's transparent, reproducible, and compatible with the full Hugging Face ecosystem.  \n",
    "Use **vLLM** when maximum throughput or production-scale inference is the priority. But be mindful of its narrower model support and limited experimental control.  \n",
    "Use **LlamaCpp** or **Ollama** for lightweight local experimentation when GPUs aren’t available or setup time is limited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
