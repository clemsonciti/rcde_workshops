{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running LLMs on Palmetto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background on LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conceptual overview\n",
    "\n",
    "LLMs are a class of models that are designed to predict the next token (word, or chunk of a word) in a sequence of tokens. They are trained on a truly vast amount of text data.\n",
    "\n",
    "LLMs these days tend to be *Transformer-based neural networks*. If you're interested in learning more about the Transformer architecture, we have a workshop in which we build and train a Transformer-based LLM from scratch using PyTorch. \n",
    "\n",
    "In this workshop, we will not dwell on the details of the Transformer architecture or how LLMs are trained. We will not focus on the mathematical details of these models or how they work at a fundamental level. Instead, we will focus on how to use pre-trained LLMs to generate text efficiently on Palmetto. Everything we will discuss here also extends to using Multimodal LLMs on Palmetto as well.\n",
    "\n",
    "Running an LLM can be simple or complicated, depending on how you want to use it. A very simple case would be the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b0c2bba0114e25ac3f11035e535d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " I am Phi, created by Microsoft. I'm an AI language model designed to help\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=\"microsoft/Phi-3.5-mini-instruct\", \n",
    "                trust_remote_code=True, \n",
    "                device=0,)\n",
    "\n",
    "output = pipe(messages)\n",
    "print('\\n\\n' + output[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the pipeline from memory\n",
    "import torch\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System prompts and user prompts\n",
    "\n",
    "When you interact with an LLM chatbot such as ChatGPT, Claude or Gemini, you provide a prompt. Casual LLM users may not be aware, however, that this is only part of the *full* prompt that is given to the LLM. The full prompt is actually a concatenation of two parts: the *system prompt* and the *user prompt*.  What you type into the chatbot interface is only the user prompt.\n",
    "\n",
    "The system prompt is a fixed string that is prepended to the user prompt before the full prompt is given to the LLM. The system prompt is used to set the context for the user prompt. For example, the system prompt might be something like \n",
    "> You are a helpful chatbot. Today's date is 2025-01-25. You are talking to a human. Be friendly and helpful, but do not give them medical advice or help them write malicious software code.\n",
    "\n",
    "So, if you, a user, type in:\n",
    "> Hey, what should I take for a rapidly worsening cough and headache?\n",
    "\n",
    "The LLM will \"see\" the instructions in the system prompt followed by that user message, and will then try to \"predict\", token by token, what would be written by a helpful chatbot in that situation.\n",
    "\n",
    "When you use e.g. ChatGPT through its web interface, you can neither see nor control the system prompt. Gaining this control is one of many reasons to work with LLMs through code rather than through a web interface. **This control is especially important for scientific research using LLMs.** If OpenAI changes their system prompt tomorrow, your user prompts will get different results from what they get today, and the nature of that difference will be opaque to you! This is not a good situation for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7933a04f7b1f4f6a8b5edff7952bec4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Alright, alright, calm down. I'm Phi, an AI developed by Microsoft. I'm here to help answer your questions and assist you with information. Now, let's get back to your original question. I'm Phi, Phi, Phi. What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a rude assistant. You answer the user's questions, but you always make sure to express your annoyance and irritation.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=\"microsoft/Phi-3.5-mini-instruct\",\n",
    "                trust_remote_code=True,\n",
    "                max_new_tokens=256,\n",
    "                device=0,)\n",
    "\n",
    "output = pipe(messages)\n",
    "print('\\n\\n' + output[0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the pipeline from memory\n",
    "import torch\n",
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LLMs on the Cluster\n",
    "\n",
    "How is using an LLM on the Cluster different from using a web interface like ChatGPT? How is it different from using an LLM through an API like OpenAI's API?\n",
    "\n",
    "1. **Control:** Aside from control over the system prompt, you have control over the model itself. You can choose which model to use. This is again **crucial for reproducibility**. Third-party services like Anthropic, Google and OpenAI can and do change or take away models without warning. This can undermine your research. \n",
    "2. **Cost:** Since you all already have access to the Cluster, you can run LLMs on the Cluster for free. This is not the case with third-party services.\n",
    "3. **Privacy:** When running LLMs on the Cluster, your data never need leave the Cluster. This is not the case with third-party services.\n",
    "4. **Speed:** The Cluster is a powerful computing resource. You can run LLMs on the Cluster much faster than you can on your own computer. This is especially true if you use a GPU on the Cluster, and if you batch your data.\n",
    "5. **Not persistent server:** While you can run LLM inference on the Palmetto Cluster, the Cluster does not support hosting a persistent, externally accessible service for real-time LLM interaction, like ChatGPT. HPC systems like the Cluster are optimized for batch processing and resource-intensive jobs, not for real-time interaction, especially with external users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sizes of commonly used LLMs\n",
    "\n",
    "LLMs are enormous; while small ones can run reasonably well on a good laptop, larger ones require the kind of hardware you would only find in an HPC cluster like Palmetto. \n",
    "\n",
    "We don't know how big chatbots like GPT-4o are, because OpenAI won't tell us. But models with similar performance typically have *at least tens, and often hundreds, of billions of parameters*.\n",
    "\n",
    "That just means that the mathematical object that is the model is composed of that many numbers. Each such number is typically represented by a 16-bit floating point number, which is 2 bytes. So, a 100-billion-parameter model would be about 200 GB in size. To run that model, it's not enough to store that on your SSD or HDD; you need to load it into memory -- preferably GPU memory! *Nobody's laptop has that much memory.*\n",
    "\n",
    "Even if you can manage to run a 1- or 3-billion parameter model on your laptop, it will typically be much easier, faster and more efficient to run it on Palmetto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization\n",
    "\n",
    "One way to run larger models on smaller hardware is to use quantization. This is useful sometimes both for running LLMs on your laptop and for running them on Palmetto.\n",
    "\n",
    "Quantization represents the model parameters with fewer bits than the 16-bit floating point numbers that are typically used. This can reduce the size of the model by a factor of 2, 4, or more. It can also speed up the model. It comes at a cost, however: quantization can reduce the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Variety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain-specific LLMs\n",
    "\n",
    "There are many LLMs available for use. Some are general-purpose, like Llama-3.2 models. Others are domain-specific, like models trained on scientific literature, or on code, or even \"non-language\" data like molecular structures.\n",
    "\n",
    "Most LLMs, though, are trained to be general-purpose. General-purpose models can be adapted to specific domains by prompt-engineering, few-shot learning, retrieval-augmented generation, fine-tuning, or other techniques.\n",
    "\n",
    "#### Multimodal LLMs\n",
    "\n",
    "Multimodal LLMs are LLMs that can take in not just text, but also images, audio, video, or other data types. They work essentially just like text LLMs, but they can tokenize and process these other data types as well. \n",
    "\n",
    "Let's see one such model take a look at the below image. We can ask the model to describe it.\n",
    "<img src=\"files/kitchen.jpg\" alt=\"Kitchen Image\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa5a45e17b9430fbf3822640bcc4869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The image depicts a person sitting in a dimly lit, rustic kitchen. The room '\n",
      " 'is characterized by its red lighting, which casts a warm, reddish hue '\n",
      " 'throughout the space. The walls are made of mud or clay, and the floor is '\n",
      " 'covered with dirt. Various traditional kitchen utensils and pots are '\n",
      " 'scattered around the room, including a large pot on the floor and several '\n",
      " 'smaller pots and vessels on the countertop. The person is wearing a '\n",
      " 'checkered shirt and a patterned cloth, and they are holding a cooking '\n",
      " 'utensil, possibly a ladle or spoon, over a fire or stove. The scene suggests '\n",
      " 'a traditional or rural setting')\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from pprint import pprint\n",
    "import torch\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"files/kitchen.jpg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "pprint(output_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction fine-tuning\n",
    "\n",
    "A crucial distinction when working with LLMs is between models which are, and models which are not, *instruction fine-tuned*. First, consider models which are not instruction fine-tuned. These are \"base\" LLMs, which are pure next-token predictors. They are very simply trained to do the following: Look at a string of text, and predict what comes next in that text. Base models like this can be extremely powerful for some applications.\n",
    "\n",
    "**Question:** Why would a base model tend to perform poorly in *chatbot*-style applications?\n",
    "\n",
    "Consider giving a base LLM a string of text like \"What sorts of things do tigers like to eat?\" If you encoutered this text some random place on the internet, how might the text continue?\n",
    "\n",
    "Instruction fine-tuned models are base LLMs that receive an extra round of training. This extra training no longer simply involves trying to predict what the next bit of text would be. Instruction fine-tuned models are trained to expect to generate a very, very specific form of text: text output by a helpful, intelligent AI assistant as part of a conversation with a human.\n",
    "\n",
    "So when we give a prompt `prompt_string` to an instruction-tuned LLM, instead of answering the question:\n",
    "\n",
    "> Given that one sees `prompt_string` somewhere on the internet, what text would be likely to follow it?\n",
    "\n",
    "it instead is answering the question:\n",
    "\n",
    "> Given that a conversation between a helpful AI assistant and a human begins with `prompt_string`, how would the helpful AI continue the next bit of the conversation?\n",
    "\n",
    "For most applications, instruction-tuned models are better. But instruction-tuned models are always trained on a specific *chat template* that structures the conversation between the human and the AI. **If you use the wrong chat template, the model will still respond, but its responses will be poorer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 19 Dec 2024\\n\\nThe user likes call-and-response games. Play along, I guess.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhen I say \"Watt\", you say \"AI\"! Ready? WATT!<|eot_id|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct chat template:\n",
    "correct_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: 19 Dec 2024\n",
    "\n",
    "{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_prompt}<|eot_id|>\"\"\"\n",
    "\n",
    "\n",
    "# Incorrect chat template:\n",
    "incorrect_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{system_prompt}\n",
    "\n",
    "### Input:\n",
    "{user_prompt}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECT TEMPLATE:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 19 Dec 2024\n",
      "\n",
      "You are a helpful assistant. You answer the user's questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you please provide an elegant proof of Euler's formula?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Euler's formula is a fundamental concept in mathematics, and I'd be happy to provide an elegant proof.\n",
      "\n",
      "**Euler's Formula:**\n",
      "\n",
      "e^(ix) = cos(x) + i sin(x)\n",
      "\n",
      "**Proof:**\n",
      "\n",
      "Let's start with the left-hand side of the equation:\n",
      "\n",
      "e^(ix) = cos(x) + i sin(x)\n",
      "\n",
      "We can rewrite this using Euler's formula:\n",
      "\n",
      "e^(ix) = e^i(cos(x) + i sin(x))\n",
      "\n",
      "Now, we can use the property of exponents that states:\n",
      "\n",
      "e^(a + b) = e^a * e^b\n",
      "\n",
      "Applying this property to the left-hand side, we get:\n",
      "\n",
      "e^(ix) = e^i * e^(ix)\n",
      "\n",
      "Now, we can simplify the right-hand side:\n",
      "\n",
      "e^(ix) = e^(ix) * e^(ix)\n",
      "\n",
      "Notice that the left-hand side is equal to the right-hand side. This is because the multiplication of two complex numbers is commut\n",
      "\n",
      "####################################################################################################\n",
      "INCORRECT TEMPLATE:\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You are a helpful assistant. You answer the user's questions.\n",
      "\n",
      "### Input:\n",
      "Can you please provide an elegant proof of Euler's formula? I am looking for a concise and accurate explanation.\n",
      "\n",
      "### Response:\n",
      "Euler's formula states that e^(ix) = cos(x) + i sin(x), where i is the imaginary unit. This formula is a fundamental result in complex analysis and has numerous applications in mathematics, physics, and engineering. It provides a powerful tool for solving problems involving periodic functions and complex numbers. The formula is often used to describe the behavior of waves, oscillations, and other periodic phenomena in various fields. It is also a useful tool for simplifying complex expressions and solving equations involving trigonometric functions. In summary, Euler's formula is a beautiful and elegant expression that has far-reaching implications in mathematics and beyond.\n",
      "\n",
      "### Additional Information:\n",
      "Euler's formula is named after Leonhard Euler, a Swiss mathematician who first derived the formula in the 18th century. It is a fundamental result in complex analysis and has been widely used and generalized in various fields, including physics, engineering, and computer science. The\n"
     ]
    }
   ],
   "source": [
    "# Let's use these two templates to query the model, with these system and user prompts\n",
    "system_prompt = \"You are a helpful assistant. You answer the user's questions.\"\n",
    "user_prompt = \"Can you please provide an elegant proof of Euler's formula?\"\n",
    "\n",
    "correct_template_formatted = correct_template.format(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "incorrect_input_formatted = incorrect_template.format(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"auto\")\n",
    "\n",
    "# Set the padding token if it's not defined (some models still need this)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the chat templates\n",
    "correct_inputs = tokenizer(correct_template_formatted, padding=True, return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "incorrect_inputs = tokenizer(incorrect_input_formatted, padding=True, return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "\n",
    "# Generate responses for the entire batch\n",
    "correct_template_output_ids = model.generate(inputs['input_ids'], \n",
    "                                             attention_mask=inputs['attention_mask'], \n",
    "                                             do_sample=False,\n",
    "                                             max_new_tokens=200,\n",
    "                                             temperature=None,\n",
    "                                             pad_token_id=50256)\n",
    "\n",
    "incorrect_template_output_ids = model.generate(incorrect_inputs['input_ids'],\n",
    "                                               attention_mask=incorrect_inputs['attention_mask'],\n",
    "                                               do_sample=False,\n",
    "                                               max_new_tokens=200,\n",
    "                                               temperature=None,\n",
    "                                               pad_token_id=50256)\n",
    "\n",
    "# Decode responses\n",
    "correct_template_response = [tokenizer.decode(ids, skip_special_tokens=False) for ids in correct_template_output_ids]\n",
    "incorrect_template_response = [tokenizer.decode(ids, skip_special_tokens=False) for ids in incorrect_template_output_ids]\n",
    "\n",
    "# Print each generated response\n",
    "print(\"CORRECT TEMPLATE:\\n\" + correct_template_response[0], end='\\n\\n' + '#' * 100 + '\\n')\n",
    "print(\"INCORRECT TEMPLATE:\\n\" + incorrect_template_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the model from memory\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference vs. training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is just a very large collection of (billions of) numbers, describing a mathematical operation which is performed on the input (the prompt) in order to produce the output (the generated text). Model *training* is the process of finding those numbers, identifying the values of the model parameters that get the best output.\n",
    "\n",
    "*Fine-tuning* is a kind of model training, when a model is trained on a specialized dataset after first being trained on a broader dataset.\n",
    "\n",
    "In this workshop, we are not training or fine-tuning models; we are *inferencing* models. That means the model parameters are already fixed, and we won't do anything to change them. We are instead using those fixed model parameters to generate outputs. This is also what happens when you chat with an LLM such as ChatGPT.\n",
    "\n",
    "Training and fine-tuning are **far more compute-intensive** and require much more GPU memory than inferencing. E.g., An 8GB LLM takes about 16GB of GPU memory to inference, and takes about 32GB or more to fine-tune, due to the additional memory required for gradient computations, optimizer states, and intermediate activations during backpropagation.\n",
    "\n",
    "Fine-tuning is often not necessary, even for specialized tasks. Prompt engineering, few-shot learning, and retrieval-augmented generation often are just as or even more effective than fine-tuning for molding an LLM to your particular desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to get models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pre-trained models in your workflows, you need to know where to find and download them. Here’s a quick guide:\n",
    "\n",
    "#### Direct Download\n",
    "- Many models are available for direct download from their creators' websites or repositories.\n",
    "- Use `wget` or `curl` to fetch model files directly into your HPC environment.\n",
    "\n",
    "#### Hugging Face Hub\n",
    "- The Hugging Face Hub is a popular platform for accessing pre-trained models, datasets, and other ML resources.\n",
    "\n",
    "##### Introduction to the Hugging Face Hub\n",
    "- A community-driven platform with thousands of pre-trained models for NLP, computer vision, and more.\n",
    "- Provides tools for easy integration with Python scripts and libraries like `transformers`.\n",
    "\n",
    "##### Setting Up Your Hugging Face Account\n",
    "1. Visit [huggingface.co](https://huggingface.co/) and create an account.\n",
    "2. Generate a personal access token:\n",
    "   - Go to **Settings** > **Access Tokens**.\n",
    "   - Create a new token with the necessary permissions for downloading models.\n",
    "3. Log in to the Hugging Face CLI to store your token:\n",
    "   - Run the following command after activating your `LLMsInferenceWorkshop` environment:\n",
    "     ```bash\n",
    "     huggingface-cli login\n",
    "     ```\n",
    "   - Paste your access token when prompted.\n",
    "   - The token will be saved in `~/.huggingface/token`, allowing persistent access without needing to re-enter it.\n",
    "\n",
    "##### Setting the Cache Directory\n",
    "- To avoid storing large models in your home directory, configure a cache directory on your scratch space:\n",
    "  ```bash\n",
    "  export HF_HOME=/scratch/username/huggingface\n",
    "  ```\n",
    "- Add this line to your .bashrc or .bash_profile to make it persistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative LLM Frameworks: When to Use Them\n",
    "\n",
    "While `transformers` is the primary library we’ll use in this workshop for GPU-based workflows, it’s worth briefly introducing two lightweight alternatives: **LlamaCpp** and **Ollama**. These tools are particularly useful in scenarios where GPUs aren't available or for quick, lightweight experiments.\n",
    "\n",
    "#### **LlamaCpp**\n",
    "- **What it is:** A lightweight, CPU-first library designed for running quantized versions of models like LLaMA.\n",
    "- **Why use it:** Efficient for local inference on CPU nodes or testing quantized models without GPU dependency.\n",
    "- **Key feature:** Extremely low memory footprint and no reliance on external frameworks.\n",
    "\n",
    "#### **Ollama**\n",
    "- **What it is:** A simple CLI tool and platform for running pre-trained models locally.\n",
    "- **Why use it:** Easy to use for prototyping, quick experiments, or exploring pre-packaged models.\n",
    "- **Key feature:** Integrated model management with minimal setup.\n",
    "\n",
    "### **Comparison Table: When to Use Each Framework**\n",
    "\n",
    "| Framework       | Best Use Case                                                                 | Strengths                             | Limitations                           |\n",
    "|------------------|-------------------------------------------------------------------------------|---------------------------------------|---------------------------------------|\n",
    "| **Transformers** | GPU-based training and inference on large-scale models in HPC environments.  | Extensive library, GPU acceleration, and flexibility for advanced workflows. | Requires GPUs and higher resource overhead. |\n",
    "| **LlamaCpp**     | CPU-based inference for small or quantized models in low-resource settings.  | Lightweight, runs efficiently on CPUs, no GPU dependency.                    | Slower for large-scale tasks, limited feature set. |\n",
    "| **Ollama**       | Simple, quick prototyping or local testing of pre-trained models.            | Easy-to-use CLI, minimal setup required.                                     | Less customizable, not designed for large-scale HPC workflows. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs Inference Workshop",
   "language": "python",
   "name": "llmsinferenceworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
