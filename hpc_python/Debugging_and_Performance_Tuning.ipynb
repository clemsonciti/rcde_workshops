{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging and Performance Tuning\n",
    "\n",
    "### Common Performance Bottlenecks in HPC Environments\n",
    "- **Memory limitations**: Insufficient memory allocation can cause jobs to fail. Ensure your SLURM script requests enough memory with `#SBATCH --mem`. Consider memory usage profiling to identify peak memory demands. Requesting too much memory can lead to inefficient use of cluster resources, while requesting too little may cause your job to fail. Tools like `jobstats` can help analyze memory usage.\n",
    "  ```bash\n",
    "    jobstats <job_id>\n",
    "  ```\n",
    "\n",
    "  OpenOnDemand provides a web-based interface for monitoring job statistics, including memory usage.\n",
    "\n",
    "- **I/O bottlenecks**: When reading or writing large files, poor I/O performance can limit overall job efficiency. Strategies like parallel I/O, caching frequently used data, and reducing the frequency of file access can help.\n",
    "  - **Parallel I/O libraries**: Use libraries like HDF5 with parallel I/O support to improve file read/write performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **I/O bottlenecks**: When reading or writing large files, poor I/O performance can limit overall job efficiency. Strategies like parallel I/O, caching frequently used data, and reducing the frequency of file access can help. \n",
    "\n",
    "    Text-based formats like CSV are particularly slow because each data point must be converted to text and parsed back, leading to high overhead.\n",
    "\n",
    "    - **Parallel I/O libraries**: Use libraries like HDF5 with parallel I/O support to improve file read/write performance. HDF5 stores data in a binary format, minimizing the overhead of serialization. Unlike CSV, which performs row-by-row writes and can grow quite large, HDF5 allows for chunked storage and direct memory mapping. This enables fast, scalable reads and writes, even for large, multidimensional datasets, without loading the entire file into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for parallel I/O with HDF5\n",
    "import h5py\n",
    "\n",
    "# Create a large dataset\n",
    "with h5py.File('data.h5', 'w') as f:\n",
    "    dset = f.create_dataset('dataset', (10000, 10000), dtype='f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset in parallel\n",
    "with h5py.File('data.h5', 'r', libver='latest', swmr=True) as f:\n",
    "    dset = f['dataset']\n",
    "    print(dset.shape)\n",
    "    print(dset[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timed comparison: Let's compare the performance of reading and writing data in CSV and HDF5 formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create toy data\n",
    "datasize = 5000\n",
    "data = np.random.rand(datasize, datasize)\n",
    "\n",
    "# Perform CSV read/write and measure total time\n",
    "csv_filename = 'data.csv'\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "start = time.time()\n",
    "df.to_csv(csv_filename, index=False)  # Write to CSV\n",
    "df_loaded = pd.read_csv(csv_filename)  # Read from CSV\n",
    "csv_total_time = time.time() - start\n",
    "del df, df_loaded\n",
    "\n",
    "print(f\"Total CSV Time (write + read): {csv_total_time:.2f} seconds\")\n",
    "\n",
    "# Perform HDF5 read/write and measure total time\n",
    "h5_filename = 'data.h5'\n",
    "\n",
    "start = time.time()\n",
    "with h5py.File(h5_filename, 'w') as f:  # Write to HDF5\n",
    "    f.create_dataset('dataset', data=data)\n",
    "\n",
    "with h5py.File(h5_filename, 'r') as f:  # Read from HDF5\n",
    "    h5_data = f['dataset'][:]\n",
    "h5_total_time = time.time() - start\n",
    "del h5_data\n",
    "del data\n",
    "\n",
    "print(f\"Total HDF5 Time (write + read): {h5_total_time:.2f} seconds\")\n",
    "\n",
    "# How much faster was HDF5?\n",
    "speedup = csv_total_time / h5_total_time\n",
    "print(f\"HDF5 was {speedup:.2f}x faster than CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools and Strategies for Debugging on the Cluster\n",
    "**Debugging tools**: Use available debugging tools such as `gdb` for C/C++ codes or Python debuggers like `pdb`. Slurm-specific commands (`squeue`, `jobstats`) can provide useful diagnostics for HPC jobs.\n",
    "  ```bash\n",
    "  squeue -u <username>\n",
    "  jobstats <job_id>\n",
    "  ```\n",
    "**SLURM job logs**: Check the output and error logs generated by SLURM for details on why a job might have failed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debugging Python code**: The `pdb` debugger can be useful for interactive debugging. Just import the `pdb` module and put `pdb.set_trace()` in your code where you want to start debugging. This will pause execution and drop you into the debugger prompt. Once you're there:\n",
    "\n",
    "- **Step through code**: Use `n` (next) to execute the next line of code. This allows you to walk through the function line by line.\n",
    "- **Step into functions**: Use `s` (step) to step into a function call. This is useful when you want to dive into the details of a function.\n",
    "- **Inspect variables**: Use `p` (print) followed by a variable name to inspect its value at the current point in the code. This helps identify unexpected values.\n",
    "- **Set breakpoints**: Use `b <line_number>` to set a breakpoint at a specific line. Execution will pause when the code reaches that point.\n",
    "- **Continue execution**: Use `c` (continue) to resume running the code until the next breakpoint or the end of the program.\n",
    "- **Exit the debugger**: Use `q` (quit) to exit the debugger when you’re done or if you’ve found the issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broken_function(data):\n",
    "    # This function attempts to compute the sum of each row in the input matrix.\n",
    "    row_sums = []\n",
    "    for row in data:\n",
    "        row_sums.append(sum(row))\n",
    "    # Introduce a bug: trying to index into a non-existent element\n",
    "    return row_sums[10000]  # Intentional out-of-bounds error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small toy dataset to trigger the error\n",
    "data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "broken_function(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to adding `pdb` setpoints in your code, you can use `pdb.run()` to start the debugger at a specific function call. This can be useful for debugging functions that are called from multiple places in your codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "# Set the breakpoint\n",
    "pdb.run('broken_function(data)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option in Jupyter is to use the `%debug` magic command. This will drop you into the debugger at the point where an exception was raised. You can then inspect variables, step through code, and identify the source of the error.\n",
    "\n",
    "This is nice when you're working in Jupyter anyway, but is not as powerful or as flexible as using `pdb` directly in a script or module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, **the most powerful debugging tool for Python is probably the combination of `pdb` and a good IDE like PyCharm or VS Code.** These tools provide a rich debugging environment with features like variable inspection, call stack visualization, and interactive breakpoints. You can use the VS Code debugger in an instance of VS Code on your local machine or using OpenOnDemand. See the [VS Code documentation](https://code.visualstudio.com/docs/editor/debugging) for more information on setting up and using the debugger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling tools\n",
    "Profiling is crucial for identifying performance bottlenecks.\n",
    "  - **cProfile**: A built-in Python module to profile code and identify slow functions.\n",
    "\n",
    "  ```python\n",
    "  # Example of profiling with cProfile\n",
    "  import cProfile\n",
    "\n",
    "  def your_function():\n",
    "      # Placeholder function for profiling\n",
    "      pass\n",
    "\n",
    "  cProfile.run('your_function()')\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example function that computes the sum of squares for a large array. We’ll profile this function to see how much time each part of the code takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cProfile\n",
    "\n",
    "# Sub-function 1: Generate data with unnecessary transformations\n",
    "def generate_data(size=1_000_000):\n",
    "    data = np.random.rand(size)\n",
    "    # Inefficient sorting operation to simulate overhead\n",
    "    sorted_data = sorted(data, reverse=True)\n",
    "    return np.array(sorted_data)\n",
    "\n",
    "# Sub-function 2: Compute the sum of squares (with multiple layers of computation)\n",
    "def sum_of_squares(data):\n",
    "    squares = [x**2 for x in data]  # First, create a list of squares\n",
    "    total_sum = sum(squares)  # Then, sum them up\n",
    "    return total_sum\n",
    "\n",
    "# Sub-function 3: Simulate a slow task with unnecessary work\n",
    "def simulate_slow_task():\n",
    "    total = 0\n",
    "    for i in range(10):\n",
    "        total += i % 3  \n",
    "        # Sleep for 0.1 seconds to simulate a slow computation\n",
    "        time.sleep(0.1)\n",
    "    return total\n",
    "\n",
    "# Main function that calls all components\n",
    "def complex_function():\n",
    "    data = generate_data(1_000_000)  # Generate data with overhead\n",
    "    result1 = sum_of_squares(data)   # Compute sum of squares\n",
    "    result2 = simulate_slow_task()   # Simulate slow task\n",
    "    return result1 + result2\n",
    "\n",
    "# Profile the complex function\n",
    "cProfile.run('complex_function()')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the cProfile Output\n",
    "\n",
    "#### Key Metrics:\n",
    "- **`ncalls`**: Number of calls to the function.\n",
    "- **`tottime`**: Time spent in the function, excluding sub-calls.\n",
    "- **`cumtime`**: Total time spent, including sub-calls.\n",
    "- **`percall`**: Time per call (`tottime/ncalls` or `cumtime/ncalls`).\n",
    "- **`filename:lineno(function)`**: Location of the function in your code.\n",
    "\n",
    "Use this workflow to quickly identify and optimize the slowest parts of your code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPC_ML",
   "language": "python",
   "name": "hpc_ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
