{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Acceleration with Python\n",
    "\n",
    "### GPU vs. CPU: When and Why to Use GPUs  \n",
    "- **Understanding the differences**:  \n",
    "  - A **CPU (Central Processing Unit)** focuses on versatility and efficiency with a few powerful cores optimized for **sequential** execution and complex tasks. It excels at operations requiring decision-making, such as control flow or branching.  \n",
    "  - In contrast, a **GPU (Graphics Processing Unit)** contains **thousands of smaller cores** designed to perform **many tasks simultaneously**. Its strength lies in **parallel execution**, which makes it ideal for workloads that involve processing large datasets with **repetitive operations**, like matrix multiplications or element-wise computations.\n",
    "\n",
    "  <img src=\"imgs/gpu.png\" alt=\"GPU Image\" width=\"60%\"/>\n",
    "\n",
    "- **Use cases for GPUs**:  \n",
    "  - **Training neural networks**: Deep learning frameworks, such as TensorFlow and PyTorch, leverage GPUs to train models faster by distributing operations over many cores.  \n",
    "  - **Data analysis and scientific computing**: GPUs shine when dealing with large datasets that require linear algebra operations, such as **matrix inversions** or **eigenvalue decompositions**.  \n",
    "  - **Image and video processing**: Tasks like image classification, object detection, and video encoding rely on pixel-level operations, which map well to GPU architectures.\n",
    "\n",
    "- **Limitations of GPUs**:  \n",
    "  - GPUs **struggle with tasks that are inherently sequential**, such as algorithms with many conditional branches or unpredictable memory access patterns.  \n",
    "  - Some workloads, such as database queries or text processing, may not benefit from GPUs because these tasks are **I/O-bound** or **latency-sensitive** and require more control logic than raw parallelism.\n",
    "\n",
    "- **Performance considerations**:  \n",
    "  - While GPUs can massively accelerate computation, **data transfer between CPU and GPU memory** can become a **bottleneck**. Minimizing these transfers is essential for performance.  \n",
    "  - **On-GPU computation** should be maximized by batching operations and keeping as much data in GPU memory as possible. Use libraries that **optimize memory management**, such as CUDA for Nvidia GPUs or ROCm for AMD GPUs, to avoid unnecessary overhead.\n",
    "\n",
    "### Introduction to GPU Libraries\n",
    "#### CuPy\n",
    "A library that provides GPU acceleration for NumPy operations by using **CUDA**. CUDA (Compute Unified Device Architecture) is a parallel computing platform and API developed by Nvidia that allows developers to run code directly on GPUs. CuPy uses CUDA to allow you to accelerate existing Python scripts with minimal code changes.  \n",
    "\n",
    "**Example use-case**: Consider a research problem involving large-scale linear algebra computations, such as solving systems of equations or performing matrix factorization. CuPy can be used to accelerate these operations by utilizing the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a simple operation in numpy\n",
    "import numpy as np\n",
    "# Perform non-GPU-accelerated array operations\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "result = np.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform same operations as in numpy but with cupy\n",
    "import cupy as cp\n",
    "# Perform GPU-accelerated array operations\n",
    "x = cp.array([1, 2, 3])\n",
    "y = cp.array([4, 5, 6])\n",
    "result = cp.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the result\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did it really land on the GPU? Let's check\n",
    "result.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timed comparison**: CuPy vs. NumPy for matrix multiplication. CuPy can be much faster if your code requires a lot of matrix operations.\n",
    "\n",
    "We can see this by trying out matrix multiplication of a 16000x16000 matrix with CuPy and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy (CPU) computation\n",
    "start = time.time()\n",
    "x_cpu = np.random.rand(16000, 16000)\n",
    "result_cpu = np.dot(x_cpu, x_cpu)\n",
    "end = time.time()\n",
    "cpu_time = end - start\n",
    "print(f\"CPU Time: {cpu_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CuPy (GPU) computation\n",
    "start = time.time()\n",
    "x_gpu = cp.random.rand(16000, 16000)\n",
    "result_gpu = cp.dot(x_gpu, x_gpu)\n",
    "end = time.time()\n",
    "gpu_time = end - start\n",
    "print(f\"GPU Time: {gpu_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much faster was the GPU?\n",
    "speedup = cpu_time / gpu_time\n",
    "print(f\"GPU was {speedup} times faster than CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to GPU Libraries  \n",
    "#### PyTorch  \n",
    "A deep learning framework that provides GPU acceleration and automatic differentiation for building and training neural networks.   \n",
    "\n",
    "**When to use CuPy vs. PyTorch**: Use **CuPy** for general-purpose **GPU-accelerated array operations** (like large-scale linear algebra or scientific computing). In contrast, **PyTorch** is more appropriate when working with **machine learning models** and **neural networks** that require features like automatic differentiation, model training loops, and GPU optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Even better: let's throw an error if GPU is not available\n",
    "assert device.type == \"cuda\", \"GPU not available!\"\n",
    "\n",
    "\n",
    "# Define a tensor\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "# Move tensor to GPU\n",
    "tensor = tensor.to(device)\n",
    "# Perform operations on GPU\n",
    "result = tensor * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training neural networks:** PyTorch makes it easy to move models and data between CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(10, 1).to(device)\n",
    "input_data = torch.randn(5, 10).to(device)\n",
    "output = model(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timed comparison:** Below is a placeholder for comparing the execution time of training a simple model on CPU versus GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "input_dim = 100000\n",
    "num_training_examples = 10000\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU Training\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "model_cpu = torch.nn.Linear(input_dim, output_dim).to(device_cpu)\n",
    "optimizer_cpu = torch.optim.SGD(model_cpu.parameters(), lr=0.01)  # Add optimizer\n",
    "input_cpu = torch.randn(num_training_examples, input_dim).to(device_cpu)\n",
    "target_cpu = torch.randn(num_training_examples, output_dim).to(device_cpu)  # Dummy target\n",
    "loss_fn = torch.nn.MSELoss()  # Define loss function\n",
    "\n",
    "start = time.time()\n",
    "optimizer_cpu.zero_grad()  # Zero gradients\n",
    "output_cpu = model_cpu(input_cpu)  # Forward pass\n",
    "loss = loss_fn(output_cpu, target_cpu)  # Compute loss\n",
    "loss.backward()  # Backward pass (compute gradients)\n",
    "optimizer_cpu.step()  # Update weights\n",
    "end = time.time()\n",
    "cpu_train_time = end - start\n",
    "print(f\"CPU Training Time: {cpu_train_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Training\n",
    "device_gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_gpu = torch.nn.Linear(input_dim, output_dim).to(device_gpu)\n",
    "optimizer_gpu = torch.optim.SGD(model_gpu.parameters(), lr=0.01)\n",
    "input_gpu = torch.randn(num_training_examples, input_dim).to(device_gpu)\n",
    "target_gpu = torch.randn(num_training_examples, output_dim).to(device_gpu)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "start = time.time()\n",
    "optimizer_gpu.zero_grad()\n",
    "output_gpu = model_gpu(input_gpu)\n",
    "loss = loss_fn(output_gpu, target_gpu)\n",
    "loss.backward()\n",
    "optimizer_gpu.step()\n",
    "end = time.time()\n",
    "gpu_train_time = end - start\n",
    "print(f\"GPU Training Time: {gpu_train_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much faster was the GPU?\n",
    "speedup = cpu_train_time / gpu_train_time\n",
    "print(f\"GPU was {speedup} times faster than CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Python Code on GPUs via SLURM\n",
    "**SLURM basics for GPU jobs**: To leverage GPUs on Palmetto, you need to request GPUs in your SLURM job script. Specify the number of GPUs required using the `--gpus` option, as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=gpu_job           # Job name\n",
    "#SBATCH --time=01:00:00              # Time limit hrs:min:sec\n",
    "#SBATCH --mem=8G                     # Memory required per node\n",
    "#SBATCH --gpus v100:1                # Request 1 V100 GPU\n",
    "#SBATCH --cpus-per-task=4            # Request 4 CPU cores per task\n",
    "\n",
    "# Load necessary modules\n",
    "module load anaconda3\n",
    "module load cuda\n",
    "\n",
    "# Run the Python script\n",
    "python your_script.py\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resource considerations**: Ensure your code properly manages GPU resources, especially when using multiple GPUs, to avoid resource contention. Use tools like `nvidia-smi` in the terminal to monitor GPU utilization.\n",
    "  ```bash\n",
    "  # Check GPU usage\n",
    "  nvidia-smi\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-GPU programming**: For workloads that can leverage multiple GPUs, libraries like PyTorch and TensorFlow provide easy-to-use APIs for distributing computations across multiple GPUs. \n",
    "\n",
    "PyTorch’s `DataParallel` splits input data across the specified GPUs, replicates the model on each one, and runs parallel computations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple linear model\n",
    "model = nn.Linear(10, 2)  # Input size 10, output size 2\n",
    "\n",
    "# Wrap the model with DataParallel to use both GPUs (IDs 0 and 1)\n",
    "model = nn.DataParallel(model, device_ids=[0, 1]).cuda()\n",
    "\n",
    "# Create dummy input data: batch of 64 samples, each with 10 features\n",
    "x = torch.randn(64, 10).cuda()\n",
    "\n",
    "# Perform a forward pass with the input data\n",
    "output = model(x)\n",
    "\n",
    "# Print the output to verify multi-GPU execution\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model is too large to fit on a single GPU, you’ll need to use model parallelism instead of DataParallel. In model parallelism, different layers or parts of the model are distributed across multiple GPUs, allowing each GPU to hold a portion of the model’s parameters. PyTorch supports this through manual partitioning or torch.distributed APIs, though it requires more complex coding compared to DataParallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPC_ML",
   "language": "python",
   "name": "hpc_ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
