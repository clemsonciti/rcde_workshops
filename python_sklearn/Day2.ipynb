{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've assumed that we already have access to a dataset that is ready to be used for training a machine learning model. However, in practice, this is rarely the case. Most of the time, the dataset will need to be cleaned, transformed, and prepared before it can be used for training.\n",
    "\n",
    "**Stages of data preparation:**\n",
    "1. Data collection\n",
    "2. Data loading\n",
    "3. Data exploration\n",
    "4. Data cleaning\n",
    "5. Feature selection and engineering\n",
    "7. Encoding categorical variables\n",
    "8. Feature scaling\n",
    "9. Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Collection\n",
    "\n",
    "Data collection is the process of gathering data from various sources. The data can be collected from databases, files, APIs, web scraping, etc. Outside of the scope of this notebook, but it's an important step in the data preparation process.\n",
    "\n",
    "### 1.2 Data Loading\n",
    "\n",
    "In this step, we load the data into the working environment. Your data might be in a CSV file, a JSON file, a SQL database, or any other format. We will use the `pandas` library to load the data into a DataFrame. Let's use the \"Heart Failure Clinical Records\" dataset from the UCI Machine Learning Repository. This dataset contains the medical records of patients who had heart failure, collected during their follow-up period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're lucky in this dataset, in that it is small enough to fit into memory. For larger datasets, we would need to use more advanced techniques to load the data in chunks. For example, we might use Dask rather than pandas, which would allow us to work with larger-than-memory datasets. (Dask is designed to be a drop-in replacement for pandas for larger-than-memory datasets, so the code would look very similar.)\n",
    "\n",
    "For very large datasets, when performing data exploration and when producing the code that will be used to prepare your data, you might want to work with a sample of the data rather than the full dataset. This will allow you to iterate more quickly and avoid long wait times. Once you have the code working with a sample, you can then run it on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load only a subset of the dataset\n",
    "small_df = pd.read_csv(url, nrows=20)\n",
    "\n",
    "# Display the shape of the datasets\n",
    "print(df.shape)\n",
    "print(small_df.shape)\n",
    "\n",
    "# We'll work with the full dataset, so let's delete the small_df\n",
    "del small_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Exploration\n",
    "\n",
    "Data exploration is the process of getting to know the data. We look at the structure of the data, the summary statistics, and the distribution of the data. We also look for missing values, outliers, and anomalies in the data. This step is crucial for understanding the data and making decisions about how to clean and transform it.\n",
    "\n",
    "It cannot be overemphasized that there is no one-size-fits-all approach to data exploration. The process will depend on the dataset, the problem you are trying to solve, and the questions you are trying to answer. A thorough understanding of the data -- its sources, its structure, its quality -- is essential for building a successful machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some basic information about the dataset\n",
    "\n",
    "# Display the column names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were really working with this dataset for research purposes, you should know what each of these columns represents, as well as the units in which they are measured. That knowledge is crucial both for knowing how best to make use of the data, as well as for detecting problems in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(df.info())\n",
    "print(\"\\nSample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real data is very clean already. Let's introduce some missing values, and also some outliers.\n",
    "# We'll do this by randomly selecting some values and setting them to NaN, and also by adding some random noise to some values.\n",
    "\n",
    "# Randomly select 10% of the data and set them to NaN\n",
    "df_nan = df.copy()\n",
    "nan_indices = np.random.choice(df.index, size=int(len(df)*0.1), replace=False)\n",
    "df_nan.loc[nan_indices, 'age'] = np.nan\n",
    "df_nan.loc[nan_indices, 'serum_creatinine'] = np.nan\n",
    "df_nan.loc[nan_indices, 'ejection_fraction'] = np.nan\n",
    "\n",
    "# Randomly select 2% of the data and make them outliers\n",
    "df_noisy = df_nan.copy()\n",
    "noisy_indices = np.random.choice(df.index, size=int(len(df)*0.02), replace=False)\n",
    "df_noisy.loc[noisy_indices, 'serum_creatinine'] = df_noisy.loc[noisy_indices, 'serum_creatinine'] * 10\n",
    "df_noisy.loc[noisy_indices, 'ejection_fraction'] = df_noisy.loc[noisy_indices, 'ejection_fraction'] * 10\n",
    "\n",
    "# Display the first few rows of the noisy dataset\n",
    "print(\"\\nNoisy dataset:\")\n",
    "df_noisy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_noisy.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that our target variable is the \"DEATH_EVENT\" column, which indicates whether the patient died during the follow-up period. We will explore the data to understand the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 2))\n",
    "df_noisy['DEATH_EVENT'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Death Events')\n",
    "plt.xlabel('Death Event')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Survived', 'Died'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Percentage of deaths:\", (df['DEATH_EVENT'].sum() / len(df)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the age distribution across the dataset. While we're at it, let's break it down by our target variable, `DEATH_EVENT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "sns.histplot(data=df_noisy, x='age', hue='DEATH_EVENT', kde=True, multiple=\"stack\")\n",
    "plt.title('Age Distribution by Outcome')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(\"Average age of survivors:\", df_noisy[df_noisy['DEATH_EVENT'] == 0]['age'].mean())\n",
    "print(\"Average age of non-survivors:\", df_noisy[df_noisy['DEATH_EVENT'] == 1]['age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very frequently, a correlation heatmap is a good way to get a quick overview of the relationships between the features in the dataset. Note that this assumes that the features are all numeric. If you have categorical features, you will need to encode them as numbers before you can use a correlation heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(df_noisy.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to look at scatterplots to see if you pick up on any patterns in the data (or verify that patterns you expect to be there are really there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "sns.scatterplot(data=df_noisy, x='ejection_fraction', y='serum_creatinine', hue='DEATH_EVENT')\n",
    "plt.title('Ejection Fraction vs. Serum Creatinine')\n",
    "plt.xlabel('Ejection Fraction (%)')\n",
    "plt.ylabel('Serum Creatinine (mg/dL)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What! There are some datapoints that look like bad outliers, before we even check what they are. And in this case, they are percentages that are well above 100%. Something is definitely wrong with those datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll check for and handle outliers. We need to be thoughtful about this step! Every choice we make says something about how we expect the future data to look, and what we think is the reason why we have outliers in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df_noisy[(df_noisy[column] < lower_bound) | (df_noisy[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Example: Check outliers in 'serum_creatinine'\n",
    "creatinine_outliers = detect_outliers(df_noisy, 'serum_creatinine')\n",
    "print(\"Outliers in serum_creatinine:\")\n",
    "print(creatinine_outliers[['age', 'sex', 'serum_creatinine', 'DEATH_EVENT']])\n",
    "\n",
    "# Remove outliers (be cautious with this step in real-world scenarios!)\n",
    "df_cleaned = df_noisy[~df_noisy.index.isin(creatinine_outliers.index)]\n",
    "\n",
    "print(\"\\nDataset shape after cleaning:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we must handle missing values in a way that is thoughtful about why the values might be missing. Are they missing at random? Are they missing because they are not applicable? Are they missing because they were never recorded? The answers to these questions will affect how we handle the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "\n",
    "# Fill missing values with the mean\n",
    "df_cleaned = df_cleaned.fillna(df_cleaned.mean())\n",
    "\n",
    "# Alternatively, fill missing values with the median\n",
    "# df_imputed = df_cleaned.fillna(df_cleaned.median())\n",
    "\n",
    "# Or, if we're dealing with categorical data, we can fill missing values with the mode\n",
    "# df_imputed = df_cleaned.fillna(df_cleaned.mode().iloc[0])\n",
    "\n",
    "# We could also fit a machine learning model to predict missing values, but this is more complex and not always necessary.\n",
    "\n",
    "# Or, just drop the missing rows. Which strategy makes the most sense depends crucially on your particular problem! No one-size-fits-all solutions.\n",
    "\n",
    "# Check if there are any missing values left\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Selection and Engineering\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features for use in model training. Feature engineering is the process of creating new features from the existing features in the dataset. Both of these processes are crucial for building a successful machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age groups\n",
    "df_cleaned['age_group'] = pd.cut(df_cleaned['age'], bins=[30, 50, 70, 100], labels=['Middle-aged', 'Senior', 'Elderly'])\n",
    "\n",
    "# Create a feature for multiple conditions\n",
    "df_cleaned['multiple_conditions'] = ((df_cleaned['diabetes'] + df_cleaned['high_blood_pressure'] + df_cleaned['sex'] + df_cleaned['smoking']) > 1).astype(int)\n",
    "\n",
    "# Log transform skewed features\n",
    "df_cleaned['log_creatinine'] = np.log1p(df_cleaned['creatinine_phosphokinase'])\n",
    "\n",
    "# Interaction terms\n",
    "df_cleaned['ef_creatinine_interaction'] = df_cleaned['ejection_fraction'] * df_cleaned['serum_creatinine']\n",
    "\n",
    "print(\"New features added:\")\n",
    "print(df_cleaned[['age_group', 'multiple_conditions', 'log_creatinine', 'ef_creatinine_interaction']].head())\n",
    "\n",
    "# Visualize the effect of a new feature\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='age_group', y='serum_creatinine', hue='DEATH_EVENT', data=df_cleaned)\n",
    "plt.title('Serum Creatinine by Age Group and Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Encoding Categorical Variables\n",
    "\n",
    "Most machine learning algorithms require that the input data be in numerical format. If the dataset contains categorical variables, we need to encode them into numerical format. One issue to which to be particularly sensitive is whether it is appropriate to treat a categorical variable as ordinal or nominal. If it is ordinal, then we should encode it as such. If it is nominal, then we should use \"one-hot encoding\".\n",
    "\n",
    "For example, suppose that our \"age group\" column were the only information about age that we have. If we treat it as ordinal, then we are saying that the different age groups are ordered in some way. If we treat it as nominal, then we are saying that the different age groups are not ordered in any way. Which seems appropriate here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the `age_group` feature\n",
    "df_encoded = df_cleaned.copy()\n",
    "df_encoded['age_group_ordinal'] = df_cleaned['age_group'].cat.codes\n",
    "\n",
    "# One-hot encode the `age_group` feature\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['age_group'], drop_first=False)\n",
    "\n",
    "df_encoded[[col for col in df_encoded.columns if 'age_group' in col]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Feature Scaling\n",
    "\n",
    "Feature scaling is the process of standardizing the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. This is helpful for algorithms that rely on the magnitude of values, such as distance-based algorithms. Even when it doesn't help the algorithm, it rarely hurts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df_cleaned.copy()\n",
    "df_scaled[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time', 'log_creatinine', 'ef_creatinine_interaction']] = scaler.fit_transform(df_encoded[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time', 'log_creatinine', 'ef_creatinine_interaction']])\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Data Splitting\n",
    "\n",
    "On day 1 we discussed train/test splits and cross-validation. This is the final step of data preparation, and is often integrated into our model training/tuning process, especially when we are using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scripting your code\n",
    "\n",
    "Once you have drafted the code to prepare your data, split it and train/evaluate your model, you should package your code into a script that you can run via SLURM.\n",
    "\n",
    "This will allow you to submit your job to the cluster and have it run asynchronously, without needing to keep your notebook open. It will also allow you to run your code on a larger dataset, or with more iterations, than you could do interactively.\n",
    "\n",
    "What is different about running your code as a script vs. in a notebook? You need to ensure each of the following:\n",
    "- Your code runs from top to bottom without needing any manual intervention\n",
    "- Your code load the full dataset, not just a sample\n",
    "- Your code checks for unexpected conditions and handles them gracefully\n",
    "- Your code logs information about what it is doing, so you can debug it later if needed\n",
    "- For long jobs, your code checkpoints its progress so that it can resume where it left off if it is interrupted\n",
    "- Make sure any desired outputs (plots, model files, etc.) are saved to disk (rather than merely displayed in the notebook)\n",
    "- Figure out what resources you need to request\n",
    "\n",
    "Let's look at a couple of ways of converting code you developed in Jupyter into a SLURM-submittable script, and then dive into some of the above considerations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running your notebook directly as a script\n",
    "\n",
    "Probably the easiest route to converting your notebook is just to run it directly as a job on the cluster. This is possible with the command:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to notebook --execute --inplace [notebook_filename].ipynb\n",
    "```\n",
    "\n",
    "where you would replace `notebook_filename` with whatever your notebook filename is. In a full SLURM script, this command might appear as follows:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name my-job-name\n",
    "#SBATCH --nodes 1\n",
    "#SBATCH --cpus-per-task 4\n",
    "#SBATCH --gpus-per-node v100:1\n",
    "#SBATCH --mem 8gb\n",
    "#SBATCH --time 08:00:00\n",
    "\n",
    "module load anaconda\n",
    "\n",
    "cd /path/to/your/notebook\n",
    "\n",
    "jupyter nbconvert --to notebook --execute --inplace [notebook_filename].ipynb\n",
    "```\n",
    "\n",
    "Notice that in this case I've determined that my code can make use of 4 cores each on 2 nodes, as well as a V100 GPU on each node. I've also requested 8GB of memory and 8 hours of runtime. You should adjust these values based on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting your notebook to a script\n",
    "\n",
    "The preferred coding practice would be to convert your notebook into a script yourself. If you have worked with Jupyter notebooks but not with .py scripts, you can think of the latter as being one big cell in a notebook. In fact, you can even make sure your code runs in a single Jupyter cell (including checkpoints, logging, etc.), and then simply copy that cell into a .py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpointing\n",
    "\n",
    "If your code is going to take a long time to run, you should consider checkpointing it. This means saving the state of your code at regular intervals, so that if it is interrupted, you can resume from the last checkpoint rather than starting over from the beginning. \n",
    "\n",
    "What exactly this looks like will depend on what you are doing. If you are searching over possible hyperparameters to find the best ones, then you should keep track of which hyperparameters you have tried and what the results were. See the below toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "import os\n",
    "\n",
    "X, y = np.random.rand(1000, 5), np.random.randint(0, 2, 1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define hyperparameters to test\n",
    "params = [(10, 5), (10, None), (50, 5), (50, None)]\n",
    "\n",
    "# Define the results file, where we'll save the information about which hyperparameters we've already tested\n",
    "results_file = 'results.csv'\n",
    "\n",
    "# Load existing results we've already computed\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        done = set(tuple(row[:2]) for row in csv.reader(f))\n",
    "else:\n",
    "    done = set()\n",
    "\n",
    "# Train models and save results for each set of hyperparameters not already tested\n",
    "with open(results_file, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    if not done:\n",
    "        writer.writerow(['n_estimators', 'max_depth', 'accuracy'])\n",
    "    for n_estimators, max_depth in params:\n",
    "        if (str(n_estimators), str(max_depth)) not in done:\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "            accuracy = accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test))\n",
    "            writer.writerow([n_estimators, max_depth, accuracy])\n",
    "            print(f\"n_estimators={n_estimators}, max_depth={max_depth}, accuracy={accuracy:.4f}\")\n",
    "\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a long training run, we might want to save the model at regular intervals. This is especially important if we are training a model that takes a long time to train, or if we are training on a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network model\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = NNModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Make some random data\n",
    "x = torch.randn(10000, 10)  # Input data\n",
    "y = torch.randn(10000, 1)  # Target data\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    loss = nn.functional.mse_loss(output, y)\n",
    "    \n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track best model\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_model = model.state_dict()\n",
    "        print(f\"New best model found with loss: {best_loss}\")\n",
    "        torch.save({\n",
    "            'model': best_model,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': best_loss\n",
    "        }, 'best_model.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': loss.item()\n",
    "        }, f'checkpoint_{epoch+1}.pt')\n",
    "\n",
    "print(\"Training complete. Best model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resource allocation\n",
    "\n",
    "In order to effectively run your code on the cluster, you need to request the appropriate resources. This includes the number of nodes, the number of cores per node, the amount of memory, the amount of time, and the type of GPU.\n",
    "\n",
    "**When multiple cores help:**\n",
    "- When you are running multiple independent jobs\n",
    "- When you are running a single job that can be parallelized (check the documentation!)\n",
    "- When you are running a single job that can be parallelized, but the parallelization is not built into the code (e.g. you are running multiple instances of the code with different hyperparameters)\n",
    "\n",
    "**When multiple cores don't help:**\n",
    "- When you are running a single job that cannot be parallelized\n",
    "- When you haven't written your code to take advantage of multiple cores\n",
    "\n",
    "**When a GPU helps:**\n",
    "- When you are running a deep learning model\n",
    "- When you are running a model that can be accelerated by a GPU (check the documentation!)\n",
    "\n",
    "**When a GPU doesn't help:**\n",
    "- When you are running a model that is not accelerated by a GPU\n",
    "- When you haven't written your code to take advantage of a GPU\n",
    "\n",
    "**How much memory to request:**\n",
    "- This depends on the size of your dataset and the size of your model, as well as whether you are using a GPU. E.g., if you are using a large language model, you might need a big GPU but not much memory. If you are using a large dataset, you might need a lot of memory but not a big GPU.\n",
    "\n",
    "Be sure to use the `jobperf` command to check how much memory your job is using. E.g., `watch -n 2 jobperf [jobid]` will show you how much memory your job is using every 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLWorkshop",
   "language": "python",
   "name": "rapids_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
