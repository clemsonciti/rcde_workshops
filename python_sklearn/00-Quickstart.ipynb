{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b8ec82-9fd4-4689-bb7a-9d4e148ebf4d",
   "metadata": {},
   "source": [
    "# Machine Learning in Python using Clemson High Performance Computing\n",
    "\n",
    "**Instructor:** Carl Ehrett\n",
    "\n",
    "**Email:** cehrett@clemson.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e4b7f-83ed-46e1-9697-49e6d823830e",
   "metadata": {},
   "source": [
    "## 1. Welcome and Overview\n",
    "\n",
    "In this workshop, we will introduce the basics of machine learning using Python. We will focus on machine learning using \"tabular data\" (i.e. spreadsheet-style data), as opposed to images or unstructured text, though most of what we talk about will also apply to those domains. Image-related ML and generative AI both typically use deep learning neural networks which are not the focus of this workshop, but will be the focuses of other upcoming CCIT workshops. In this workshop, we will emphasize the use of Clemson's Palmetto Cluster for running machine learning algorithms on large datasets. We will cover the following topics:\n",
    "* What is machine learning?\n",
    "* What are some of the python tools that facilitate machine learning?\n",
    "* What are the different types of machine learning?\n",
    "* What are some of the common machine learning algorithms?\n",
    "* How do we evaluate the performance of machine learning algorithms?\n",
    "* How do we explore and clean data?\n",
    "* How do we prepare data for machine learning?\n",
    "* How do we make use of Clemson's Palmetto Cluster to efficiently run our machine learning code?\n",
    "* How can we run code that is too complex, or use data that is too large, for a Jupyter notebook?\n",
    "* What sorts of Palmetto resources should we request to allocate for our machine learning jobs?\n",
    "\n",
    "### 1.1 Getting started\n",
    "You can download this notebook and its contents as follows.\n",
    "\n",
    "In the terminal, run the following command: `wget https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/python_sklearn/download.sh`\n",
    "\n",
    "This copies to your drivespace a script `download.sh` that, when run, will copy the full workshop files to your drivespace. So now that you have that script, run the command: `bash download.sh`. You should now have a folder, `python_sklearn`, which contains this notebook and the rest of the workshop.\n",
    "\n",
    "You can run most of this notebook using the default kernel, though some of the code cells will only run if you have created an environment with specialized libraries installed.\n",
    "\n",
    "### 1.2 What is machine learning?\n",
    "\n",
    "People use the term \"machine learning\" in a variety of ways. Some people use it more or less synonymously with \"artificial intelligence.\" And these days, AI does indeed usually work under the paradigm of machine learning. But \"machine learning\" refers to the use of algorithms to learn from data. The contrast here is with traditional programming, where a programmer writes code that tells the computer exactly what to do. In machine learning, the programmer writes code that tells the computer how to learn from data to make decisions.\n",
    "\n",
    "![ML vs traditional](img/ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4106be-6e1a-4192-b6b2-bf1cda4a35bd",
   "metadata": {},
   "source": [
    "## 2. Setting up the Environment\n",
    "\n",
    "### 2.1 Creating a Conda Environment\n",
    "Why we use conda for ML environments:\n",
    "* Simplified package management and dependency resolution\n",
    "* Easy creation and management of isolated environments\n",
    "* Cross-platform compatibility (Windows, macOS, Linux)\n",
    "* Support for multiple programming languages (not just Python)\n",
    "* Ability to specify and replicate exact environment configurations\n",
    "* Large repository of pre-built packages optimized for different systems\n",
    "```bash\n",
    "# Commands for creating and activating a conda environment\n",
    "conda create -n hpc_ml -c rapidsai -c conda-forge -c nvidia cudf cuml numpy pandas scikit-learn matplotlib seaborn rapids jupyterlab python=3.11 'cuda-version>=12.0,<=12.5'\n",
    "# Warning: the above command may take awhile! ~20 minutes. Next:\n",
    "source activate hpc_ml\n",
    "```\n",
    "\n",
    "### 2.2 Registering as a jupyter kernel\n",
    "In addition to installing JupyterLab, we need to register our environment as a Jupyter kernel in order for it to show up as an option for us when running a notebook.\n",
    "```bash\n",
    "# Register the env as a kernel\n",
    "python -m ipykernel install --user --name hpc_ml --display-name \"HPC_ML\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53369bf4-6024-4730-b9c8-ba341b9c7f31",
   "metadata": {},
   "source": [
    "## 3. Example end-to-end ML: [Forest Covertypes Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype)\n",
    "\n",
    "First, we will use the Forest Covertypes dataset, which is a dataset that contains information about the forest cover type in the Roosevelt National Forest of northern Colorado. The dataset contains 581,012 samples and 54 features. The goal is to predict the forest cover type based on the cartographic features provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768928a2-0861-4160-b2dd-d93256f30acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "cov_type = fetch_covtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d4034-06e9-4207-a803-926173527ee2",
   "metadata": {},
   "source": [
    "As we should always do with any dataset we're working with, we should poke around it a bit to see what it looks like. It's always good to know what is the datatype of the object we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c120dc-e1b5-4610-9764-a09a8cd7b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cov_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67174031-3f52-40f7-a24b-405164c33dc6",
   "metadata": {},
   "source": [
    "In this case we've got a scikit-learn \"Bunch\" object. A quick google search shows us this page of documentation: [sklearn.utils.Bunch Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html), where we learn that a Bunch is a dictionary-like object that exposes keys as attributes. So, let's see what keys are in this Bunch object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651e822-2ba8-41e9-a029-cf0420caca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31098619-58d6-42e3-9930-0222fe674c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cov_type.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40e581-f615-4b29-9fda-8c0ecd235f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783645bf-deeb-4b14-935f-a9f433615fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6e4aa-b98c-404b-bb11-37cc3fea08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd12d334-3c4f-4cbf-85fe-dcf4916e9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21b4d6-c9a5-421e-b565-83daa014c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc8a04-0c3d-452e-a2a8-2a801b20b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cov_type.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39b142-46d9-4c4a-adae-bd29879f1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10ab1c-23fc-43cf-b298-132be2ef8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319bcf6-3dd9-433a-95b8-bc229e392fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame using the feature names and data from cov_type\n",
    "df_cov_type = pd.DataFrame(data=cov_type.data, columns=cov_type.feature_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the Forest Covertypes Dataset:\")\n",
    "df_cov_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749127a-861d-4181-af46-3393360bcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_type[[col for col in df_cov_type.columns if 'Soil_Type' in col]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0e1b7-9468-4a4d-9700-af6a5dd428d9",
   "metadata": {},
   "source": [
    "Let's use the K-nearest neighbors algorithm to try to predict covertype using the information contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4aba26-4683-44d9-b15f-dd268ad6bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X, y = cov_type.data, cov_type.target\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26e286-b844-41e4-9587-e0098ef4f419",
   "metadata": {},
   "source": [
    "Now it's time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49595b-651e-4128-9927-7c77bd7815a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the classifier\n",
    "kn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "kn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81dc3b-327c-4666-9426-a3dcf7047c9b",
   "metadata": {},
   "source": [
    "Now let's see how the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1bfbf-dbea-4f04-a3f6-0d5758f00f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Start a timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = kn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get unique class labels\n",
    "unique_labels = np.unique(y)\n",
    "target_names = [f\"Class {label}\" for label in unique_labels]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# See how long that took\n",
    "print(f\"\\nTotal time taken: {time.time() - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e4812-023c-44ba-9855-943e9910ca2e",
   "metadata": {},
   "source": [
    "The above code works fine, but takes a long time to run! On the order of 5-10 minutes, depending on how fast your machine is. And that's not even bad, as far as what machine learning requires.\n",
    "\n",
    "Now let's look at a version of the same machine learning approach that makes better use of the resources available to us -- using both parallelization across cores and also using the GPU we've provisioned to speed things up.\n",
    "\n",
    "First, we'll load the data and prepare it for the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98353691-c26c-4ade-a8df-6f4acad0f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml import KNeighborsClassifier as cuKNN\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Initialize Dask client for distributed computing\n",
    "n_workers = 4  # Adjust based on your HPC resources\n",
    "cluster = LocalCluster(n_workers=n_workers)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names)\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f12db4-3ade-4561-9f74-84e8fd4485c6",
   "metadata": {},
   "source": [
    "Now we fit the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076fca0-bdb0-4f54-92fa-a9cd9fd79767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the classifier\n",
    "kn_classifier_cuml = cuKNN(n_neighbors=5)\n",
    "kn_classifier_cuml.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf6467-a2b5-47d6-b069-7ae29fa1b590",
   "metadata": {},
   "source": [
    "Now let's see how this model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfc3f8-1f14-4e66-8309-a7e137a3e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = kn_classifier_cuml.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test.to_cupy().get(), y_pred.to_cupy().get())\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert predictions to numpy for classification report\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_pred_np = y_pred.to_numpy()\n",
    "\n",
    "# Get unique class labels\n",
    "unique_labels = np.unique(y_test_np)\n",
    "target_names = [f\"Class {label}\" for label in unique_labels]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_np, y_pred_np, target_names=target_names))\n",
    "\n",
    "# Clean up\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# See how long that took\n",
    "print(f\"\\nTotal time taken: {time.time() - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97847e89-859f-453a-bd42-485da24c1964",
   "metadata": {},
   "source": [
    "Notice how much faster inference is in the second case! And this is just a simple case of getting predictions for 116k samples. Imagine if we needed to get predictions for millions of samples. In that case, the second approach would be much more feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbae271-e29e-4c72-8859-f7fece26c284",
   "metadata": {},
   "source": [
    "## 4. Hyperoptimization: search for the best version of the model to maximize performance\n",
    "\n",
    "In the above example, I used a KNN model with a k value of 5. But how do we know that k=5 is the best value? We don't. We need to search for the best value of k. This is an example of hyperoptimization. You can think of hyperoptimization as a search for the best version of the model to maximize performance. \n",
    "\n",
    "Hyperoptimization is inherently computationally expensive. It involves training many models with different hyperparameters and evaluating their performance. This is a perfect use case for the Palmetto 2 Cluster. We can use the cluster to train many models in parallel, which will speed up the hyperoptimization process. And if we submit our hyperoptimization job as a batch job, then we don't need to tie up our local machine for hours -- or days! -- while the hyperoptimization process runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29f643-46ed-464e-b1aa-3a83dc7ae877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.metrics import accuracy_score\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing and ensure float32 dtype\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names).astype('float32')\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)\n",
    "\n",
    "# Use a smaller subset for hyperparameter tuning\n",
    "X_tune, _, y_tune, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=355)\n",
    "\n",
    "# Define the parameter space for KNN\n",
    "param_dist = {\n",
    "    'n_neighbors': np.arange(1, 21, dtype=int),\n",
    "    'p': [1, 2],  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Function to perform k-fold cross-validation with batched prediction\n",
    "def cross_validate(X, y, model, n_splits=3, batch_size=10000):\n",
    "    fold_size = len(X) // n_splits\n",
    "    scores = []\n",
    "    for i in range(n_splits):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        X_val = X.iloc[start:end]\n",
    "        y_val = y.iloc[start:end]\n",
    "        X_train = cudf.concat([X.iloc[:start], X.iloc[end:]])\n",
    "        y_train = cudf.concat([y.iloc[:start], y.iloc[end:]])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Batched prediction\n",
    "        y_pred = cudf.Series()\n",
    "        for j in range(0, len(X_val), batch_size):\n",
    "            X_batch = X_val.iloc[j:j+batch_size]\n",
    "            y_pred = cudf.concat([y_pred, model.predict(X_batch)])\n",
    "        \n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perform manual randomized search\n",
    "n_iter = 10\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    params = {k: np.random.choice(v) for k, v in param_dist.items()}\n",
    "    knn = KNeighborsClassifier(metric='minkowski', **params)\n",
    "    score = cross_validate(X_tune, y_tune, knn)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "    \n",
    "    print(f\"Iteration {_+1}/{n_iter} - Score: {score:.4f} - Params: {params}\")\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set in batches\n",
    "batch_size = 10000\n",
    "y_pred = cudf.Series()\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test.iloc[i:i+batch_size]\n",
    "    y_pred = cudf.concat([y_pred, best_knn.predict(X_batch)])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBest model accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9812c5-8ed3-44c9-b78b-00e1fd5a33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import pickle\n",
    "with open(\"best_knn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_knn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ddbc1-e577-49b7-9e15-5e0aaf350c09",
   "metadata": {},
   "source": [
    "The above, being in a Jupyter notebook, is fine for something that only takes a few minutes. But if we want to e.g. try hundreds of different settings, or a model that takes longer to fit, we should make a script that we can submit as a SLURM job. We just need to copy our code into a .py file and make a SLURM script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1c492-effb-4763-9cc0-43b2e79abc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be what we put in our .py file:\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_covtype\n",
    "import pickle\n",
    "\n",
    "# Load the covertype dataset\n",
    "cov_type = fetch_covtype()\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing and ensure float32 dtype\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names).astype('float32')\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)\n",
    "\n",
    "# Use a smaller subset for hyperparameter tuning\n",
    "X_tune, _, y_tune, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=355)\n",
    "\n",
    "# Define the parameter space for KNN\n",
    "param_dist = {\n",
    "    'n_neighbors': np.arange(1, 21, dtype=int),\n",
    "    'p': [1, 2],  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Function to perform k-fold cross-validation with batched prediction\n",
    "def cross_validate(X, y, model, n_splits=3, batch_size=10000):\n",
    "    fold_size = len(X) // n_splits\n",
    "    scores = []\n",
    "    for i in range(n_splits):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        X_val = X.iloc[start:end]\n",
    "        y_val = y.iloc[start:end]\n",
    "        X_train = cudf.concat([X.iloc[:start], X.iloc[end:]])\n",
    "        y_train = cudf.concat([y.iloc[:start], y.iloc[end:]])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Batched prediction\n",
    "        y_pred = cudf.Series()\n",
    "        for j in range(0, len(X_val), batch_size):\n",
    "            X_batch = X_val.iloc[j:j+batch_size]\n",
    "            y_pred = cudf.concat([y_pred, model.predict(X_batch)])\n",
    "        \n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perform manual randomized search\n",
    "n_iter = 10 \n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    params = {k: np.random.choice(v) for k, v in param_dist.items()}\n",
    "    knn = KNeighborsClassifier(metric='minkowski', **params)\n",
    "    score = cross_validate(X_tune, y_tune, knn)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "    \n",
    "    print(f\"Iteration {_+1}/{n_iter} - Score: {score:.4f} - Params: {params}\")\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set in batches\n",
    "batch_size = 10000\n",
    "y_pred = cudf.Series()\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test.iloc[i:i+batch_size]\n",
    "    y_pred = cudf.concat([y_pred, best_knn.predict(X_batch)])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBest model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "with open(\"best_knn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_knn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2a7b3-e079-4024-a380-ee956b68c43b",
   "metadata": {},
   "source": [
    "And our SLURM script (saved in a separate file, e.g. `hyperparam_opt.sh`) can be:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=hyperparam_opt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=4\n",
    "#SBATCH --time=16:00:00\n",
    "#SBATCH --gres=gpu:v100:1\n",
    "#SBATCH --output=hyperparam_opt_%j.out\n",
    "\n",
    "# Load the modules we need\n",
    "module load anaconda3\n",
    "module load cuda\n",
    "\n",
    "# Activate the environment we created to work in\n",
    "source activate MLWorkshop\n",
    "\n",
    "# Change to the directory where the .py script is\n",
    "cd /home/[username]/dir/where/the/py/script/is/\n",
    "\n",
    "# And run the script!\n",
    "python hyperparameter_optimization.py\n",
    "```\n",
    "\n",
    "Then we can submit the job with `sbatch hyperparam_opt.sh`, on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0457ecb-a0b6-4651-bb1c-7f42e1bf5f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPC_ML",
   "language": "python",
   "name": "hpc_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
