{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b8ec82-9fd4-4689-bb7a-9d4e148ebf4d",
   "metadata": {},
   "source": [
    "# Machine Learning in Python using Clemson High Performance Computing\n",
    "\n",
    "**Instructor:** Carl Ehrett\n",
    "\n",
    "**Email:** cehrett@clemson.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e4b7f-83ed-46e1-9697-49e6d823830e",
   "metadata": {},
   "source": [
    "## 1. Welcome and Overview\n",
    "\n",
    "In this workshop, we will introduce the basics of machine learning using Python. We will focus on machine learning using \"tabular data\" (i.e. spreadsheet-style data), as opposed to images or unstructured text, though most of what we talk about will also apply to those domains. Image-related ML and generative AI both typically use deep learning neural networks which are not the focus of this workshop, but will be the focuses of other upcoming CCIT workshops. In this workshop, we will emphasize the use of Clemson's Palmetto Cluster for running machine learning algorithms on large datasets. We will cover the following topics:\n",
    "* What is machine learning?\n",
    "* What are some of the python tools that facilitate machine learning?\n",
    "* What are the different types of machine learning?\n",
    "* What are some of the common machine learning algorithms?\n",
    "* How do we evaluate the performance of machine learning algorithms?\n",
    "* How do we explore and clean data?\n",
    "* How do we prepare data for machine learning?\n",
    "* How do we make use of Clemson's Palmetto Cluster to efficiently run our machine learning code?\n",
    "* How can we run code that is too complex, or use data that is too large, for a Jupyter notebook?\n",
    "* What sorts of Palmetto resources should we request to allocate for our machine learning jobs?\n",
    "\n",
    "### 1.1 Getting started\n",
    "\n",
    "To get started with the notebooks, please follow the steps below carefully.\n",
    "\n",
    "#### 1.1.1 Start a Jupyter session in Open OnDemand\n",
    "\n",
    "- Launch **Jupyter Notebook (Containerized)** from Open OnDemand.\n",
    "- In the form:\n",
    "  - Set **CPU Cores**: `8`\n",
    "  - Set **Memory**: `32G`\n",
    "  - Select **1 V100 GPU**\n",
    "  - Set **Path to container image (.sif)**: `/project/rcde/cehrett/python_sklearn/workshop.sif`\n",
    "\n",
    "#### 1.1.2 Copy the workshop files to your home directory\n",
    "\n",
    "Once your session starts:\n",
    "\n",
    "- Open a terminal from the JupyterLab interface.\n",
    "- Run the following command to copy the workshop materials: `cp -r /opt/workshop/ ./`\n",
    "- If you already have a folder named `workshop` in your home directory, either rename or remove it first, or copy to a different location: `cp -r /opt/workshop/ ~/some/other/location/`\n",
    "\n",
    "#### 1.1.3 About using this code outside the container\n",
    "\n",
    "If you’d like to run these notebooks outside of the container (e.g., on your laptop or another system), you’ll need to set up a Python environment with the required libraries.\n",
    "\n",
    "```bash\n",
    "# Commands for creating and activating a conda environment (in the terminal on a compute node)\n",
    "module load miniforge3\n",
    "conda create -n hpc_ml -c rapidsai -c conda-forge -c nvidia cudf cuml numpy pandas scikit-learn matplotlib seaborn rapids jupyterlab python=3.11 'cuda-version>=12.0,<=12.5'\n",
    "# Warning: the above command may take awhile! ~20 minutes. Next:\n",
    "source activate hpc_ml\n",
    "```\n",
    "In addition to installing JupyterLab, we need to register our environment as a Jupyter kernel in order for it to show up as an option for us when running a notebook.\n",
    "```bash\n",
    "# Register the env as a kernel\n",
    "python -m ipykernel install --user --name hpc_ml --display-name \"HPC_ML\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58072988-fe1a-4835-b1b4-48b435a78f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"Please enter your name.\", \"00-01\")\n",
    "create_answer_box(\"Please enter your Clemson email address.\", \"00-02\")\n",
    "create_answer_box(\"Please briefly describe your level of experience with the Python programming language.\", \"00-03\")\n",
    "create_answer_box(\"Please briefly describe your level of experience with Palmetto. (Have you used Open OnDemand? Have you submitted batch jobs? Etc.)\", \"00-04\")\n",
    "create_answer_box(\"Are there any particular areas of machine learning that you especially hope to learn about today?\", \"00-05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ad72d-d72c-4258-82f5-c5b60c213257",
   "metadata": {},
   "source": [
    "### 1.2 What is machine learning?\n",
    "\n",
    "People use the term \"machine learning\" in a variety of ways. Some people use it more or less synonymously with \"artificial intelligence.\" And these days, AI does indeed usually work under the paradigm of machine learning. But \"machine learning\" refers to the use of algorithms to learn from data. The contrast here is with traditional programming, where a programmer writes code that tells the computer exactly what to do. In machine learning, the programmer writes code that tells the computer how to learn from data to make decisions.\n",
    "\n",
    "<div style=\"display: inline-block; text-align: center;\">\n",
    "  <img src=\"img/traditional_programming_diagram_titled.png\" alt=\"TP paradigm\" style=\"height: 250px; margin-right: 20px;\"/>\n",
    "  <img src=\"img/machine_learning_diagram_titled.png\" alt=\"ML paradigm\" style=\"height: 250px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53369bf4-6024-4730-b9c8-ba341b9c7f31",
   "metadata": {},
   "source": [
    "## 3. Example end-to-end ML: [Forest Covertypes Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype)\n",
    "\n",
    "First, we will use the Forest Covertypes dataset, which is a dataset that contains information about the forest cover type in the Roosevelt National Forest of northern Colorado. The dataset contains 581,012 samples and 54 features. The goal is to predict the forest cover type based on the cartographic features provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768928a2-0861-4160-b2dd-d93256f30acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "cov_type = fetch_covtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d4034-06e9-4207-a803-926173527ee2",
   "metadata": {},
   "source": [
    "As we should always do with any dataset we're working with, we should poke around it a bit to see what it looks like. It's always good to know what is the datatype of the object we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c120dc-e1b5-4610-9764-a09a8cd7b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cov_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67174031-3f52-40f7-a24b-405164c33dc6",
   "metadata": {},
   "source": [
    "In this case we've got a scikit-learn \"Bunch\" object. A quick google search shows us this page of documentation: [sklearn.utils.Bunch Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html), where we learn that a Bunch is a dictionary-like object that exposes keys as attributes. So, let's see what keys are in this Bunch object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651e822-2ba8-41e9-a029-cf0420caca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31098619-58d6-42e3-9930-0222fe674c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cov_type.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb40e581-f615-4b29-9fda-8c0ecd235f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783645bf-deeb-4b14-935f-a9f433615fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019922dd-90a1-403f-9f98-22cc5eb297c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cov_type.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6e4aa-b98c-404b-bb11-37cc3fea08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd12d334-3c4f-4cbf-85fe-dcf4916e9a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21b4d6-c9a5-421e-b565-83daa014c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc8a04-0c3d-452e-a2a8-2a801b20b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cov_type.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39b142-46d9-4c4a-adae-bd29879f1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10ab1c-23fc-43cf-b298-132be2ef8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_type.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6ba288-5c93-422b-a541-19e47fe00fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cov_type.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319bcf6-3dd9-433a-95b8-bc229e392fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame using the feature names and data from cov_type\n",
    "df_cov_type = pd.DataFrame(data=cov_type.data, columns=cov_type.feature_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the Forest Covertypes Dataset:\")\n",
    "df_cov_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749127a-861d-4181-af46-3393360bcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use pandas' functionality to check out the distribution of the data in the Soil Type columns.\n",
    "df_cov_type[[col for col in df_cov_type.columns if 'Soil_Type' in col]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0e1b7-9468-4a4d-9700-af6a5dd428d9",
   "metadata": {},
   "source": [
    "Let's use the K-nearest neighbors algorithm to try to predict covertype using the information contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4aba26-4683-44d9-b15f-dd268ad6bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X, y = cov_type.data, cov_type.target\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=355)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26e286-b844-41e4-9587-e0098ef4f419",
   "metadata": {},
   "source": [
    "Now it's time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd49595b-651e-4128-9927-7c77bd7815a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the classifier\n",
    "kn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "kn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81dc3b-327c-4666-9426-a3dcf7047c9b",
   "metadata": {},
   "source": [
    "Now let's see how the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1bfbf-dbea-4f04-a3f6-0d5758f00f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Start a timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = kn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get unique class labels\n",
    "unique_labels = np.unique(y)\n",
    "target_names = [f\"Class {label}\" for label in unique_labels]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# See how long that took\n",
    "print(f\"\\nTotal time taken: {time.time() - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e4812-023c-44ba-9855-943e9910ca2e",
   "metadata": {},
   "source": [
    "The above code works fine, but takes a long time to run! On the order of 1-2 minutes, depending on how fast your machine is. And that's not even bad, as far as what machine learning requires.\n",
    "\n",
    "Now let's look at a version of the same machine learning approach that makes better use of the resources available to us -- using both parallelization across cores and also using the GPU we've provisioned to speed things up.\n",
    "\n",
    "First, we'll load the data and prepare it for the KNN algorithm. If you get an error like `OSError: libcudart.so: cannot open shared object file: No such file or directory`, then you likely need to add `cuda` as a module loaded when launching your Open OnDemand session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98353691-c26c-4ade-a8df-6f4acad0f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml import KNeighborsClassifier as cuKNN\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Initialize Dask client for distributed computing\n",
    "n_workers = 4  # Adjust based on your available resources\n",
    "cluster = LocalCluster(n_workers=n_workers)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names)\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=355)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f12db4-3ade-4561-9f74-84e8fd4485c6",
   "metadata": {},
   "source": [
    "Now we fit the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076fca0-bdb0-4f54-92fa-a9cd9fd79767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the classifier\n",
    "kn_classifier_cuml = cuKNN(n_neighbors=5)\n",
    "kn_classifier_cuml.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf6467-a2b5-47d6-b069-7ae29fa1b590",
   "metadata": {},
   "source": [
    "Now let's see how this model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfc3f8-1f14-4e66-8309-a7e137a3e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = kn_classifier_cuml.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test.to_cupy().get(), y_pred.to_cupy().get())\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert predictions to numpy for classification report\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_pred_np = y_pred.to_numpy()\n",
    "\n",
    "# Get unique class labels\n",
    "unique_labels = np.unique(y_test_np)\n",
    "target_names = [f\"Class {label}\" for label in unique_labels]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_np, y_pred_np, target_names=target_names))\n",
    "\n",
    "# Clean up\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# See how long that took\n",
    "print(f\"\\nTotal time taken: {time.time() - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97847e89-859f-453a-bd42-485da24c1964",
   "metadata": {},
   "source": [
    "Notice how much faster inference is in the second case! And this is just a simple case of getting predictions for 26k samples. Imagine if we needed to get predictions for millions of samples. In that case, the second approach would be much more feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbae271-e29e-4c72-8859-f7fece26c284",
   "metadata": {},
   "source": [
    "## 4. Hyperoptimization: search for the best version of the model to maximize performance\n",
    "\n",
    "In the above example, I used a KNN model with a k value of 5. But how do we know that k=5 is the best value? We don't. We need to search for the best value of k. This is an example of hyperoptimization. You can think of hyperoptimization as a search for the best version of the model to maximize performance. \n",
    "\n",
    "Hyperoptimization is inherently computationally expensive. It involves training many models with different hyperparameters and evaluating their performance. This is a perfect use case for the Palmetto 2 Cluster. We can use the cluster to train many models in parallel, which will speed up the hyperoptimization process. And if we submit our hyperoptimization job as a batch job, then we don't need to tie up our local machine for hours -- or days! -- while the hyperoptimization process runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29f643-46ed-464e-b1aa-3a83dc7ae877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.metrics import accuracy_score\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing and ensure float32 dtype\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names).astype('float32')\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=355)\n",
    "\n",
    "# Define the parameter space for KNN\n",
    "param_dist = {\n",
    "    'n_neighbors': np.arange(1, 21, dtype=int),\n",
    "    'p': [1, 2],  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Function to perform k-fold cross-validation with batched prediction\n",
    "def cross_validate(X, y, model, n_splits=3, batch_size=10000):\n",
    "    fold_size = np.min([len(X),20000]) // n_splits\n",
    "    scores = []\n",
    "    for i in range(n_splits):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        X_val = X.iloc[start:end]\n",
    "        y_val = y.iloc[start:end]\n",
    "        X_train = cudf.concat([X.iloc[:start], X.iloc[end:]])\n",
    "        y_train = cudf.concat([y.iloc[:start], y.iloc[end:]])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Batched prediction\n",
    "        y_pred = cudf.Series()\n",
    "        for j in range(0, len(X_val), batch_size):\n",
    "            X_batch = X_val.iloc[j:j+batch_size]\n",
    "            y_pred = cudf.concat([y_pred, model.predict(X_batch)])\n",
    "        \n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perform manual randomized search\n",
    "n_iter = 10\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    params = {k: np.random.choice(v) for k, v in param_dist.items()}\n",
    "    knn = KNeighborsClassifier(metric='minkowski', **params)\n",
    "    score = cross_validate(X_train, y_train, knn)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "    \n",
    "    print(f\"Iteration {_+1}/{n_iter} - Score: {score:.4f} - Params: {params}\")\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set in batches\n",
    "batch_size = 10000\n",
    "y_pred = cudf.Series()\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test.iloc[i:i+batch_size]\n",
    "    y_pred = cudf.concat([y_pred, best_knn.predict(X_batch)])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBest model accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9812c5-8ed3-44c9-b78b-00e1fd5a33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import pickle\n",
    "with open(\"best_knn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_knn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ddbc1-e577-49b7-9e15-5e0aaf350c09",
   "metadata": {},
   "source": [
    "The above, being in a Jupyter notebook, is fine for something that only takes a few minutes. But if we want to e.g. try hundreds of different settings, or a model that takes longer to fit, we should make a script that we can submit as a SLURM job. We just need to copy our code into a .py file and make a SLURM script.\n",
    "\n",
    "See RCD job submission documentation [here](https://docs.rcd.clemson.edu/palmetto/job_management/submit/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c1c492-effb-4763-9cc0-43b2e79abc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be what we put in our .py file:\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_covtype\n",
    "import pickle\n",
    "\n",
    "# Load the covertype dataset\n",
    "cov_type = fetch_covtype()\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing and ensure float32 dtype\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names).astype('float32')\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)\n",
    "\n",
    "# Define the parameter space for KNN\n",
    "param_dist = {\n",
    "    'n_neighbors': np.arange(1, 21, dtype=int),\n",
    "    'p': [1, 2],  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Function to perform k-fold cross-validation with batched prediction\n",
    "def cross_validate(X, y, model, n_splits=3, batch_size=10000):\n",
    "    fold_size = np.min([len(X),20000]) // n_splits\n",
    "    scores = []\n",
    "    for i in range(n_splits):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        X_val = X.iloc[start:end]\n",
    "        y_val = y.iloc[start:end]\n",
    "        X_train = cudf.concat([X.iloc[:start], X.iloc[end:]])\n",
    "        y_train = cudf.concat([y.iloc[:start], y.iloc[end:]])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Batched prediction\n",
    "        y_pred = cudf.Series()\n",
    "        for j in range(0, len(X_val), batch_size):\n",
    "            X_batch = X_val.iloc[j:j+batch_size]\n",
    "            y_pred = cudf.concat([y_pred, model.predict(X_batch)])\n",
    "        \n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perform manual randomized search\n",
    "n_iter = 10 \n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    params = {k: np.random.choice(v) for k, v in param_dist.items()}\n",
    "    knn = KNeighborsClassifier(metric='minkowski', **params)\n",
    "    score = cross_validate(X_train, y_train, knn)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "    \n",
    "    print(f\"Iteration {_+1}/{n_iter} - Score: {score:.4f} - Params: {params}\")\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set in batches\n",
    "batch_size = 10000\n",
    "y_pred = cudf.Series()\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test.iloc[i:i+batch_size]\n",
    "    y_pred = cudf.concat([y_pred, best_knn.predict(X_batch)])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBest model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "with open(\"best_knn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_knn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2a7b3-e079-4024-a380-ee956b68c43b",
   "metadata": {},
   "source": [
    "And our SLURM script (saved in a separate file, e.g. `hyperparam_opt.sh`) can be:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=hyperparam_opt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=4\n",
    "#SBATCH --time=16:00:00\n",
    "#SBATCH --gres=gpu:v100:1\n",
    "#SBATCH --output=hyperparam_opt_%j.out\n",
    "\n",
    "# Load the modules we need\n",
    "module load anaconda3\n",
    "module load cuda\n",
    "\n",
    "# Activate the environment we created to work in\n",
    "source activate MLWorkshop\n",
    "\n",
    "# Change to the directory where the .py script is\n",
    "cd /home/[username]/dir/where/the/py/script/is/\n",
    "\n",
    "# And run the script!\n",
    "python hyperparameter_optimization.py\n",
    "```\n",
    "\n",
    "Then we can submit the job with `sbatch hyperparam_opt.sh`, on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0457ecb-a0b6-4651-bb1c-7f42e1bf5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"The `cross_validate` function defined above internally splits the data into a set used to train the data, and a separate \\\"validation\\\" set on which accuracy is evaluated. We already have a test set -- why is a separate validation set necessary? Why not just use our test set when checking the accuracy of each model fit?\", \"00-06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f7f7c-78c4-4055-b709-9fa872e7f636",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
