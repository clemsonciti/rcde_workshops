{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpUMVeVm2_Cr"
   },
   "source": [
    "# Evaluating Machine Learning Models\n",
    "\n",
    "Machine learning is awesome, right?! Define \"awesome\".\n",
    "\n",
    "The goal of machine learning algorithms is to **extract meaningful patterns** from training data either to help us understand that training data or to use those patterns to **make inferences** about new data (or both). If our goal is to better understand our training data, how do we know that the identified patterns are reliable insights into our data? If our goal is to make inferences about new data, how do we estimate the performance of our model on the new data? To succesfully apply machine learning methodology in research or industry applications, we must be able to answer these questions. While tools like IBM Watson greatly simplify the actual training of Machine Learning models, the careful evaluation of those models by the practicioner is indispensible.\n",
    "\n",
    "In this notebook, we will \n",
    "\n",
    "    1. Define what we mean by \"awesome\"\n",
    "    2. Introduce some common pitfalls on the road to awesome\n",
    "    3. Discuss how to calculate awesomeness\n",
    "    4. Leverage this knowledge to create even awesomer models\n",
    "    \n",
    "While this notebook include code examples, the content is intended for a general audience. The concepts that we discuss here have relevance for almost any data analysis project and should help you think critically and strategically whether you are a coder or not! If you do want to play with the code, feel free! You can double-click a cell to edit it. Press Shift+Enter to execute the cell. If you want to save your progress, you will first have to save a copy of the notebook to your drive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvxpgJFU49Ez"
   },
   "source": [
    "## Evaluating _Supervised_ Models\n",
    "In supervised learning, we fit a model that predicts a target variable $y$ given som input features $X$ by looking at lots of pre-labelled example pairs, $(y_1, X_1), (y_2, X_2), \\ldots, (y_N, X_N)$. These pre-labelled examples make the evaluation of supervised models relatively straightforward compared to unsupervised.\n",
    "\n",
    "For the remainder of this notebook, we will use an example from a political science project where we are modeling whether Tweets are political attacks based on the text of the tweet. The data set below includes our target column called `attack` and many feature columns called `tweet_vec_n` where n ranges from 1 to 75. In the attack column, 1s correspond to attacks and 0s correspond to non-attacks. The labels were assigned by humans. The `tweet_vec_n` columns form a numerical representation of the text of the tweet. Very similar tweets should have similar numbers. If this sounds crazy, don't worry about it for now as you won't need to understand this to understand the concepts in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EfL5jZ1AMME4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4_WPpN_P6bS",
    "outputId": "a1239328-4bc5-4a09-eadb-01958ed5a1ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attack</th>\n",
       "      <th>vec_1</th>\n",
       "      <th>vec_2</th>\n",
       "      <th>vec_3</th>\n",
       "      <th>vec_4</th>\n",
       "      <th>vec_5</th>\n",
       "      <th>vec_6</th>\n",
       "      <th>vec_7</th>\n",
       "      <th>vec_8</th>\n",
       "      <th>vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>vec_66</th>\n",
       "      <th>vec_67</th>\n",
       "      <th>vec_68</th>\n",
       "      <th>vec_69</th>\n",
       "      <th>vec_70</th>\n",
       "      <th>vec_71</th>\n",
       "      <th>vec_72</th>\n",
       "      <th>vec_73</th>\n",
       "      <th>vec_74</th>\n",
       "      <th>vec_75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.016942</td>\n",
       "      <td>0.024855</td>\n",
       "      <td>0.033888</td>\n",
       "      <td>-0.034978</td>\n",
       "      <td>-0.031188</td>\n",
       "      <td>-0.005101</td>\n",
       "      <td>-0.049382</td>\n",
       "      <td>0.035185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034461</td>\n",
       "      <td>-0.010897</td>\n",
       "      <td>-0.041929</td>\n",
       "      <td>-0.000717</td>\n",
       "      <td>0.057181</td>\n",
       "      <td>0.017889</td>\n",
       "      <td>-0.030877</td>\n",
       "      <td>-0.039927</td>\n",
       "      <td>0.027440</td>\n",
       "      <td>0.016859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.026694</td>\n",
       "      <td>0.010135</td>\n",
       "      <td>-0.018828</td>\n",
       "      <td>0.046631</td>\n",
       "      <td>-0.022001</td>\n",
       "      <td>0.010430</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>-0.039059</td>\n",
       "      <td>-0.007851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004756</td>\n",
       "      <td>-0.012542</td>\n",
       "      <td>0.028331</td>\n",
       "      <td>0.005426</td>\n",
       "      <td>0.015014</td>\n",
       "      <td>0.010646</td>\n",
       "      <td>-0.029356</td>\n",
       "      <td>-0.027508</td>\n",
       "      <td>0.053418</td>\n",
       "      <td>0.024528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.080366</td>\n",
       "      <td>-0.015909</td>\n",
       "      <td>-0.014785</td>\n",
       "      <td>-0.022373</td>\n",
       "      <td>0.113360</td>\n",
       "      <td>-0.029767</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>-0.059703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013075</td>\n",
       "      <td>-0.006471</td>\n",
       "      <td>0.018220</td>\n",
       "      <td>-0.027452</td>\n",
       "      <td>0.028064</td>\n",
       "      <td>-0.020817</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.024037</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>-0.033408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.015414</td>\n",
       "      <td>-0.013312</td>\n",
       "      <td>-0.021079</td>\n",
       "      <td>0.045578</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>0.018513</td>\n",
       "      <td>0.052609</td>\n",
       "      <td>-0.016076</td>\n",
       "      <td>0.052258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043686</td>\n",
       "      <td>-0.029191</td>\n",
       "      <td>0.027954</td>\n",
       "      <td>-0.037915</td>\n",
       "      <td>0.036626</td>\n",
       "      <td>0.005471</td>\n",
       "      <td>-0.028403</td>\n",
       "      <td>-0.089455</td>\n",
       "      <td>0.070966</td>\n",
       "      <td>-0.020188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.029376</td>\n",
       "      <td>0.027177</td>\n",
       "      <td>-0.041911</td>\n",
       "      <td>0.020376</td>\n",
       "      <td>-0.005441</td>\n",
       "      <td>-0.019955</td>\n",
       "      <td>-0.005843</td>\n",
       "      <td>-0.006628</td>\n",
       "      <td>-0.006893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015489</td>\n",
       "      <td>-0.014160</td>\n",
       "      <td>0.024181</td>\n",
       "      <td>-0.007469</td>\n",
       "      <td>0.010908</td>\n",
       "      <td>0.012564</td>\n",
       "      <td>-0.017050</td>\n",
       "      <td>-0.012459</td>\n",
       "      <td>0.057483</td>\n",
       "      <td>0.035029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>-0.021085</td>\n",
       "      <td>-0.021229</td>\n",
       "      <td>0.049246</td>\n",
       "      <td>-0.010870</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>-0.027241</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032286</td>\n",
       "      <td>-0.024165</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>0.027083</td>\n",
       "      <td>0.022846</td>\n",
       "      <td>-0.060983</td>\n",
       "      <td>-0.016991</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>-0.038895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>1</td>\n",
       "      <td>0.007446</td>\n",
       "      <td>0.012536</td>\n",
       "      <td>-0.092788</td>\n",
       "      <td>-0.009357</td>\n",
       "      <td>-0.000470</td>\n",
       "      <td>-0.018887</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.024066</td>\n",
       "      <td>0.056112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023681</td>\n",
       "      <td>-0.017766</td>\n",
       "      <td>0.033713</td>\n",
       "      <td>-0.015553</td>\n",
       "      <td>-0.051125</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.023976</td>\n",
       "      <td>-0.013475</td>\n",
       "      <td>0.037990</td>\n",
       "      <td>-0.001396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>0</td>\n",
       "      <td>0.025122</td>\n",
       "      <td>-0.058875</td>\n",
       "      <td>0.035881</td>\n",
       "      <td>0.038083</td>\n",
       "      <td>-0.060735</td>\n",
       "      <td>-0.019663</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>-0.002776</td>\n",
       "      <td>-0.047708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010515</td>\n",
       "      <td>0.057939</td>\n",
       "      <td>-0.031716</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.015016</td>\n",
       "      <td>0.121070</td>\n",
       "      <td>-0.051429</td>\n",
       "      <td>-0.004486</td>\n",
       "      <td>0.021345</td>\n",
       "      <td>-0.061438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.038971</td>\n",
       "      <td>-0.035440</td>\n",
       "      <td>0.049050</td>\n",
       "      <td>-0.028902</td>\n",
       "      <td>0.085988</td>\n",
       "      <td>0.039999</td>\n",
       "      <td>0.007620</td>\n",
       "      <td>-0.068482</td>\n",
       "      <td>0.035770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052571</td>\n",
       "      <td>-0.033085</td>\n",
       "      <td>0.032628</td>\n",
       "      <td>0.016246</td>\n",
       "      <td>0.022087</td>\n",
       "      <td>-0.014952</td>\n",
       "      <td>0.045145</td>\n",
       "      <td>-0.014866</td>\n",
       "      <td>0.051695</td>\n",
       "      <td>0.008636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.027141</td>\n",
       "      <td>-0.025524</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.031791</td>\n",
       "      <td>0.047001</td>\n",
       "      <td>-0.015007</td>\n",
       "      <td>0.015960</td>\n",
       "      <td>-0.045486</td>\n",
       "      <td>-0.032509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021693</td>\n",
       "      <td>-0.002078</td>\n",
       "      <td>-0.000358</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.034625</td>\n",
       "      <td>-0.076178</td>\n",
       "      <td>0.021523</td>\n",
       "      <td>0.048660</td>\n",
       "      <td>0.005742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1275 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      attack     vec_1     vec_2     vec_3     vec_4     vec_5     vec_6  \\\n",
       "0          0  0.004863  0.016942  0.024855  0.033888 -0.034978 -0.031188   \n",
       "1          1  0.026694  0.010135 -0.018828  0.046631 -0.022001  0.010430   \n",
       "2          0 -0.080366 -0.015909 -0.014785 -0.022373  0.113360 -0.029767   \n",
       "3          0  0.015414 -0.013312 -0.021079  0.045578  0.040534  0.018513   \n",
       "4          1  0.029376  0.027177 -0.041911  0.020376 -0.005441 -0.019955   \n",
       "...      ...       ...       ...       ...       ...       ...       ...   \n",
       "1270       0  0.000710 -0.021085 -0.021229  0.049246 -0.010870  0.007334   \n",
       "1271       1  0.007446  0.012536 -0.092788 -0.009357 -0.000470 -0.018887   \n",
       "1272       0  0.025122 -0.058875  0.035881  0.038083 -0.060735 -0.019663   \n",
       "1273       0 -0.038971 -0.035440  0.049050 -0.028902  0.085988  0.039999   \n",
       "1274       0 -0.027141 -0.025524  0.031718  0.031791  0.047001 -0.015007   \n",
       "\n",
       "         vec_7     vec_8     vec_9  ...    vec_66    vec_67    vec_68  \\\n",
       "0    -0.005101 -0.049382  0.035185  ...  0.034461 -0.010897 -0.041929   \n",
       "1     0.009476 -0.039059 -0.007851  ... -0.004756 -0.012542  0.028331   \n",
       "2     0.008176  0.003181 -0.059703  ...  0.013075 -0.006471  0.018220   \n",
       "3     0.052609 -0.016076  0.052258  ...  0.043686 -0.029191  0.027954   \n",
       "4    -0.005843 -0.006628 -0.006893  ...  0.015489 -0.014160  0.024181   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1270  0.001050 -0.027241  0.004095  ...  0.032286 -0.024165  0.009352   \n",
       "1271  0.014990  0.024066  0.056112  ...  0.023681 -0.017766  0.033713   \n",
       "1272  0.019662 -0.002776 -0.047708  ... -0.010515  0.057939 -0.031716   \n",
       "1273  0.007620 -0.068482  0.035770  ... -0.052571 -0.033085  0.032628   \n",
       "1274  0.015960 -0.045486 -0.032509  ...  0.021693 -0.002078 -0.000358   \n",
       "\n",
       "        vec_69    vec_70    vec_71    vec_72    vec_73    vec_74    vec_75  \n",
       "0    -0.000717  0.057181  0.017889 -0.030877 -0.039927  0.027440  0.016859  \n",
       "1     0.005426  0.015014  0.010646 -0.029356 -0.027508  0.053418  0.024528  \n",
       "2    -0.027452  0.028064 -0.020817 -0.107500  0.024037  0.009529 -0.033408  \n",
       "3    -0.037915  0.036626  0.005471 -0.028403 -0.089455  0.070966 -0.020188  \n",
       "4    -0.007469  0.010908  0.012564 -0.017050 -0.012459  0.057483  0.035029  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1270 -0.002891  0.027083  0.022846 -0.060983 -0.016991  0.011163 -0.038895  \n",
       "1271 -0.015553 -0.051125  0.045200  0.023976 -0.013475  0.037990 -0.001396  \n",
       "1272  0.007308  0.015016  0.121070 -0.051429 -0.004486  0.021345 -0.061438  \n",
       "1273  0.016246  0.022087 -0.014952  0.045145 -0.014866  0.051695  0.008636  \n",
       "1274  0.001623  0.081250  0.034625 -0.076178  0.021523  0.048660  0.005742  \n",
       "\n",
       "[1275 rows x 76 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of full data (num. rows, num. cols.):  (1275, 76)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/zfs/citi/workshop_data/python_ml/attacks_with_vecs.csv')\n",
    "display(data)\n",
    "print(\"\\nShape of full data (num. rows, num. cols.): \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5qQWFdV4ETY"
   },
   "source": [
    "For use throughout, we will split this data into\n",
    "* $y$ - our \"target\" indicating whether or not the tweet is an attack. \n",
    "* $X$ - the length 75 numerical vectors representing the tweet text.\n",
    "\n",
    "We wish to find a model $f$ such that\n",
    "$$\n",
    "y\\approx f(X).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rY6RbM1D4ML_",
    "outputId": "05473e6e-d884-476a-9a2c-5a34ca63be7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y:  (1275,)\n",
      "Shape of X:  (1275, 75)\n"
     ]
    }
   ],
   "source": [
    "# data = data[data.sample_id==2]\n",
    "y = data.attack\n",
    "X = data.iloc[:,1:]\n",
    "\n",
    "print(\"Shape of y: \", y.shape)\n",
    "print(\"Shape of X: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8P8YmWL4yf7"
   },
   "source": [
    "### Basic Concepts\n",
    "These lay the conceptual groundwork for understanding how to evaluate machine learning models (or statistical models in general). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5XZiuz5754P"
   },
   "source": [
    "#### Don't test your model on the training data!\n",
    "![train_test_split](https://miro.medium.com/max/2272/1*-8_kogvwmL1H6ooN1A1tsQ.png)\n",
    "The number one rule for evaluating machine learning models is, **don't test your model on the same data you used to train the model!** Doing so is kindof like reusing homework problems verbatim on an algebra exam. The algebra students could get a perfect score on the exam simply by memorizing the answers to each homework problem without learning anything about algebra. This is unsatisfactory, because if faced with a new problem the algebra student would be completely lost. Similarly, under some conditions, ML algorithms are prone to \"memorizing\" responses rather than learning generalizeable patterns. Even in circumstances where the model is not precisely memorizing the data, it will perform at least somewhat better on the training set than on a held-out test set, so one should never expect the performance on the training set to generalize to new data.\n",
    "\n",
    "To demonstrate this idea, we will split our data $y$, $X$ into a training set $y_\\mathrm{train}$, $X_\\mathrm{train}$ and a test set $y_\\mathrm{test}$, $X_\\mathrm{test}$. We then train a model on $y_\\mathrm{train}$ and look at the mean accuracy when we test our model on the training set and the test set. By the way, the type of model we use here is called [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "MPU6D4Iuz409"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1275, 2926)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute polynomial features up to degree 2\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X_poly = PolynomialFeatures(2).fit_transform(X)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "MPU6D4Iuz409"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= (850, 2926)\n",
      "X_test.shape= (425, 2926)\n"
     ]
    }
   ],
   "source": [
    "# make the train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.333, random_state=42)\n",
    "print(\"X_train.shape=\", X_train.shape)\n",
    "print(\"X_test.shape=\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "MPU6D4Iuz409"
   },
   "outputs": [],
   "source": [
    "# fit the model on the training data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=10000, solver='liblinear')\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kecr05iF5W78",
    "outputId": "cd32d91a-05e8-4106-b321-a8867319df39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy on the training set: 99.8%\n",
      "Mean accuracy on the test set: 88.7%\n"
     ]
    }
   ],
   "source": [
    "def fmt_pct(rat):\n",
    "    return str(round(rat*100, 1)) + \"%\"\n",
    "\n",
    "print(\"Mean accuracy on the training set: \" + fmt_pct(model.score(X_train, y_train)))\n",
    "print(\"Mean accuracy on the test set: \" + fmt_pct(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyDYLvc_60ar"
   },
   "source": [
    "Training set accuracy is much higher than test set accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzG6mYy75etd"
   },
   "source": [
    "### Rigorous Model Evaluation with Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhDy3x-xoyT_"
   },
   "source": [
    "#### shortcoming of simple train/test split\n",
    "The train/test split has the latent assumption that the error on the test set is a good proxy for the error on new unseen data. But our test sample will always be non-representative, to some extent, of the total population. In the train/test approach, you don't have any way to estimate how different your test data is from the population.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxLuhRBn7Qkz"
   },
   "source": [
    "#### k-Fold Cross Validation\n",
    "\n",
    "We've emphasized the importance of using a witheld test set to evaluate our models in order to get an unbiased estimate of model performance. But what if our test set happens to be substantially different from the full population that we are sampling from? It may be that we substantially underestimate or worse overestimate the performance of our model. For instance, maybe some Tweets are very easy to classify as attack or non-attack while others are very difficult to classify with a given modeling approach. If we happen to get a larger portion of the easy cases in our test dataset, then our evaluation metrics (precision, recall, F1, ROC-AUC) will be larger than if we had a representative distribution of easier and harder Tweets! This problem is especially pronounced with smaller datasets such as ours. Note that this is not an error of bias, but rather a random statistical error. \n",
    "\n",
    "To demonstrate, let's split our dataset into several groups of equal size and withold in turn each of the five as our test dataset. We will train the model on the remaining four, and calculate the ROC-AUC score on the fifth to see how much the score varies from group to group. The figure below demonstrates this procedure:\n",
    "\n",
    "![cross validation](https://i.stack.imgur.com/1fXzJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8YxazyYkAfZ"
   },
   "source": [
    "The code below performs this splitting and outputs the AUC score for each of 10 splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9k3JZGWkAAu",
    "outputId": "fb82415c-2e2d-4f24-f38b-104492f1c44f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fmt_rat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m auc_split \u001b[38;5;241m=\u001b[39m roc_auc_score(y\u001b[38;5;241m.\u001b[39miloc[test], y_test_probs)\n\u001b[1;32m     18\u001b[0m aucs\u001b[38;5;241m.\u001b[39mappend(auc_split)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. ROC-AUC: \u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[43mfmt_rat\u001b[49m(auc_split, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     21\u001b[0m i\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fmt_rat' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# define how splitting is performed\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "model = LogisticRegression(C=10000, solver='liblinear')\n",
    "\n",
    "# loop over splits\n",
    "i=1\n",
    "aucs = []\n",
    "for train, test in cv.split(X, y):\n",
    "    # train model\n",
    "    model_split = model.fit(X.iloc[train], y.iloc[train])\n",
    "    # get test probs\n",
    "    y_test_probs = model_split.predict_proba(X.iloc[test])[:,1]\n",
    "    # get roc-auc\n",
    "    auc_split = roc_auc_score(y.iloc[test], y_test_probs)\n",
    "    aucs.append(auc_split)\n",
    "\n",
    "    print(\"Split \" + str(i) +\". ROC-AUC: \" , fmt_rat(auc_split, 3))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLxCuC1m4tj2"
   },
   "source": [
    "#### Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1p27u2o7sLN"
   },
   "source": [
    "The problem we encountered above results from a phenomenon called **overfitting**. During the fitting process, the model learns (or memorizes) many of the specifics of the training data. This memorization doesn't apply to new data, so we see a big difference in the performance on the training data and the test data. This difference might be OK as long as the performance on the test set is acceptable. Sometimes to learn generalizeable patterns from the training data, you have to settle for a bit of overfitting. However, too much overfitting will eventually hurt performance. This source of error is sometimes called model _variance_. We'll make this idea more concrete later. \n",
    "\n",
    "**Underfitting** is the opposite of overfitting and occurs when the model is not sufficiently flexible to capture the general trends in the data. This source of error is sometimes called model _bias_.\n",
    "\n",
    "The figure below summarizes both of these ideas. We would like to arrive at a model (blue line) which captures the overall behavior of the data (orange dots) and that will generalize well to new data sampled from the same distribution. The straight line model on the left is insufficiently flexible to capture the curved shape of the data and is said to be _underfitting_. The wiggly line model on the right is so flexible that it's fitting random noise in the data rather than the overall behavior and is said to be _overfitting_. The middle model does a good job of capturing the overall behavior of the data and is said to be _juuuuuust right_.\n",
    "\n",
    "![Model under and over-fitting](https://docs.aws.amazon.com/machine-learning/latest/dg/images/mlconcepts_image5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzFhGIhX6WJL"
   },
   "source": [
    "#### The bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOIMKOXWF5Mk"
   },
   "source": [
    "Above, we discussed how both overfitting and underfitting can harm the performance of our model. So we should somehow try to minimize them. However, reducing the error resulting from one tends to increase the error resulting from the other. This phenomenon is called the bias-variance tradeoff, where bias and variance are understood to be sources of error resulting from underfitting and overfitting respectively. In particular:\n",
    "  * **Bias error** results from a model that is improperly constrained (or biased) in a way that prevents it from representing the actual patterns in the data, such as the left-most plot above. \n",
    "  * **Variance error** results from a model that is so unconstrained that it fits the noise in the sample dataset preventing it from generalizing well to new data such as the rightmost plot above. The term variance refers to how, since the model is fitting the noise of the sample data, the parameters of the model would change dramatically if trained on a new sample.\n",
    " \n",
    "A good machine learning model finds the sweet spot which balances these two sources of error. But how exactly do we do that? There are many ways, and we won't get into the specific now. However, to demonstrate the idea, we will take our attack data and train progressively more complex models and see how the error changes on the training set and the test set. To acheive this, we again use a Logistic Regression but with progressively larger values for a parameter called the [\"inverse regularization strength\"](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) or \"C\" for short. The basic idea is that larger values of C result in more flexible models that have more potential for overfitting; small values of C result in less flexible models leading to bias. We will train models for values of C between 0 and 1,000 and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "mSE1NsuDF2rz",
    "outputId": "1f6ccd38-ec48-470d-a368-383694b90dd3"
   },
   "outputs": [],
   "source": [
    "# we will return to GridSearchCV later in the notebook\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We want to see how the model performs \n",
    "params = {\"C\": np.logspace(-1,4,20)}\n",
    "\n",
    "# train the model at the specified max depth\n",
    "model = LogisticRegression(solver='liblinear', max_iter=200)\n",
    "model_cv = GridSearchCV(model, params, cv=15, \n",
    "                        scoring='roc_auc', \n",
    "                        n_jobs=6,\n",
    "                        refit=True,\n",
    "                        return_train_score=True,\n",
    "                        verbose=10)\n",
    "\n",
    "model_cv.fit(X,y);\n",
    "\n",
    "bias_variance = params['C']\n",
    "scores_train = model_cv.cv_results_[\"mean_train_score\"]\n",
    "scores_test = model_cv.cv_results_[\"mean_test_score\"]\n",
    "\n",
    "print(\"Optimal value. C=\" + str(round(model_cv.best_estimator_.C)))\n",
    "\n",
    "# Plot the scores\n",
    "plt.semilogx(bias_variance, scores_train, 'ro-', label='train')\n",
    "plt.semilogx(bias_variance, scores_test, 'bo-', label='test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"(<--Bias) C (Variance-->)\")\n",
    "plt.ylabel(\"Model Performance\")\n",
    "plt.title(\"Bias-Variance Tradeoff\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question**: Why does the training set keep going up, while the test set goes up then comes back down?  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L12VWqWP3nFr"
   },
   "source": [
    "#### The class imbalance problem\n",
    "Often with classification problems, some classes will be more abundant than others. This leads to a few issues which are collectively called the **class imbalance problem**:\n",
    "  1. The model will tend to perform much better for the classes that have more data\n",
    "  2. Model metrics like accuracy will tend to give an inflated sense of the model performance.\n",
    "  \n",
    "Let's look at how many attacks and non-attacks we have in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlHB02_iHi6-",
    "outputId": "2a7a756b-7a2f-4f20-84ea-70892333829a"
   },
   "outputs": [],
   "source": [
    "data.attack.value_counts() / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5F_H042fBtX"
   },
   "source": [
    "We see that only about 13% of our tweets are attacks. Let's imagine a very simple model where we just say that every tweet is not an attack. This stupid model knows nothing about attacks, and yet, due to the portion of non-attacks in our dataset,  it is 87% accurate! \n",
    "\n",
    "Look back at the Bias-Variance tradeoff plot above. On the far left, the model accuracy approaches 88%. At the time we may have thought this was pretty good. Now we see that the stupid model is simply predicting that none of the tweets are attacks.\n",
    "\n",
    "Clearly we need new metrics which take class imbalance into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bau4lsN3ahvs"
   },
   "source": [
    "A few important things to observe from this plot:\n",
    "* On the far left, the training and test accuracies are similar. These models have little overfitting (good), but the error due to bias makes the accuracy quite low compared to the rest of the plot (bad). \n",
    "* As we move toward the middle of the plot, we see the train accuracy rise far above the test accuracy, corresponding to the onset of model variance error. However, the test accuracy is continueing to rise, so that's OK. \n",
    "* Once we get beyond $C=100$ the test accuracy starts to drop even though the train accuracy continues to rise due to large model variance error. The peak at $C\\sim50$ is the optimal balance between the two sources of error! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkJsS_hWUzt7"
   },
   "source": [
    "#### Confidence scores and decision thresholds\n",
    "Most classification models output a confidence score rather than a discreet label. For example, our attack classifier will output a number between 0 and 1. The closer this score is to 1, the more \"confident\" the model is that the tweet is an attack. Let's look at the confidence scores for our model. \n",
    "\n",
    "The figures below show the confidence scores that our model predicted on held-out test data. The figure on the left shows the confidence scores for tweets that we know are actually non-attacks. As we would expect, the confidence scores are mostly close to zero. The figure on the right shows the scores for tweets that we know to be attacks. Compared to the non-attack tweets, these scores are shifted toward 1 as we would hope. There are quite a few attack tweets that have very low confidence scores. It would be interesting to look at these to see why the model is failing to grasp that they are attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "CI9SZLxfjXVq",
    "outputId": "ac198012-9671-49f4-9744-16f8cc86a03e"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "clf = model_cv.best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "X_test_attack = X_test[y_test==1]\n",
    "X_test_non_attack = X_test[y_test==0]\n",
    "\n",
    "confidence_scores_attack = clf.predict_proba(X_test_attack)[:,1]\n",
    "confidence_scores_non_attack = clf.predict_proba(X_test_non_attack)[:,1]\n",
    "\n",
    "ax=plt.subplot(1,2,1)\n",
    "plt.hist(confidence_scores_non_attack,bins=10)\n",
    "plt.vlines(0.5,0,500, colors='k', linestyles='dotted')\n",
    "plt.xticks([0,0.25,0.5,0.75,1.0])\n",
    "plt.ylim(0,500)\n",
    "plt.title(\"Actual Non-attacks\")\n",
    "plt.text(0.25, 0.99,'Predicted Non-attacks\\n(True Negatives)',\n",
    "     horizontalalignment='center',\n",
    "     verticalalignment='top',\n",
    "     transform = ax.transAxes)\n",
    "plt.text(0.75, 0.99,'Predicted Attacks\\n(False Positives) ',\n",
    "     horizontalalignment='center',\n",
    "     verticalalignment='top',\n",
    "     transform = ax.transAxes)\n",
    "plt.xlabel(\"Confidence score\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "\n",
    "ax=plt.subplot(1,2,2)\n",
    "plt.hist(confidence_scores_attack,bins=10)\n",
    "plt.vlines(0.5,0,300, colors='k', linestyles='dotted')\n",
    "plt.xticks([0,0.25,0.5,0.75,1.0])\n",
    "plt.ylim(0,35)\n",
    "plt.title(\"Actual Attacks\")\n",
    "plt.text(0.25, 0.99,'Predicted Non-attacks\\n(False Negatives)',\n",
    "     horizontalalignment='center',\n",
    "     verticalalignment='top',\n",
    "     transform = ax.transAxes)\n",
    "plt.text(0.75, 0.99,'Predicted Attacks\\n(True Positives) ',\n",
    "     horizontalalignment='center',\n",
    "     verticalalignment='top',\n",
    "     transform = ax.transAxes)\n",
    "plt.xlabel(\"Confidence score\")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question**: Why do you think the model appears to be struggling more in the case of attacks than non-attacks? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kL5iasGukFp"
   },
   "source": [
    "In many cases, we will need to turn the confidence score between 0 and 1 into a model prediction, non-attack or attack. How do we do this? \n",
    "\n",
    "As suggested by the plot above, we can define a decision threshold (vertical dotted line) so that everything to the left of the vertical line is predicted to be a non-attack and everything to the left is predicted to be an attack. In the plots, the decision threshold is set at 0.5, but there's no rule that says we have to set it there. Depending upon our model and application, we may want to choose a smaller or larger value. Note that when we calculated accuracy in previous examples, we assumed a decision threshold of 0.5. \n",
    "\n",
    "We'll discuss decision thresholds in more detail when we get to the classification metrics section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM4BmLeS4qb8"
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKrTlsaj5071"
   },
   "source": [
    "### Metrics for regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycmuEeB9DM7R"
   },
   "source": [
    "##### MSE/RMSE, MAE, and R2\n",
    "\n",
    "Mean Square Error (MSE) is one of the most commonly used model metrics for regression models -- where we're trying to use our model to predict some quantity. \n",
    "\n",
    "The MSE model metric is simple. To evaluate a model's MSE, you simply use the model to predict each target in your test set. For each target you predicted, you get some error -- how far your prediction is from the target. If you take all your errors, square them, and sum them, you have the MSE of your model on the test set. To get the RMSE, you just take the square root of the MSE.\n",
    "\n",
    "MSE and RMSE are essentially two ways of expressing the same information. At root, RMSE is more interpretable because it is on the same scale as the original data -- if your target is, e.g.,  *dollars*, then the RMSE will also be expressed in dollars. On the other hand, MSE can be more practical, because it's slightly easier to work with (mathematically) in most contexts.\n",
    "\n",
    "Let's take a look at an application of MSE.\n",
    "\n",
    "The below code fits a simple linear regression model for a single measure of diabetes progression. The regressor in this case is body mass index (BMI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "2H9xUdgFIWGL",
    "outputId": "617d114a-be25-4936-e427-9480fe584cbb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: ', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "print('Root mean squared error: %.2f'\n",
    "      % np.sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred)))\n",
    "print('Mean absolute error: %.2f'\n",
    "      % mean_absolute_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xlabel('BMI (centered, standardized)')\n",
    "plt.ylabel('Measure of diabetes progression')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "483ZwpK_N8Wg"
   },
   "source": [
    "That's all well and good, but our dataset actually contains more than just BMI; it also contains information about patients' age and blood pressure. We could use a multiple linear regression model that includes *all* of these measures, but it's also interesting to see which of them is the best one to include in a simple (single-regressor) model. Among other things, this helps us figure out which measure is most closely related to diabetes.\n",
    "\n",
    "Toward that end, let's look at our model metrics for each of the three models under consideration; the one (already examined above) based on age, one based on BMI, and one based on blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "id": "gsAclBBD--ZS",
    "outputId": "8580837b-cace-4cfe-dbee-8c3a4a1cef76"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X_age = diabetes_X[:, np.newaxis, 0]\n",
    "diabetes_X_bp = diabetes_X[:, np.newaxis, 3]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_age_train = diabetes_X_age[:-20]\n",
    "diabetes_X_age_test = diabetes_X_age[-20:]\n",
    "diabetes_X_bp_train = diabetes_X_bp[:-20]\n",
    "diabetes_X_bp_test = diabetes_X_bp[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_age_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_age_test)\n",
    "\n",
    "# The coefficients\n",
    "print('AGE:')\n",
    "print('Coefficients: ', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "print('Root mean squared error: %.2f'\n",
    "      % np.sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred)))\n",
    "print('Mean absolute error: %.2f'\n",
    "      % mean_absolute_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_age_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_age_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xlabel('Age (centered, standardized)')\n",
    "plt.ylabel('Measure of diabetes progression')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(diabetes_X_bp_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "diabetes_y_pred = regr.predict(diabetes_X_bp_test)\n",
    "\n",
    "# The coefficients\n",
    "print('BLOOD PRESSURE:')\n",
    "print('Coefficients: ', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "print('Root mean squared error: %.2f'\n",
    "      % np.sqrt(mean_squared_error(diabetes_y_test, diabetes_y_pred)))\n",
    "print('Mean absolute error: %.2f'\n",
    "      % mean_absolute_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.scatter(diabetes_X_bp_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_bp_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xlabel('Blood pressure (centered, standardized)')\n",
    "plt.ylabel('Measure of diabetes progression')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbEv3RLoDsiD"
   },
   "source": [
    "We see that age is not nearly as good a basis for inferring diabetes progression as is BMI. Blood pressure is much closer -- much better than age -- but is still not as good as BMI. So if, for some reason, we wanted to choose a single metric to use to predict someone's progression in diabetes measures, we would settle on BMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvWJ5URW5wzz"
   },
   "source": [
    "### Metrics for classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sWnDg4aWk9s"
   },
   "source": [
    "##### TP, FP, TN, FN and the Confusion Matrix\n",
    "\n",
    "An understanding of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN) is the foundation for understanding all of the other metrics we will discuss in the context of classification. This topic is not complicated, but it can be a challenge to keep everything straight in your head. \n",
    "\n",
    "First, we need to understand what we mean by _Positive_ and _Negative_. Both of these refer to one of the classes in a two-class comparison, and actually it's up to you to decide which class is called \"Positive\" and which class is called \"Negative\". Usually, the more interesting class is assigned the Positive label. For our attack tweet data, this would be the tweets which are attacks (non-attacks are Negatives). In a cancer screening application, this would be the cases where the subject is cancerous (non-cancerous are Negatives). Usually, but not always, the positive class ends up being the minority class. \n",
    "\n",
    "Now we're ready to understand TP, FP, etc.:\n",
    "* **True Positive**:  _predicts_ positive; _actually_ positive\n",
    "* **True Negative**:  _predicts_ negative; _actually_ negative\n",
    "* **False Positive**:  _predicts_ positive; _actually_ negative\n",
    "* **False Negative**:  _predicts_ negative; _actually_ positive\n",
    "\n",
    "The matrix below, called the **Confusion Matrix**, summarizes the performance of a model by looking at the number of test set examples that fall into each possible scenario. This is a very common way to present model performance, so it's worth pausing to make sure you understand it. \n",
    "\n",
    "|   |Actually Negative   | Actually Positive  |\n",
    "|:-:|:-:|:-:|\n",
    "|  **Predicted Negative** |  True Negative | False Negative  |\n",
    "|  **Predicted Positive** |  False Positive | True Positive  |\n",
    "\n",
    "Above, we discussed how most machine learning models output a confidence score, and, in order to get class label predictions, we have to apply a decision threshold. Clearly, then, the values in the confusion matrix will depend on our choice for the decision threshold. Look again at the [confidence scores histograms](https://colab.research.google.com/drive/1jw_JRCrGougID6n2mruf2BqksImGrlB-?authuser=1#scrollTo=FkJsS_hWUzt7) above. Notice how the different regions of the plot are lablled. It's worth stopping to make sure these labels make sense to you.\n",
    "\n",
    "Below we show the confusion matrix for the best model that we trained in the [bias-variance tradeoff](https://colab.research.google.com/drive/1jw_JRCrGougID6n2mruf2BqksImGrlB-?authuser=1#scrollTo=TzFhGIhX6WJL) section (looking at the plot in that section, this is the model with C=100). The 3 different confusion matrices below are for three different values of the decision threshold (0.25, 0.5, and 0.75). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EaacJ4nN3jpJ",
    "outputId": "da3f78af-29fe-4cf9-e968-3aad7ff2c2f7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_prob = model_cv.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Note that sklearn uses the opposite convention for rows and columns than used above\n",
    "# So, we transpose the output of the sklearn function.\n",
    "\n",
    "# Confusion Matrix (eps = 0.25)\n",
    "cm25 =  confusion_matrix(y_test, (y_prob>0.25).astype(int)).transpose()\n",
    "print(\"Confusion matrix for threshold=0.25: \\n\", cm25)\n",
    "\n",
    "# Confusion Matrix (eps = 0.50)\n",
    "cm50 = confusion_matrix(y_test, (y_prob>0.50).astype(int)).transpose()\n",
    "print(\"Confusion matrix for threshold=0.50: \\n\", cm50)\n",
    "\n",
    "# Confusion Matrix (eps = 0.75)\n",
    "cm75 = confusion_matrix(y_test, (y_prob>0.75).astype(int)).transpose()\n",
    "print(\"Confusion matrix for threshold=0.75: \\n\", cm75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JE8on2qX5NN3"
   },
   "source": [
    "How would you describe what happens to outputs of our model as we increase the decision threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltJn-YIchaYt"
   },
   "outputs": [],
   "source": [
    "# save the results\n",
    "df_results = pd.DataFrame([])\n",
    "df_results['y_test'] = y_test\n",
    "df_results['y_predicted_prob'] = y_prob\n",
    "df_results.to_csv('logistic_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GDwPOH4YbGJ"
   },
   "source": [
    "##### TP, FP, TN, FN Rates\n",
    "\n",
    "The numbers in the confusion matrix above scale with the size of our test dataset. This makes it hard to compare between different applications or test data sets. To solve this problem, we can define rates which scale with the size of the test dataset:\n",
    "* **True Positive Rate (TPR)** -- TP divided by the number of actual positives, $P$ : \n",
    "$$ TPR = \\frac{TP}{P} = \\frac{TP}{TP+FN} $$\n",
    "This metric answers the question, \"what fraction of the positive cases will the model correctly identify?\". You may see this called \"sensitivity\", \"recall\", or \"hit rate\" -- sorry.\n",
    "* **True Negative Rate (TNR)** - TN divided by the number of actual negative, $N$:\n",
    "$$ TNR = \\frac{TN}{N} = \\frac{TN}{TN+FP} $$\n",
    "You may see this called \"specificity\". Because the positive class is usually more interesting, this metric is not used very frequently. \n",
    "* **False Positive Rate (FPR)** - FP divided by the number of actual negatives:\n",
    "$$ FPR = \\frac{FP}{N} = \\frac{FP}{TN+FP} = 1-TNR$$\n",
    "This is the fraction of negatives that were incorrectly predicted to be positive. False positives are often very costly (think of the cancer screening example). You may see this called the \"fall-out rate\". \n",
    "* **False Negative Rate (FNR)** - FN divided by the number of actual positives:\n",
    "$$ FNR = \\frac{FN}{P} = \\frac{FN}{TP+FN} = 1-TPR$$\n",
    "This is the fraction of positives that were incorrectly predicted to be negative. Like TNR, you won't run into this one very often. Sometimes this is called the \"miss rate\". \n",
    "\n",
    "Who can remember all these names?! TPR and FPR are the most important here. \n",
    "\n",
    "Let's look at TPR and FPR for our model for the three threshold values we used above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4H1-EFSXOK2",
    "outputId": "ddea7d01-6462-4ef4-8f8f-e8eac902bd75"
   },
   "outputs": [],
   "source": [
    "def cm_rates(cm):\n",
    "    tpr = cm[1,1]/cm[:,1].sum()\n",
    "    tnr = cm[0,0]/cm[:,0].sum()\n",
    "    fpr = 1 - tnr\n",
    "    fnr = 1 - tpr\n",
    "    \n",
    "    return dict(tpr= tpr, tnr= tnr, fpr= fpr, fnr= fnr)\n",
    "\n",
    "cmr25 = cm_rates(cm25)\n",
    "cmr50 = cm_rates(cm50)\n",
    "cmr75 = cm_rates(cm75)\n",
    "\n",
    "print(\"Threshold = 0.25:\\nTPR: \", round(cmr25['tpr'],2), \"\\nFPR: \", round(cmr25['fpr'],2))\n",
    "print(\"\\nThreshold = 0.50:\\nTPR: \", round(cmr50['tpr'],2), \"\\nFPR: \", round(cmr50['fpr'],2))\n",
    "print(\"\\nThreshold = 0.75:\\nTPR: \", round(cmr75['tpr'],2), \"\\nFPR: \", round(cmr75['fpr'],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQxjLMBHbCch"
   },
   "source": [
    "Why is there a tradeoff between TPR and FPR as we change the threshold?\n",
    "\n",
    "How does class imbalance effect our interpretation of these numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxv0Ab1VjFks"
   },
   "source": [
    "##### Accuracy\n",
    "\n",
    "Accuracy answers the question, \"what fraction of my model's predictions were correct?\". \n",
    "$$\n",
    "\\mathrm{Accuracy} = \\frac{TP + TN}{P + N} \n",
    "$$\n",
    "Accuracy is very sensitive to class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5p4XjRmYb6Y"
   },
   "source": [
    "##### Precision and Recall\n",
    "\n",
    "Taken together, these two metrics give a good sense for the performance of a binary classification model:\n",
    "\n",
    "* **Precision**: the percentage of predicted attacks that are actually attacks.  Higher precision suggests that when a model predicts that a tweet is an attack, it is more likely to be correct.  Note that a model with high precision can still be inadequate if the model fails to correctly identify a sufficient number of the actual attacks.  A precision of 1 can be achieved simply by predicting that no tweets are attacks. In terms of quantities defined above\n",
    "$$ \\mathrm{Precision} = \\frac{TP}{\\mathrm{Predicted\\, Positives}} = \\frac{TP}{TP + FP} $$\n",
    "* **Recall**: the percentage of actual attacks that are predicted to be attacks.  Higher recall suggests that if a tweet is actually an attack, the model is more likely to identify it as such.  Note that a model with very high recall can still be inadequate if an insufficient number of the predicted attacks are actually attacks.  A recall of 1 can be achieved by simply predicting that all tweets are attacks. Recall is equal to $TPR$ defined above:\n",
    "$$ \\mathrm{Recall} = TPR = \\frac{TP}{P} = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "The figure below demonstrates these metrics visually. The \"relevant\" label is what we have been calling \"Actual Positives\". \n",
    "\n",
    "![precision and recall](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)\n",
    "\n",
    "Like all of the metrics we've discussed so far, precision and recall depend upon the decision threshold. We can increase the precision by decreasing the threshold at the cost of decreasing the recall or visa-versa. The Precision-Recall curve, shown below, looks at the values of precision and recall as the threshold is tuned between 0 and 1. The numbered points show the decision threshold for a few points along the curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "_sm4F2UKLKwQ",
    "outputId": "c955dd1f-2cd6-4be4-f553-70f388296a03"
   },
   "outputs": [],
   "source": [
    "def fmt_rat(flt,dec=3):\n",
    "    flt = str(round(flt,dec))\n",
    "    return flt\n",
    "\n",
    "# Precision-Recall Analysis:\n",
    "from sklearn import metrics\n",
    "precision, recall, eps = metrics.precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "# precision recall curve\n",
    "fig, ax = plt.subplots();\n",
    "ax.plot(recall, precision, 'b-')\n",
    "ax.set(xlabel='recall', ylabel='precision',ylim=(0,None))\n",
    "ax.set_aspect('equal')\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "\n",
    "# annotations\n",
    "idx = np.linspace(0,len(eps)-3,20).astype(int)-1\n",
    "ax.plot(recall[idx], precision[idx], 'ko')\n",
    "for i in idx:\n",
    "    ax.annotate(fmt_rat(eps[i],2), (recall[i]+.02, precision[i]+.02), \n",
    "                horizontalalignment='left', \n",
    "                verticalalignment='bottom',\n",
    "                fontsize=12\n",
    "               )\n",
    "    \n",
    "fig.set_size_inches(6,6)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4xFBL6eWMtf"
   },
   "source": [
    "Note how, overall, precision and recall trade off with one another. Ideally, we would like both to be as high as possible, but we may be more concerned about one type of error. For the anaysis of attack tweets, we may be content with a lower recall (not picking out all attacks) as long as we have high precision (most predicted attacks are actually attacks). So, we could choose a threshold that favors precision over recall, such as the point near $\\epsilon=0.86$ where the precision is near $90\\%$ and the recall is near $25\\%$. \n",
    "\n",
    "We pointed out earlier how *accuracy* can be misleading for imbalanced datasets. Is there a similar problem with *precision* and *recall*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPSbRdLXYek3"
   },
   "source": [
    "##### F1 Score\n",
    "\n",
    "\n",
    "The **F1 score**: a composite metric equal to the harmonic mean of precision and recall:\n",
    "$$\n",
    "F1 = \\frac{\\mathrm{precision} \\times \\mathrm{recall}}{\\tfrac{1}{2}\\left(\\mathrm{precision} + \\mathrm{recall}\\right)}\n",
    "$$ \n",
    "By combining precision and recall in this way, $F1$ represents the overall performance for a specified threshold. Below, we show the maximum possible F1 for our trained model along with the associated precision, recall, and decision threshold:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-iHU3FvYrlb",
    "outputId": "83164eed-115f-4be1-d01d-3a9d9aeee8a3"
   },
   "outputs": [],
   "source": [
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "f1=f1[f1==f1]\n",
    "f1max_ind = np.argmax(f1)\n",
    "print(\"Maximum F1 score: \" + fmt_rat(f1[f1max_ind]))\n",
    "print(\"Precision at max: \" + fmt_rat(precision[f1max_ind]))\n",
    "print(\"Recall at max: \" + fmt_rat(recall[f1max_ind]))\n",
    "print(\"Optimal threshold: \" + fmt_rat(eps[f1max_ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oIXkpzYZUcE"
   },
   "source": [
    "Try to find this point on the plot above. When will this be a good choice for the decision threshold?\n",
    "\n",
    "While F1 does give a single number for model performance, that number still depends upon the decision threshold which is based on application-specific considerations. It would be great to have a single metric which didn't depend on a choice of threshold!\n",
    "\n",
    "Does F1 do a good job of measuring model performance for datasets with class imbalance? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaGiQOJ8YfKs"
   },
   "source": [
    "\n",
    "##### Receiver Operating Characteristic (ROC)\n",
    "The **Receiver Operating Characteristic (ROC)** Curve shows the true-positive rate vs the false-positive rate. Like the Precision-Recall curve, the points are generated by different choices of the decision threshold. Also like precision and recall, TPR and FPR trade off against one another as we vary the threshold. Below, we show the ROC Curve for our trained model evaluated on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "TcmzozKOdSJH",
    "outputId": "aa98c1ed-33d5-4332-fff2-cc85c3dd64a2"
   },
   "outputs": [],
   "source": [
    "# ROC-AUC Analysis\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob, pos_label=1)\n",
    "roc_auc_test = metrics.roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# roc curve\n",
    "fig, ax = plt.subplots() \n",
    "fig.set_size_inches(6,6)\n",
    "ax.plot(fpr, tpr, 'b-', label='Model, AUC: ' + fmt_rat(roc_auc_test))\n",
    "ax.plot([0,1],[0,1],'k--', label='Chance, AUC: 0.5')\n",
    "ax.set(xlabel='FPR', ylabel='TPR')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# annotations\n",
    "idx = np.linspace(0,len(thresholds)-3,15).astype(int)-1\n",
    "ax.plot(fpr[idx], tpr[idx], 'ko')\n",
    "for i in idx:\n",
    "    ax.annotate(fmt_rat(thresholds[i],2), (fpr[i]+0.02, tpr[i]-.05), \n",
    "                horizontalalignment='left', \n",
    "                verticalalignment='bottom',\n",
    "                fontsize=12\n",
    "               )\n",
    "\n",
    "ax.legend()\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_Yrz1W2fzTF"
   },
   "source": [
    "\n",
    "The area under the ROC curve (**ROC-AUC**) turns out to be a great threshold-independent way to quantify overall model performance. There are several nice properties:\n",
    "\n",
    "1. The AUC is independent of choice of threshold! This means we don't have to make any assumptions about how the model will be used when evaluating the goodness of fit of the model.\n",
    "2. A coin-flip model will produce an AUC of 0.5, and a perfect model will produce an AUC of 1. These boundaries make it easy to interpret the ROC score.\n",
    "3. AUC only depends on the sorting of the confidence scores predicted by the model, not the actual confidence scores themselves. Well-fitting, but poorly calibrated models will still receive strong ROC scores.\n",
    "4. Because of points 1 and 3, the ROC-AUC can be directly compared for different models of the same data.\n",
    "5. The ROC-AUC score is not biased to be larger/smaller for class-imbalanced datasets.\n",
    "\n",
    "ROC-AUC is a great metric to use while you are experimenting with different modeling approaches and trying to decide what works best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhDy3x-xoyT_"
   },
   "source": [
    "#### shortcoming of simple train/test split\n",
    "The train/test split has the latent assumption that the error on the test set is a good proxy for the error on new unseen data. But our test sample will always be non-representative, to some extent, of the total population. In the train/test approach, you don't have any way to estimate how different your test data is from the population.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxLuhRBn7Qkz"
   },
   "source": [
    "#### k-Fold Cross Validation\n",
    "\n",
    "We've emphasized the importance of using a witheld test set to evaluate our models in order to get an unbiased estimate of model performance. But what if our test set happens to be substantially different from the full population that we are sampling from? It may be that we substantially underestimate or worse overestimate the performance of our model. For instance, maybe some Tweets are very easy to classify as attack or non-attack while others are very difficult to classify with a given modeling approach. If we happen to get a larger portion of the easy cases in our test dataset, then our evaluation metrics (precision, recall, F1, ROC-AUC) will be larger than if we had a representative distribution of easier and harder Tweets! This problem is especially pronounced with smaller datasets such as ours. Note that this is not an error of bias, but rather a random statistical error. \n",
    "\n",
    "To demonstrate, let's split our dataset into several groups of equal size and withold in turn each of the five as our test dataset. We will train the model on the remaining four, and calculate the ROC-AUC score on the fifth to see how much the score varies from group to group. The figure below demonstrates this procedure:\n",
    "\n",
    "![cross validation](https://i.stack.imgur.com/1fXzJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8YxazyYkAfZ"
   },
   "source": [
    "The code below performs this splitting and outputs the AUC score for each of 10 splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9k3JZGWkAAu",
    "outputId": "fb82415c-2e2d-4f24-f38b-104492f1c44f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# define how splitting is performed\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# loop over splits\n",
    "i=1\n",
    "aucs = []\n",
    "for train, test in cv.split(X, y):\n",
    "  # train model\n",
    "  model_split = model_cv.best_estimator_.fit(X.iloc[train], y.iloc[train])\n",
    "  # get test probs\n",
    "  y_test_probs = model_split.predict_proba(X.iloc[test])[:,1]\n",
    "  # get roc-auc\n",
    "  auc_split = metrics.roc_auc_score(y.iloc[test], y_test_probs)\n",
    "  aucs.append(auc_split)\n",
    "  \n",
    "  print(\"Split \" + str(i) +\". ROC-AUC: \" , fmt_rat(auc_split, 3))\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaqRZHPU1uK4"
   },
   "source": [
    "The different splits give quite different scores! \n",
    "\n",
    "Often the AUC score is used to choose between candidate models. Given the variation we see above, how could we decide between competing models?  One obvious step is to use the mean value of the 10 scores we calculated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ixSOUwB3Gbg",
    "outputId": "9a049de4-0e0f-4b55-f5e9-1ad60d469e2e"
   },
   "outputs": [],
   "source": [
    "mean_auc = np.mean(aucs)\n",
    "print(\"Mean value of ROC-AUC scores for \"+ str(len(aucs)) + \" splits: \", fmt_rat(mean_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu6sEnux3mdH"
   },
   "source": [
    "This is better than just using one random test set, but it still doesn't take into account the variation among the auc scores. The standard error of the mean tells us how uncertain our estimate of this mean value is given the variation in the auc scores and the number of splits. The standard error of the mean (SEM) is estimated as \n",
    "$$\n",
    "\\mathrm{SEM} = \\frac{\\mathrm{Standard \\, Deviation \\, of \\, Scores}}{\\sqrt{\\mathrm{Number \\, of \\, Scores}}}\n",
    "$$\n",
    "The code below calculates the standard error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OH90w-Wj41gH",
    "outputId": "e60b1ccb-f010-4864-bd20-13cdfb40cf3f"
   },
   "outputs": [],
   "source": [
    "SEM_auc = np.std(aucs) / np.sqrt(len(aucs))\n",
    "print(\"SEM value of ROC-AUC scores for 10 splits: \", fmt_rat(SEM_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-_Ilu5J5to6"
   },
   "source": [
    "So, even though the variation in auc score between splits is quite large, the error in our estimate of the mean is quite small. Statisticians love 95% confidence intervals. Assuming the auc scores are normally distributed, we can now calculate a 95% confidence interval in the estimated 95% confidence interval. It turns out that the bounds of this confidence interval equals\n",
    "$$\n",
    "\\mathrm{95\\% \\, Conf.\\, Int.} = \\mathrm{Mean \\, AUC  \\mp t(\\alpha=0.05, \\mathrm{df}=9)\\times SEM}\n",
    "$$\n",
    "The code below calculates this confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1FridHd6AUZ",
    "outputId": "3d5e592c-68d2-400a-df71-e65f3ad714b2"
   },
   "outputs": [],
   "source": [
    "t_score = np.abs(t.ppf(0.05, 10-1))\n",
    "print(\"t-value:\", t_score)\n",
    "print(\"95%% Conf. Int. in mean ROC-AUC score: [%0.3f, %0.3f]\" % (mean_auc -  t_score * SEM_auc, mean_auc + t_score * SEM_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87-p2txCelvY"
   },
   "outputs": [],
   "source": [
    "# put the above steps for computing confidence interval into a function\n",
    "def conf_int(ar):\n",
    "  mean = np.mean(ar)\n",
    "  sem = np.std(ar) / np.sqrt(len(ar))\n",
    "  t_score = np.abs(t.ppf(0.05, len(ar)-1))\n",
    "  return (mean - t_score * sem, mean + t_score * sem )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj4KOGzG7-zp"
   },
   "source": [
    "Now we are ready to compare different models!\n",
    "\n",
    "Say we used a completely different model, went through the procedure above and arrived at a confidence band $[0.942, 0.961]$. In that case we would be very confident that the latter model was superior. Alternatively, if the second model had a confidence band of $[0.920, 0.95]$ we wouldn't be sure -- the second model could be superior, but the differences could also be explained by the error resuling from the fact that our data set is just a sample.\n",
    "\n",
    "It turns out that the procedure we went through above has a name -- **k-Fold Cross Validation**. This is the de-facto method for calculating robust estimates of model performance. Make sure you understand it!\n",
    "\n",
    "Can you think of any limitations or pitfalls with this procedure? \n",
    "\n",
    "To demonstrate the real application of this procedure, we will repeat the process above, but we will also train a classification model called a Multilayer Perceptron Classificer in addition to the Logistic Regression model that we've been working with above. Multilayer Perceptrons are the simplest type of Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDMf4nekq18b",
    "outputId": "3aa48298-dac4-4d4a-e273-57c8d7317dc7"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as Clf\n",
    "\n",
    "mlp_params = {\n",
    "    'alpha':  np.logspace(-0.8,0.3,8, 30, 80),\n",
    "    'hidden_layer_sizes': [(100,),]\n",
    "}\n",
    "\n",
    "mlp_cv = GridSearchCV(Clf(max_iter=400), mlp_params, scoring='roc_auc', cv=4, n_jobs=6, verbose=10)\n",
    "mlp_cv.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "zOYOOVousSua",
    "outputId": "d32847cc-05a5-47bd-b656-1884885d5613"
   },
   "outputs": [],
   "source": [
    "bias_variance = mlp_params['alpha']\n",
    "scores_test = mlp_cv.cv_results_[\"mean_test_score\"]\n",
    "\n",
    "print(\"Optimal value: alpha=%0.3f\" % mlp_cv.best_estimator_.alpha)\n",
    "\n",
    "# Plot the scores\n",
    "plt.plot(bias_variance, scores_test, 'bo-', label='test')\n",
    "plt.legend()\n",
    "plt.xlabel(\"(<--Variance) alpha (Bias-->)\")\n",
    "plt.ylabel(\"ROC_AUC\")\n",
    "plt.xscale('log')\n",
    "plt.title(\"Bias-Variance Tradeoff\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpJrWfdu0b0W",
    "outputId": "3e4f9a49-41e7-4b2e-9101-09a4b8cc20db"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.svm import SVC as Clf\n",
    "\n",
    "# define how splitting is performed\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# loop over splits\n",
    "i=1\n",
    "aucs_mlp = []\n",
    "for train, test in cv.split(X, y):\n",
    "  Xtr, ytr = X.iloc[train], y.iloc[train]\n",
    "  Xte, yte = X.iloc[test], y.iloc[test]\n",
    "  \n",
    "  # train models\n",
    "  model_split = mlp_cv.best_estimator_.fit(Xtr, ytr)\n",
    "  \n",
    "  # get test probs\n",
    "  y_test_probs = model_split.predict_proba(Xte)[:,1]\n",
    "  # get roc-auc\n",
    "  auc_split = metrics.roc_auc_score(yte, y_test_probs)\n",
    "  aucs_mlp.append(auc_split)\n",
    "  \n",
    "  print(\"MLP Split \" + str(i) +\". ROC-AUC: \" , fmt_rat(auc_split, 3))\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_jBdIs89jnN"
   },
   "source": [
    "Now let's calculat the 95% confidence interval for the ROC-AUC and compare with the Logistic Regression model from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EolN6gOE8s8b"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "print(\"Logistic Regression: \\n  Mean ROC-AUC:\", \n",
    "      fmt_rat(np.mean(aucs)),\n",
    "      \"\\n  95% Conf. Int.:\", \n",
    "      conf_int(aucs),\n",
    "      \"\\n\"\n",
    "     )\n",
    "\n",
    "print(\"Multilayer Perceptron Classifier: \\n  Mean ROC-AUC:\", \n",
    "      fmt_rat(np.mean(aucs_mlp)),\n",
    "      \"\\n  95% Conf. Int.:\", \n",
    "      conf_int(aucs_mlp)\n",
    "     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpJztEHhEvRi"
   },
   "source": [
    "The Mean ROC-AUC for the Multilayer Perceptron is higher than Logistic Regression, and the confidence bands have almost no overlap. We can now say with confidence that the neural network model performs better with regard to ROC-AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spinUnXl5W_T"
   },
   "source": [
    "#### Variants\n",
    "There are many variants of cross validations that work better or worse in various situations. For example, for very small datasets, k-fold cross validation has the problem, that there isn't enough data to get enough sub-sample estimates to accuractly estimate the sample mean and SEM. In this case, a technique called Repeated k-fold cross validation can be useful. See here for the sklearn implementation: [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html). \n",
    "\n",
    "[This guide](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation), also from sklearn, gives a fantastic overview of various cross-validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ifkYLYk8vyu"
   },
   "source": [
    "## How to prevent overfitting\n",
    "\n",
    "So say you've got a model and you know or suspect that your model variance is too high, and you're overfitting your training data. What strategies can you take to fix this? There are three simple steps you can take, which we'll consider one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9b8n-qa-tU3"
   },
   "source": [
    "#### Reduce model complexity\n",
    "\n",
    "The first strategy is to reduce the complexity of your model. This is usually (but not always) the same as reducing the number of parameters in your model.\n",
    "\n",
    "One way to think about this is that the parameters of the model -- for example, the coefficients of the regression terms in a linear regression model, or the weights and biases in a neural network -- are the way the model stores information. If your model has enough \"storage capacity\", then it will solve the AI problem you're working on by just \"memorizing\" the way your training data looks. \n",
    "\n",
    "Think of it like when instructors let you bring a 3x5 index card of notes to an exam. They specify 3x5 because if they let you bring giant broadsheets of notes, then your learning might be too shallow; you will pass the exam just by virtue of having brought all that information with you. Reducing you to a 3x5 requires you to do some deep learning before the exam. AI is the same way; **reducing the model complexity forces the model to learn the underlying phenomenon that produced the training data, instead of just learning the training data itself**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5nQCD9TBbrr"
   },
   "source": [
    "Let's see this in action. Below, I generate some data for a regression problem. I have one feature, `X`, and I generate a response `Y` that we want to try to model. Since I'm the one making this data, I can tell you that `Y` is just three times `X` with some noise added. But don't tell the model that -- we want to see how well the model can learn this relationship just by looking at some training data.\n",
    "\n",
    "In this cell, I generate the data, separate it into train and test sets, and plot the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcF-fOpoE1vC"
   },
   "outputs": [],
   "source": [
    "# Make some fake data\n",
    "\n",
    "# Import what we need\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(355)\n",
    "\n",
    "# Make the data\n",
    "datasize=14\n",
    "Xlin = np.sort(rng.uniform(low=-4,high=4,size=datasize))\n",
    "Y = 3*Xlin + rng.normal(loc=1,scale=8,size=datasize)\n",
    "\n",
    "# Divide into train and test\n",
    "Xlin_train, Xlin_test, Y_train, Y_test = train_test_split(\n",
    "    Xlin, Y, test_size=0.5, random_state=355)\n",
    "\n",
    "plt.plot(Xlin_train,Y_train,'ro')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMbl23-mCULv"
   },
   "source": [
    "Now let's try a model that is overly complex -- a 4-degree polynomial regression. The real phenomenon is a 1-degree polynomial: `Y = 3X + noise`. Of course, since we're pretending we don't know the true relationship between `X` and `Y`, we also wouldn't know that this model is too complex. But in general, high-degree polynomial fits are prone to being overly complex. It's very rare you'd want to go above a 3-degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7MrjYmUEcES"
   },
   "source": [
    "In the cell below, we fit a degree 4 polynomial regression model on the training data and look at the resulting fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "Uab-W8cpH7tD",
    "outputId": "f389221a-564e-4d60-b8c3-d850f1f52094"
   },
   "outputs": [],
   "source": [
    "# Fit complex model (high-degree polynomial regression)\n",
    "high_degree=4\n",
    "hdp = np.poly1d(np.polyfit(Xlin_train,Y_train,deg=high_degree))\n",
    "\n",
    "# Take a look\n",
    "Xplot = np.linspace(-4,4)\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "axs[0].plot(Xplot,hdp(Xplot))\n",
    "axs[0].plot(Xlin_train,Y_train,'ro')\n",
    "axs[1].plot(Xplot,hdp(Xplot))\n",
    "axs[1].plot(Xlin_train,Y_train,'ro')\n",
    "axs[1].set_ylim(-25,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeTVF-n2EosQ"
   },
   "source": [
    "You can see that our line fits the training data quite well. You also might be worried about that enormous dropoff on the right, that doesn't seem to be reflected in our training data. This is one of the dangers of high-degree polynomial fits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu6ewN63E2TK"
   },
   "source": [
    "In the cell below we find the RMSE of the complex model on the training set as well as on the validation set. We also add the test set data to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "8njNZCK0N8A9",
    "outputId": "d9214aca-09cf-4ef4-af2f-2ed893476edc"
   },
   "outputs": [],
   "source": [
    "# Get RMSE on train and test set\n",
    "\n",
    "# Import\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Get metrics\n",
    "train_rmse = mean_squared_error(Y_train, hdp(Xlin_train), squared=False)\n",
    "test_rmse = mean_squared_error(Y_test, hdp(Xlin_test), squared=False)\n",
    "\n",
    "# Print\n",
    "print('Training set RMSE: ',train_rmse)\n",
    "print('Test set RMSE: ',test_rmse)\n",
    "\n",
    "# Take a look\n",
    "Xplot = np.linspace(-4,4)\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "axs[0].plot(Xplot,hdp(Xplot))\n",
    "axs[0].plot(Xlin_train,Y_train,'ro')\n",
    "axs[0].plot(Xlin_test,Y_test,'bo')\n",
    "axs[1].plot(Xplot,hdp(Xplot))\n",
    "axs[1].plot(Xlin_train,Y_train,'ro')\n",
    "axs[1].plot(Xlin_test,Y_test,'bo')\n",
    "axs[1].set_ylim(-25,15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFnpZaZmFLJb"
   },
   "source": [
    "Both the RMSE numbers and the plots look pretty abysmal. We've fit a model that is flexible enough to fit itself very nicely to the training data, and in the process it completely misses the true underlying phenomenon. We've fit the *noise* in the training data, which is what we need our model to ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2TcwHFIFe8x"
   },
   "source": [
    "In the cell below, we fit a degree one polynomial. In other words, this is simple linear regression, and it's the right model for this circumstance. We also find the RMSE for the training and test sets, and plot the model along with the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQjr_2NyQLXJ",
    "outputId": "72799de4-6471-4147-ca6b-38bed549b3e1"
   },
   "outputs": [],
   "source": [
    "# Fit simple model (low-degree polynomial regression)\n",
    "low_degree=1\n",
    "ldp = np.poly1d(np.polyfit(Xlin_train,Y_train,deg=low_degree))\n",
    "\n",
    "# Get metrics\n",
    "train_rmse = mean_squared_error(Y_train, ldp(Xlin_train), squared=False)\n",
    "test_rmse = mean_squared_error(Y_test, ldp(Xlin_test), squared=False)\n",
    "\n",
    "# Print\n",
    "print('Training set RMSE: ',train_rmse)\n",
    "print('Test set RMSE: ',test_rmse)\n",
    "\n",
    "# Take a look\n",
    "Xplot = np.linspace(-4,4)\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "axs[0].plot(Xplot,ldp(Xplot))\n",
    "axs[0].plot(Xlin_train,Y_train,'ro')\n",
    "axs[0].plot(Xlin_test,Y_test,'bo')\n",
    "axs[1].plot(Xplot,ldp(Xplot))\n",
    "axs[1].plot(Xlin_train,Y_train,'ro')\n",
    "axs[1].plot(Xlin_test,Y_test,'bo')\n",
    "axs[1].set_ylim(-25,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-dsJ4ayFtmX"
   },
   "source": [
    "Much better! Our training RMSE is still lower than our test RMSE, by a wide margin, but that's not unexpected, especially with such a small data set. They are much closer together than they were for the high-degree fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6n3gBT5GGh1"
   },
   "source": [
    "Now let's go all out -- let's fire up a neural network with a thousand internal nodes and apply it to our training data, to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7aVZS8ESP5y"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor as Rgr\n",
    "\n",
    "mlp_params = {\n",
    "    'alpha':  np.logspace(-0.8,0.3,8, 30, 80),\n",
    "    'hidden_layer_sizes': [(1000,)]\n",
    "}\n",
    "\n",
    "mlp_cv = GridSearchCV( Rgr(max_iter=400), mlp_params, scoring='neg_mean_squared_error', cv=4, n_jobs=6, verbose=0)\n",
    "mlp_cv.fit(Xlin_train.reshape(-1, 1),Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrgmvKNIUVNp"
   },
   "outputs": [],
   "source": [
    "# Fit MLP neural network\n",
    "mlpnn = mlp_cv.best_estimator_.predict\n",
    "\n",
    "# Get metrics\n",
    "train_rmse = mean_squared_error(Y_train, mlpnn(Xlin_train.reshape(-1, 1)), squared=False)\n",
    "test_rmse = mean_squared_error(Y_test, mlpnn(Xlin_test.reshape(-1, 1)), squared=False)\n",
    "\n",
    "# Print\n",
    "print('Training set RMSE: ',train_rmse)\n",
    "print('Test set RMSE: ',test_rmse)\n",
    "\n",
    "# Take a look\n",
    "Xplot = np.linspace(-4,4)\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "axs[0].plot(Xplot,mlpnn(Xplot.reshape(-1, 1)))\n",
    "axs[0].plot(Xlin_train,Y_train,'ro')\n",
    "axs[0].plot(Xlin_test,Y_test,'bo')\n",
    "axs[1].plot(Xplot,mlpnn(Xplot.reshape(-1, 1)))\n",
    "axs[1].plot(Xlin_train,Y_train,'ro')\n",
    "axs[1].plot(Xlin_test,Y_test,'bo')\n",
    "axs[1].set_ylim(-25,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8zdMOvHGUJf"
   },
   "source": [
    "Not bad! This model too is overly complex, but it didn't do anywhere near as poorly as the high-degree polynomial fit. But it isn't as good as the simple linear model -- and given our \"true\" phenomenon, it never could be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4TILzEvl_3M"
   },
   "source": [
    "#### Reduce data complexity (dimensional reduction)\n",
    "\n",
    "The second strategy we'll look at is reducing the complexity of your *data*, rather than that of your model. If you feed your model more data than it needs, it will use that data to \"cheat\" again. Think of it like this: even if some of the features you give your model are pure noise, completely useless, if you have enough of that data there will be some patterns in it (by pure chance) that the model can use to help itself memorize your training data and overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMu-lXU_JRwb"
   },
   "source": [
    "In the two cells below we take the same small data set from above and complicate it by adding a bunch of useless features, that are pure noise. Then, using the same linear regression model that was best above, we'll see how the extra data complexity affects our fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkAlUCI0eFEv"
   },
   "outputs": [],
   "source": [
    "# Make overly complex data\n",
    "Xcomplex = np.concatenate([Xlin.reshape(-1,1),\n",
    "                           rng.normal(loc=0,scale=.5,size=datasize).reshape(-1,1),\n",
    "                           rng.normal(loc=0,scale=.5,size=datasize).reshape(-1,1),\n",
    "                           rng.normal(loc=0,scale=.5,size=datasize).reshape(-1,1),\n",
    "                           rng.normal(loc=0,scale=.5,size=datasize).reshape(-1,1),\n",
    "                           rng.normal(loc=0,scale=.5,size=datasize).reshape(-1,1),\n",
    "                           rng.normal(loc=0,scale=.5,size=datasize).reshape(-1,1),\n",
    "                           rng.normal(loc=0,scale=.5,size=datasize).reshape(-1,1),\n",
    "                           rng.normal(loc=0,scale=.5,size=datasize).reshape(-1,1)],axis=1)\n",
    "\n",
    "# Divide into train and test\n",
    "Xcomplex_train, Xcomplex_test, Y_train, Y_test = train_test_split(\n",
    "    Xcomplex, Y, test_size=0.5, random_state=355)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNSQI2NWiqUn"
   },
   "outputs": [],
   "source": [
    "# Fit simple model (linear regression)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression().fit(Xcomplex_train,Y_train)\n",
    "\n",
    "# Get metrics\n",
    "train_rmse = mean_squared_error(Y_train, reg.predict(Xcomplex_train), squared=False)\n",
    "test_rmse = mean_squared_error(Y_test, reg.predict(Xcomplex_test), squared=False)\n",
    "\n",
    "# Print\n",
    "print('Training set RMSE: ',train_rmse)\n",
    "print('Test set RMSE: ',test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBELszH6LY03"
   },
   "source": [
    "Maybe this doesn't look so different from the good fit above, with Training RMSE around 3 and Test RMSE around 10. But look again at that Training RMSE from this fit -- it's almost 0. Our model is fitting itself almost *perfectly* to the training data, which makes it overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1_DeRVX-yp7"
   },
   "source": [
    "#### Get more data\n",
    "\n",
    "Let's look at our third strategy for preventing overfitting: get more data.\n",
    "\n",
    "The bigger your training set, the less likely it is that your training data will look very different from the test set. Part of the reason it's so easy to overfit the example we've been looking at is that we've only got a handful of cases in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlPxAuzgMH06"
   },
   "source": [
    "In the cell below, I generate a much bigger data set -- 500 cases -- from the same underlying phenomenon `Y = 3x + noise`. We'll see how this affects the fit of our simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irHuxVzt2J22",
    "outputId": "cefd690f-ffb8-450c-f682-e2f63e7a7ec7"
   },
   "outputs": [],
   "source": [
    "# Now try linear model with big data\n",
    "\n",
    "# Make big data\n",
    "datasize=500\n",
    "Xlin_big = np.sort(rng.uniform(low=-4,high=4,size=datasize))\n",
    "Y_big = 3*Xlin_big + rng.normal(loc=1,scale=8,size=datasize)\n",
    "\n",
    "# Divide into train and test\n",
    "Xlin_train, Xlin_test, Y_train, Y_test = train_test_split(\n",
    "    Xlin_big, Y_big, test_size=0.5, random_state=355)\n",
    "\n",
    "# Fit simple model (low-degree polynomial regression)\n",
    "low_degree=1\n",
    "ldp = np.poly1d(np.polyfit(Xlin_train,Y_train,deg=low_degree))\n",
    "\n",
    "# Get metrics\n",
    "train_rmse = mean_squared_error(Y_train, ldp(Xlin_train), squared=False)\n",
    "test_rmse = mean_squared_error(Y_test, ldp(Xlin_test), squared=False)\n",
    "\n",
    "# Print\n",
    "print('Training set RMSE: ',train_rmse)\n",
    "print('Test set RMSE: ',test_rmse)\n",
    "\n",
    "# Take a look\n",
    "Xplot = np.linspace(-4,4)\n",
    "fig, axs = plt.subplots(1,2,figsize=(12,5))\n",
    "axs[0].plot(Xplot,ldp(Xplot))\n",
    "axs[0].plot(Xlin_train,Y_train,'ro')\n",
    "axs[0].plot(Xlin_test,Y_test,'bo')\n",
    "axs[1].plot(Xplot,ldp(Xplot))\n",
    "axs[1].plot(Xlin_train,Y_train,'ro')\n",
    "axs[1].plot(Xlin_test,Y_test,'bo')\n",
    "axs[1].set_ylim(-25,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrrlYolhMXU-"
   },
   "source": [
    "Now, here's a model that for sure is not overfitting. The training and test RMSE are very close. And notice that, with the bigger data, our training RMSE has gone UP! This should not concern us in the slightest. Our test RMSE has gone down -- that is what matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQMOay5r-3up"
   },
   "source": [
    "#### Regularize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irwrFtqcAeDU"
   },
   "source": [
    "#### Hyper-parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hiVeDaK59yM"
   },
   "source": [
    "##TODO:  Evaluating _Unsupervised_ Models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
