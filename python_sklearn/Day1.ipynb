{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3365801-3238-40e2-98eb-ba48300db557",
   "metadata": {},
   "source": [
    "# Machine Learning in Python using Clemson High Performance Computing\n",
    "\n",
    "**Instructor:** Carl Ehrett\n",
    "\n",
    "**Email:** cehrett@clemson.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6c079-e0fd-4dde-97ea-8c0dc6044f93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Welcome and Overview\n",
    "\n",
    "#### Overview\n",
    "\n",
    "Machine learning is a powerful tool for making sense of large datasets. In this workshop, we will introduce the basics of machine learning using Python. We will emphasize the use of Clemson's Palmetto Cluster for running machine learning algorithms on large datasets. We will cover the following topics:\n",
    "* What is machine learning?\n",
    "* What are some of the python tools that facilitate machine learning?\n",
    "* What are the different types of machine learning?\n",
    "* What are some of the common machine learning algorithms?\n",
    "* How do we evaluate the performance of machine learning algorithms?\n",
    "* How do we explore and clean data?\n",
    "* How do we prepare data for machine learning?\n",
    "* How do we make use of Clemson's Palmetto Cluster to efficiently run our machine learning code?\n",
    "* How can we run code that is too complex, or use data that is too large, for a Jupyter notebook?\n",
    "* What sorts of Palmetto resources should we request to allocate for our machine learning jobs?\n",
    "\n",
    "##### Getting started\n",
    "You can download this notebook and its contents as follows.\n",
    "\n",
    "In the terminal, run the following command: `wget https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/python_sklearn/download.sh`\n",
    "\n",
    "This copies to your drivespace a script that, when run, will copy the full workshop files to your drivespace. So now that you have that script, run the command: `bash download.sh`. You should now have a folder, `python_sklearn`, which contains this notebook and the rest of the workshop.\n",
    "\n",
    "You can run most of this notebook using the default kernel, though some of the code cells will only run if you have created an environment with specialized libraries installed.\n",
    "\n",
    "##### What is machine learning?\n",
    "\n",
    "People use the term \"machine learning\" in a variety of ways. Some people use it more or less synonymously with \"artificial intelligence.\" And these days, AI does indeed usually work under the paradigm of machine learning. But \"machine learning\" refers to the use of algorithms to learn from data. The contrast here is with traditional programming, where a programmer writes code that tells the computer exactly what to do. In machine learning, the programmer writes code that tells the computer how to learn from data to make decisions.\n",
    "\n",
    "![ML vs traditional](img/ml.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa420fd",
   "metadata": {},
   "source": [
    "## 1.2 Examples showcase\n",
    "\n",
    "Python libraries like Scikit-Learn and PyTorch are widely used for machine learning tasks. Before diving in to the rest of the workshop, in this section we will showcase what is possible when such libraries are combined with the powerful computational resources available to you through the Palmetto 2 Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94e776-f46d-434f-b34d-f9f45913c35f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [Forest Covertypes Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype)\n",
    "\n",
    "First, we will use the Forest Covertypes dataset, which is a dataset that contains information about the forest cover type in the Roosevelt National Forest of northern Colorado. The dataset contains 581,012 samples and 54 features. The data contains binary (0 or 1) columns of data for qualitative independent variables (wilderness areas and soil types). The goal is to predict the forest cover type based on the cartographic features provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370f0cf-60d0-41db-aa93-1469cddadf9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "cov_type = fetch_covtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e124ba74",
   "metadata": {},
   "source": [
    "As we should always do with any dataset we're working with, we should poke around it a bit to see what it looks like. It's always good to know what is the datatype of the object we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea4c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(cov_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8260c1",
   "metadata": {},
   "source": [
    "In this case we've got a scikit-learn \"Bunch\" object. A quick google search shows us this page of documentation: [sklearn.utils.Bunch Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html), where we learn that a Bunch is a dictionary-like object that exposes keys as attributes. So, let's see what keys are in this Bunch object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be5ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_type.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb006761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cov_type.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54a95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_type.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81113078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_type.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0295381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_type.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea698d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_type.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636d7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_type.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e487ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(cov_type.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9763d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_type.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0171bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_type.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7431f081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame using the feature names and data from cov_type\n",
    "df_cov_type = pd.DataFrame(data=cov_type.data, columns=cov_type.feature_names)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the Forest Covertypes Dataset:\")\n",
    "df_cov_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd616f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_cov_type[[col for col in df_cov_type.columns if 'Soil_Type' in col]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3adf89",
   "metadata": {},
   "source": [
    "Let's use the K-nearest neighbors algorithm to try to predict covertype using the information contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4435383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X, y = cov_type.data, cov_type.target\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c7e1c",
   "metadata": {},
   "source": [
    "Now it's time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0ebf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize and train the classifier\n",
    "kn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "kn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4420c37",
   "metadata": {},
   "source": [
    "Now let's see how the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339862d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# Start a timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = kn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Get unique class labels\n",
    "unique_labels = np.unique(y)\n",
    "target_names = [f\"Class {label}\" for label in unique_labels]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# See how long that took\n",
    "print(f\"\\nTotal time taken: {time.time() - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1381d09",
   "metadata": {},
   "source": [
    "The above code works fine, but takes a long time to run! On the order of 10 minutes, depending on how fast your machine is. And that's not even bad, as far as what machine learning requires.\n",
    "\n",
    "Now let's look at a version of the same machine learning approach that makes better use of the resources available to us -- using both parallelization across cores and also using the GPU we've provisioned to speed things up.\n",
    "\n",
    "First, we'll load the data and prepare it for the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232f4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cuml import KNeighborsClassifier as cuKNN\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Initialize Dask client for distributed computing\n",
    "n_workers = 4  # Adjust based on your HPC resources\n",
    "cluster = LocalCluster(n_workers=n_workers)\n",
    "client = Client(cluster)\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names)\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622a14f",
   "metadata": {},
   "source": [
    "Now we fit the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017e618",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize and train the classifier\n",
    "kn_classifier_cuml = cuKNN(n_neighbors=5)\n",
    "kn_classifier_cuml.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf07f4e",
   "metadata": {},
   "source": [
    "Now let's see how this model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42652ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start a timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = kn_classifier_cuml.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test.to_cupy().get(), y_pred.to_cupy().get())\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Convert predictions to numpy for classification report\n",
    "y_test_np = y_test.to_numpy()\n",
    "y_pred_np = y_pred.to_numpy()\n",
    "\n",
    "# Get unique class labels\n",
    "unique_labels = np.unique(y_test_np)\n",
    "target_names = [f\"Class {label}\" for label in unique_labels]\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_np, y_pred_np, target_names=target_names))\n",
    "\n",
    "# Clean up\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "# See how long that took\n",
    "print(f\"\\nTotal time taken: {time.time() - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ade15c",
   "metadata": {},
   "source": [
    "Notice how much faster inference is in the second case! And this is just a simple case of getting predictions for 116k samples. Imagine if we needed to get predictions for millions of samples. In that case, the second approach would be much more feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7743f6c",
   "metadata": {},
   "source": [
    "### Hyperoptimization: search for the best version of the model to maximize performance\n",
    "\n",
    "In the above example, I used a KNN model with a k value of 5. But how do we know that k=5 is the best value? We don't. We need to search for the best value of k. This is an example of hyperoptimization. You can think of hyperoptimization as a search for the best version of the model to maximize performance. \n",
    "\n",
    "Hyperoptimization is inherently computationally expensive. It involves training many models with different hyperparameters and evaluating their performance. This is a perfect use case for the Palmetto 2 Cluster. We can use the cluster to train many models in parallel, which will speed up the hyperoptimization process. And if we submit our hyperoptimization job as a batch job, then we don't need to tie up our local machine for hours -- or days! -- while the hyperoptimization process runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923caab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.metrics import accuracy_score\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing and ensure float32 dtype\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names).astype('float32')\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)\n",
    "\n",
    "# Use a smaller subset for hyperparameter tuning\n",
    "X_tune, _, y_tune, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=355)\n",
    "\n",
    "# Define the parameter space for KNN\n",
    "param_dist = {\n",
    "    'n_neighbors': np.arange(1, 21, dtype=int),\n",
    "    'p': [1, 2],  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Function to perform k-fold cross-validation with batched prediction\n",
    "def cross_validate(X, y, model, n_splits=3, batch_size=10000):\n",
    "    fold_size = len(X) // n_splits\n",
    "    scores = []\n",
    "    for i in range(n_splits):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        X_val = X.iloc[start:end]\n",
    "        y_val = y.iloc[start:end]\n",
    "        X_train = cudf.concat([X.iloc[:start], X.iloc[end:]])\n",
    "        y_train = cudf.concat([y.iloc[:start], y.iloc[end:]])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Batched prediction\n",
    "        y_pred = cudf.Series()\n",
    "        for j in range(0, len(X_val), batch_size):\n",
    "            X_batch = X_val.iloc[j:j+batch_size]\n",
    "            y_pred = cudf.concat([y_pred, model.predict(X_batch)])\n",
    "        \n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perform manual randomized search\n",
    "n_iter = 10\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    params = {k: np.random.choice(v) for k, v in param_dist.items()}\n",
    "    knn = KNeighborsClassifier(metric='minkowski', **params)\n",
    "    score = cross_validate(X_tune, y_tune, knn)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "    \n",
    "    print(f\"Iteration {_+1}/{n_iter} - Score: {score:.4f} - Params: {params}\")\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set in batches\n",
    "batch_size = 10000\n",
    "y_pred = cudf.Series()\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test.iloc[i:i+batch_size]\n",
    "    y_pred = cudf.concat([y_pred, best_knn.predict(X_batch)])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBest model accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d916f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import pickle\n",
    "with open(\"best_knn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_knn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f153ba5",
   "metadata": {},
   "source": [
    "The above, being in a Jupyter notebook, is fine for something that only takes a few minutes. But if we want to e.g. try hundreds of different settings, or a model that takes longer to fit, we should make a script that we can submit as a SLURM job. We just need to copy our code into a .py file and make a SLURM script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be what we put in our .py file:\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "from cuml.neighbors import KNeighborsClassifier\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_covtype\n",
    "import pickle\n",
    "\n",
    "# Load the covertype dataset\n",
    "cov_type = fetch_covtype()\n",
    "\n",
    "# Convert data to cuDF DataFrames for GPU processing and ensure float32 dtype\n",
    "X = cudf.DataFrame(cov_type.data, columns=cov_type.feature_names).astype('float32')\n",
    "y = cudf.Series(cov_type.target)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=355)\n",
    "\n",
    "# Use a smaller subset for hyperparameter tuning\n",
    "X_tune, _, y_tune, _ = train_test_split(X_train, y_train, train_size=0.3, random_state=355)\n",
    "\n",
    "# Define the parameter space for KNN\n",
    "param_dist = {\n",
    "    'n_neighbors': np.arange(1, 21, dtype=int),\n",
    "    'p': [1, 2],  # 1 for Manhattan distance, 2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Function to perform k-fold cross-validation with batched prediction\n",
    "def cross_validate(X, y, model, n_splits=3, batch_size=10000):\n",
    "    fold_size = len(X) // n_splits\n",
    "    scores = []\n",
    "    for i in range(n_splits):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "        X_val = X.iloc[start:end]\n",
    "        y_val = y.iloc[start:end]\n",
    "        X_train = cudf.concat([X.iloc[:start], X.iloc[end:]])\n",
    "        y_train = cudf.concat([y.iloc[:start], y.iloc[end:]])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Batched prediction\n",
    "        y_pred = cudf.Series()\n",
    "        for j in range(0, len(X_val), batch_size):\n",
    "            X_batch = X_val.iloc[j:j+batch_size]\n",
    "            y_pred = cudf.concat([y_pred, model.predict(X_batch)])\n",
    "        \n",
    "        score = accuracy_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Perform manual randomized search\n",
    "n_iter = 10 \n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    params = {k: np.random.choice(v) for k, v in param_dist.items()}\n",
    "    knn = KNeighborsClassifier(metric='minkowski', **params)\n",
    "    score = cross_validate(X_tune, y_tune, knn)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "    \n",
    "    print(f\"Iteration {_+1}/{n_iter} - Score: {score:.4f} - Params: {params}\")\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "# Train the best model on the full training set\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set in batches\n",
    "batch_size = 10000\n",
    "y_pred = cudf.Series()\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test.iloc[i:i+batch_size]\n",
    "    y_pred = cudf.concat([y_pred, best_knn.predict(X_batch)])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBest model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "with open(\"best_knn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_knn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652d113",
   "metadata": {},
   "source": [
    "And our SLURM script (saved in a separate file, e.g. `hyperparam_opt.sh`) can be:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=hyperparam_opt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=4\n",
    "#SBATCH --time=16:00:00\n",
    "#SBATCH --gres=gpu:v100:1\n",
    "#SBATCH --output=hyperparam_opt_%j.out\n",
    "\n",
    "# Load the modules we need\n",
    "module load anaconda3\n",
    "module load cuda\n",
    "\n",
    "# Activate the environment we created to work in\n",
    "source activate MLWorkshop\n",
    "\n",
    "# Change to the directory where the .py script is\n",
    "cd /home/[username]/dir/where/the/py/script/is/\n",
    "\n",
    "# And run the script!\n",
    "python hyperparameter_optimization.py\n",
    "```\n",
    "\n",
    "Then we can submit the job with `sbatch hyperparam_opt.sh`, on the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3334a0",
   "metadata": {},
   "source": [
    "## 2. Setting up the Environment (15 minutes)\n",
    "\n",
    "### 2.1 Creating a Conda Environment\n",
    "Why we use conda for ML environments:\n",
    "* Simplified package management and dependency resolution\n",
    "* Easy creation and management of isolated environments\n",
    "* Cross-platform compatibility (Windows, macOS, Linux)\n",
    "* Support for multiple programming languages (not just Python)\n",
    "* Ability to specify and replicate exact environment configurations\n",
    "* Large repository of pre-built packages optimized for different systems\n",
    "```bash\n",
    "# Commands for creating and activating a conda environment\n",
    "conda create -n hpc_ml -c rapidsai -c conda-forge -c nvidia numpy pandas scikit-learn matplotlib seaborn rapidsai jupyterlab python=3.11 'cuda-version>=12.0,<=12.5'\n",
    "conda activate hpc_ml\n",
    "```\n",
    "\n",
    "### 2.2 Registering as a jupyter kernel\n",
    "In addition to installing JupyterLab, we need to register our environment as a Jupyter kernel in order for it to show up as an option for us when running a notebook.\n",
    "```bash\n",
    "# Register the env as a kernel\n",
    "python -m ipykernel install --user --name hpc_ml --display-name \"HPC_ML\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29bc24",
   "metadata": {},
   "source": [
    "## 3 Introduction to Python for Machine Learning\n",
    "\n",
    "Python has become the de facto language for machine learning due to its simplicity, readability, and extensive ecosystem of libraries. Its flexibility allows for rapid prototyping and development, while its powerful libraries like NumPy, Pandas, and Scikit-learn provide efficient tools for data manipulation and model building.\n",
    "\n",
    "### 3.1 Essential Python Concepts Review\n",
    " \n",
    "For a broad introduction to Python, check out the [Python Programming Guide](https://www.python.org/about/gettingstarted/). Here are some key features of Python that are particularly useful for machine learning:\n",
    "\n",
    "* **List Comprehensions**: A concise way to create lists in Python.\n",
    "* **Lambda Functions**: Anonymous functions that can be defined in a single line.\n",
    "* **Error Handling**: Using `try`, `except`, and `finally` blocks to handle exceptions.\n",
    "* **Generators**: Functions that return an iterator, allowing for lazy evaluation (meaning they don't store all values in memory at once).\n",
    "* **Dynamic Typing**: Variables in Python are dynamically typed, meaning you don't need to specify the type of a variable when you declare it.\n",
    "* **Runtime Compilation**: Python code is compiled to bytecode, which is then interpreted by the Python interpreter. This allows for dynamic execution of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032b4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code snippets demonstrating key Python concepts\n",
    "# List comprehension example\n",
    "squares = [x**2 for x in range(10)]\n",
    "squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4e1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lambda function example\n",
    "multiply = lambda x, y: x * y\n",
    "multiply(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5949a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Error handling example\n",
    "try:\n",
    "    result = 10 / 0\n",
    "except ZeroDivisionError:\n",
    "    print(\"Cannot divide by zero\")\n",
    "    \n",
    "print(\"This line still runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02de5ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generators example\n",
    "def fibonacci(n):\n",
    "    a, b = 0, 1\n",
    "    for _ in range(n):\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "\n",
    "fib = fibonacci(int(1e18)) # Large number to demonstrate generator memory efficiency, the first 1e18 (i.e. 1 quintillion) Fibonacci numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0be76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the next Fibonacci number\n",
    "next(fib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455b10c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How much memory does the fib object consume?\n",
    "import sys\n",
    "sys.getsizeof(fib)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55f038",
   "metadata": {},
   "source": [
    "If we were to run `list(fib)`, we would get a list of the first quintillion Fibonacci numbers. But we don't want to do that, because it would take up all the memory on our machine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59126a",
   "metadata": {},
   "source": [
    "### 3.2 Introduction to NumPy\n",
    "\n",
    "NumPy is a powerful library for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. NumPy is the foundation for many other libraries in the Python data science ecosystem, such as Pandas, Scikit-learn, PyTorch, and TensorFlow. The reason NumPy is so fast is that it is implemented in C, which is a much faster language than Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1416e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic NumPy operations and array manipulations\n",
    "import numpy as np\n",
    "\n",
    "# Create an array\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Array operations\n",
    "print(\"Array operations:\")\n",
    "print(\"Array:\")\n",
    "print(arr)\n",
    "print(\"Array x 2:\")\n",
    "print(arr * 2)\n",
    "print(\"Array summed:\")\n",
    "print(np.sum(arr))\n",
    "\n",
    "# Broadcasting example\n",
    "matrix = np.array([[1, 2, 3, 4 , 5], [6, 7, 8, 9, 10]])\n",
    "print(\"\\nBroadcasting example:\")\n",
    "print(\"Matrix:\")\n",
    "print(matrix)\n",
    "print(\"Array:\")\n",
    "print(arr)\n",
    "print(\"Matrix + Array:\")\n",
    "print(matrix + arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfa5ca",
   "metadata": {},
   "source": [
    "### 3.3 Intro to Pandas\n",
    "\n",
    "Pandas is a powerful data manipulation library for Python. It is built on top of NumPy and provides data structures and functions for efficiently manipulating large datasets. Pandas is widely used in data science and machine learning for data cleaning, exploration, and preparation.\n",
    "\n",
    "Pandas isn't the best choice for truly massive datasets, since it loads the entire dataset into memory. But even libraries that are better suited for massive datasets, like Dask, tend to conform to the Pandas API, so learning Pandas is a good foundation for working with other libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8ac269",
   "metadata": {},
   "source": [
    "Pandas is great for organizing and exploring data. We'll spend more time on data exploration in Day 2, but let's take a quick look at how to use Pandas dataframes to organize data. We'll use the California Housing dataset, which is a dataset containing information about housing prices. The dataset contains 20,640 samples and 9 features. The goal is to predict the house value, given a set of features about the property and its district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb0af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California housing dataset\n",
    "california = fetch_california_housing()\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "# The above created a dataframe with the features, but it doesn't have the target variable. It's easy to add new columns to a Pandas DataFrame:\n",
    "df['Price'] = california.target\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the California Housing Dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17a2589",
   "metadata": {},
   "source": [
    "Pandas includes some methods for quickly summarizing the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b911327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"DataFrame Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcdefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Basic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a9735",
   "metadata": {},
   "source": [
    "Pandas also makes it easy to filter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296457ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filtering data\n",
    "print(\"Houses with more than 4 rooms on average:\")\n",
    "print(df[df['AveRooms'] > 4].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5417698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nHouses with more than 4 rooms on average and a price above the median:\")\n",
    "print(df[(df['AveRooms'] > 4) & (df['Price'] > df['Price'].median())].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47219a51",
   "metadata": {},
   "source": [
    "We can also sort the data easily, and add new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be56c517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sorting data\n",
    "print(\"\\nTop 5 most expensive areas:\")\n",
    "print(df.sort_values('Price', ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fbe80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding a new column\n",
    "df['PriceCategory'] = pd.cut(df['Price'], bins=[0, 1.25, 2.5, 3.75, np.inf], labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "print(\"\\nDataFrame with new PriceCategory column:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f44bc9c",
   "metadata": {},
   "source": [
    "A particularly useful feature of Pandas is the ability to group data by a particular column and then apply a function to each group. This is similar to the SQL `GROUP BY` clause, or to Excel's pivot tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82f616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by operations\n",
    "print(\"\\nAverage house age by price category:\")\n",
    "print(df.groupby('PriceCategory', observed=False)['HouseAge'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c669b",
   "metadata": {},
   "source": [
    "Pandas also includes some plotting functionality, which is built on top of the Matplotlib library. Very useful for quickly visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec8d2fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic data visualization\n",
    "plt.figure(figsize=(4, 3))\n",
    "df.plot(x='MedInc', y='Price', kind='scatter', alpha=0.5)\n",
    "plt.title('Median Income vs House Price')\n",
    "plt.xlabel('Median Income')\n",
    "plt.ylabel('House Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8255c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap (excluding categorical column)\n",
    "plt.figure(figsize=(4, 3))\n",
    "correlation_matrix = df.drop('PriceCategory', axis=1).corr()\n",
    "plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)\n",
    "plt.title('Correlation Heatmap of California Housing Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f477dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handling categorical data\n",
    "print(\"\\nCount of houses in each price category:\")\n",
    "print(df['PriceCategory'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0559760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizing categorical data\n",
    "plt.figure(figsize=(6, 4))\n",
    "df['PriceCategory'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of House Prices by Category')\n",
    "plt.xlabel('Price Category')\n",
    "plt.ylabel('Number of Houses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1bab89",
   "metadata": {},
   "source": [
    "## 4. Introduction to ML Concepts\n",
    "\n",
    "Time to dive into some of the core concepts of machine learning! We'll start with a high-level overview of different types of machine learning, then move on to some common machine learning algorithms.\n",
    "\n",
    "### 4.1 Types of Machine Learning\n",
    "- Supervised Learning: Learning from labeled data\n",
    "  - Examples: classification, regression\n",
    "- Unsupervised Learning: Finding patterns in unlabeled data\n",
    "  - Examples: clustering, dimensionality reduction\n",
    "- Reinforcement Learning: Learning through interaction with an environment\n",
    "  - Examples: game playing, robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430624c",
   "metadata": {},
   "source": [
    "#### 4.2 Common ML Algorithms\n",
    "\n",
    "##### 4.2.1 Supervised Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419dda48",
   "metadata": {},
   "source": [
    "**Supervised learning:** learning a function which approximates the relationship between an input and output from a set of labeled training examples.\n",
    "\n",
    "E.g.:\n",
    "\n",
    "![Home price data](img/hp.png)\n",
    "\n",
    "Our *target* (or dependent variable, or output) is the variable we would like to predict/estimate.\n",
    "\n",
    "Our *features* (or regressors, or independent variables, or inputs, or covariates) are the variables we can use to make our prediction/estimate.\n",
    "\n",
    "In this case, $\\mathrm{Home \\$}\\approx f(\\mathrm{Sq. ft., \\#bed, \\#bath,}\\ldots)$\n",
    "\n",
    "##### Regression vs. Classification\n",
    "In the case of **regression**, we estimate a *quantity*.\n",
    "\n",
    "![regression](img/reg.png)\n",
    "\n",
    "In the case of **classification**, we predict a *label* (i.e. a category).\n",
    "\n",
    "![classification](img/class.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80e9fa",
   "metadata": {},
   "source": [
    "##### Linear Regression\n",
    "\n",
    "Linear regression assumes that $f$ is just a weighted sum of the variables in the covariate matrix $X$:\n",
    "$$f(X)=\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_px_p + \\epsilon$$\n",
    "Which we can express as just $f(X)=X\\mathbf{\\beta}$ (and so $Y=X\\mathbf{\\beta}+\\epsilon$).\n",
    "Turns out the best estimate of $\\beta$ is just $(X^TX)^{-1}X^TY$. This is called the Ordinary Least Squares (OLS) estimate. However, that expression sometimes cannot be calculated, and is not computationally efficient to use with large data.\n",
    "\n",
    "In order to apply OLS regression, our problem should obey certain assumptions.\n",
    "1. The linear model is correct.\n",
    "2. The error term Îµ has mean 0.\n",
    "3. The regressors (the $x$ terms) are linearly independent.\n",
    "4. The errors are homoscedastic and uncorrelated.\n",
    "5. The errors are normally distributed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d48e2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple linear regression example\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate some random data - in a real problem, we would not know the true relationship described here.\n",
    "X = np.random.uniform(-4, 4, (100, 1))\n",
    "y = X**2 + np.random.normal(0, 3, X.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"R-squared score: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c97d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting the data and the regression line\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X, y, alpha=0.5)\n",
    "plt.plot(X, model.predict(X), color='red')\n",
    "plt.title('Simple Linear Regression Example')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcc8730",
   "metadata": {},
   "source": [
    "Notice a couple of things. One, our fit is abysmal.\n",
    "Two, there *does* nonetheless seem (visually) to be an interesting relationship between the variables. Maybe $y$ is related not directly to $x$, but to some function of $x$.\n",
    "\n",
    "In this case, we can get ideas from visualizing the data, but in most cases, a deep understanding of the data will be necessary to make a good model. E.g., suppose in this case we know that $X$ is wind speed, and $y$ is power generated by a wind turbine. An engineer might tell us that in practice $y$ is typically related to the *square* of $X$, rather than $X$ itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b9a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit a linear model where y is regressed on X^2\n",
    "X_squared_train = X_train**2\n",
    "X_squared_test = X_test**2\n",
    "\n",
    "model_squared = LinearRegression()\n",
    "model_squared.fit(X_squared_train, y_train)\n",
    "\n",
    "print(f\"R-squared score (X^2 model): {model_squared.score(X_squared_test, y_test)}\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "# Plotting the data and the regression curve\n",
    "axes[0].scatter(X, y, alpha=0.5)\n",
    "X_curve = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_pred = model_squared.predict(X_curve**2)\n",
    "axes[0].plot(X_curve, y_pred, color='red')\n",
    "axes[0].set_title('Linear Regression with X^2')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "# Plotting y_train vs X_squared_train\n",
    "axes[1].scatter(X_squared_train, y_train, alpha=0.5)\n",
    "axes[1].plot(X_squared_train, model_squared.predict(X_squared_train), color='red')\n",
    "axes[1].set_title('y_train vs X_squared_train')\n",
    "axes[1].set_xlabel('X_squared_train')\n",
    "axes[1].set_ylabel('y_train')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998ca64",
   "metadata": {},
   "source": [
    "\"Linear\" regression is surprisingly flexible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676c7f6",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n",
    "\n",
    "Despite its name, logistic regression is a powerful algorithm for *classification*. In a binary classification problem, our target can be thought of as being either 1 or 0. It is possible (but not advisable!) to use a regression algorithm, like linear regression, in such a case.\n",
    "\n",
    "Suppose that I have data where the target is a binary indicator for whether a student passed a certain class. The data is the student's score on a that class's first exam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc79ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate continuous covariate\n",
    "n_samples = 100\n",
    "covariate = np.random.uniform(0, 10, n_samples)\n",
    "\n",
    "# Generate binary data correlated with covariate\n",
    "probabilities = 1 / (1 + np.exp(-(covariate - 5)))\n",
    "binary_data = np.random.binomial(1, probabilities)\n",
    "\n",
    "# Fit linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(covariate.reshape(-1, 1), binary_data)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(covariate, binary_data, color='blue', alpha=0.6)\n",
    "\n",
    "# Plot linear regression line\n",
    "x_plot = np.linspace(0, 10, 100)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))\n",
    "plt.plot(x_plot, y_plot, color='red', lw=2)\n",
    "\n",
    "plt.xlabel('Covariate')\n",
    "plt.ylabel('Binary Outcome')\n",
    "plt.title('Binary Data vs Continuous Covariate\\nwith Linear Regression Fit')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear Regression Coefficient: {model.coef_[0]:.4f}\")\n",
    "print(f\"Linear Regression Intercept: {model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bb413",
   "metadata": {},
   "source": [
    "The idea of logistic regression is that instead of directly modeling the target, we instead model the *probability* that the target is 1 or 0. This is a specific type of *generalized linear model*, in which the target is transformed by a *link function*. In this case, the link function is the *logit* function, which is the inverse of the *logistic* function. The logistic function is defined as: $$\\mathrm{logit}(p)=\\log\\left(\\frac{p}{1-p}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30335a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit logistic regression\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(covariate.reshape(-1, 1), binary_data)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(covariate, binary_data, color='blue', alpha=0.6, label='Data')\n",
    "\n",
    "# Plot linear regression line\n",
    "x_plot = np.linspace(0, 10, 100)\n",
    "y_linear = model.predict(x_plot.reshape(-1, 1))\n",
    "plt.plot(x_plot, y_linear, color='red', lw=2, label='Lin. Reg.')\n",
    "\n",
    "# Plot logistic regression curve\n",
    "y_logistic = logistic_model.predict_proba(x_plot.reshape(-1, 1))[:, 1]\n",
    "plt.plot(x_plot, y_logistic, color='green', lw=2, label='Log. Reg.')\n",
    "\n",
    "plt.xlabel('Covariate')\n",
    "plt.ylabel('Binary Outcome / Probability')\n",
    "plt.title('Binary Data vs Continuous Covariate:\\nLinear and Logistic Regression')\n",
    "plt.legend(loc='center left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.1, 1.1)  # Set y-axis limits for better visualization\n",
    "plt.show()\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"Coefficient: {model.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(f\"Coefficient: {logistic_model.coef_[0][0]:.4f}\")\n",
    "print(f\"Intercept: {logistic_model.intercept_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f6375",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "A totally different approach to modeling data is to use a decision tree. A decision tree is a tree-like model of the dataset. It is a simple model that is easy to interpret and understand. It is also a non-parametric model, which means that it makes no assumptions about the shape of the data - sometimes a big advantage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077bc497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's make some data.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'income': np.random.randint(20000, 200000, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples),\n",
    "    'debt_to_income': np.random.uniform(0, 0.6, n_samples),\n",
    "    'employment_length': np.random.randint(0, 30, n_samples),\n",
    "    'loan_amount': np.random.randint(5000, 100000, n_samples)\n",
    "})\n",
    "\n",
    "# Create a rule-based target variable\n",
    "data['loan_approved'] = (\n",
    "    (data['credit_score'] > 700) & \n",
    "    (data['debt_to_income'] < 0.4) & \n",
    "    (data['income'] > 50000)\n",
    ").astype(int)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data[['income', 'credit_score', 'debt_to_income', 'employment_length', 'loan_amount']]\n",
    "y = data['loan_approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a27708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and train the Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(6, 6))\n",
    "plot_tree(dt, filled=True, feature_names=X.columns.to_list(), class_names=['Denied', 'Approved'], rounded=True, impurity=False, proportion=True, precision=2)\n",
    "plt.title(\"Loan Approval Decision Tree\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Decision Tree created and visualized based on the loan approval data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037d588a",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "The decision tree is highly interpretable, which is sometimes a big advantage. But, it is a weak machine learning algorithm; not nearly as powerful as some others. One way to make it more powerful is to use a *random forest*. A random forest is an ensemble of decision trees, which means that it is a collection of decision trees that are trained separately and then combined to make a prediction. Essentially, a random forest is a collection of decision trees that are trained separately and then combined (by averaging or voting) to make a prediction. They can be used for both classification and regression tasks.\n",
    "\n",
    "You lose the interpretability of a single decision tree, but you gain a lot of predictive power. And random forests are very easy to use and very flexible, don't require much tuning, are very hard to overfit, and don't make many assumptions about the data. A good general-purpose algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf6c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Create and train the Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=42)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10302fc1",
   "metadata": {},
   "source": [
    "Random Forests makes it easy to see which features are most important in making a prediction. This is because the algorithm can keep track of how much each feature contributes to the trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e210f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize feature importances\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.bar(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.title(\"Feature Importance in Random Forest\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d8ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot two trees side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# First tree\n",
    "plot_tree(rf.estimators_[0], \n",
    "          filled=True, \n",
    "          feature_names=X.columns.to_list(), \n",
    "          class_names=['Denied', 'Approved'], \n",
    "          rounded=True,\n",
    "          impurity=False,\n",
    "          proportion=False,\n",
    "          node_ids=False,\n",
    "          precision=0,\n",
    "          ax=axes[0])\n",
    "axes[0].set_title(\"Tree 1 from Random Forest\")\n",
    "\n",
    "# Second tree\n",
    "plot_tree(rf.estimators_[1], \n",
    "          filled=True, \n",
    "          feature_names=X.columns.to_list(), \n",
    "          class_names=['Denied', 'Approved'], \n",
    "          rounded=True,\n",
    "          impurity=False,\n",
    "          proportion=False,\n",
    "          node_ids=False,\n",
    "          precision=0,\n",
    "          ax=axes[1])\n",
    "axes[1].set_title(\"Tree 2 from Random Forest\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c00814",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for classification or regression tasks. They are based on the idea of finding the hyperplane that best divides a dataset into two classes. The hyperplane is the line that best separates the two classes. The SVM algorithm finds the hyperplane that maximizes the margin between the two classes.\n",
    "\n",
    "![hyperplanes](img/svm_hp.png)\n",
    "\n",
    "In cases where the data cannot be linearly separated, SVMs can use a *kernel trick* to transform the data into a higher-dimensional space where it can be separated. This is a very powerful technique that allows SVMs to work well on a wide variety of datasets.\n",
    "\n",
    "![kernel](img/svm_hd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7910866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's prepare some data.\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]  # We'll use petal length and width\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b95284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's train a Support Vector Machine (SVM) classifier.\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_classifier = svm.SVC(kernel='rbf', random_state=42)\n",
    "svm_classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4a6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's make predictions and evaluate the model.\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Print the accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Visualize the decision boundaries\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = plt.cm.RdYlBu\n",
    "\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, color=cmap(idx / len(np.unique(y))),\n",
    "                    marker=markers[idx], label=iris.target_names[cl])\n",
    "\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='none', alpha=1.0, linewidth=1, marker='o', \n",
    "                    s=55, edgecolors='black', label='test set')\n",
    "\n",
    "# Visualize the results\n",
    "X_combined = np.vstack((X_train_scaled, X_test_scaled))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X_combined, y_combined, classifier=svm_classifier, test_idx=range(len(y_train), len(y_combined)))\n",
    "\n",
    "plt.xlabel('Petal length (standardized)')\n",
    "plt.ylabel('Petal width (standardized)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('SVM Decision Regions - Iris Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cf2cf",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbors\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, easy-to-understand algorithm that can be used for both classification and regression tasks. It is a non-parametric algorithm, which means that it makes no assumptions about the shape of the data. The basic idea behind KNN is that similar data points are close to each other in the feature space. To make a prediction, KNN looks at the K-nearest neighbors of a data point and takes a majority vote (for classification) or an average (for regression) to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6624dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# Train KNN classifier on the iris data\n",
    "k = 5  # number of neighbors\n",
    "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "knn_classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9194e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Print the accuracy and classification report\n",
    "print(f\"KNN (k={k}) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Visualize the decision boundaries\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = plt.cm.RdYlBu\n",
    "\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, color=cmap(idx / len(np.unique(y))),\n",
    "                    marker=markers[idx], label=iris.target_names[cl])\n",
    "\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='none', alpha=1.0, linewidth=1, marker='o', \n",
    "                    s=55, edgecolors='black', label='test set')\n",
    "\n",
    "# Visualize the results\n",
    "X_combined = np.vstack((X_train_scaled, X_test_scaled))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X_combined, y_combined, classifier=knn_classifier, test_idx=range(len(y_train), len(y_combined)))\n",
    "\n",
    "plt.xlabel('Petal length (standardized)')\n",
    "plt.ylabel('Petal width (standardized)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(f'KNN (k={k}) Decision Regions - Iris Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f4cec",
   "metadata": {},
   "source": [
    "#### 4.2.2 Unsupervised Learning Algorithms\n",
    "\n",
    "**Unsupervised learning:** finding patterns in data without any labels.\n",
    "\n",
    "![unsupervised](img/kmc.png)\n",
    "\n",
    "Common types of unsupervised learning algorithms include:\n",
    "* Clustering: grouping similar data points together\n",
    "* Dimensionality reduction: reducing the number of features in a dataset\n",
    "* Anomaly detection: finding outliers in a dataset\n",
    "\n",
    "##### Clustering\n",
    "\n",
    "Clustering is a common way to discover patterns and subgroups that are interesting in our data. For example, we might want to group customers into segments based on their purchasing behavior, or group documents based on their content. There are many different clustering algorithms. See just a few of those implemented in Scikit-Learn:\n",
    "\n",
    "<img src=\"img/skl_cl.png\" alt=\"clustering\" width=\"85%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d807b685",
   "metadata": {},
   "source": [
    "##### K-Means Clustering\n",
    "\n",
    "K-Means is a simple and popular clustering algorithm that is used to partition a dataset into K clusters. The algorithm works by iteratively assigning data points to clusters based on the distance between the data point and the cluster center. The cluster center is then updated to be the mean of all the data points assigned to that cluster. This process is repeated until the algorithm converges.\n",
    "\n",
    "We can look at this in action using the Iris dataset, which is a dataset containing information about iris flowers. The dataset contains 150 samples and 4 features. The goal is to cluster the flowers into different groups based on their features. Though we actually know the flower species, for this example we imagine that we don't -- we just have the features, and we want to see if we can group the flowers into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bea5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]  # We'll use petal length and width\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform K-means clustering\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "y_kmeans = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluate the clustering\n",
    "print(\"Silhouette Score:\", silhouette_score(X_scaled, y_kmeans))\n",
    "print(\"Calinski-Harabasz Index:\", calinski_harabasz_score(X_scaled, y_kmeans))\n",
    "\n",
    "# Compare with true labels\n",
    "print(\"\\nComparison with true labels:\")\n",
    "print(\"Adjusted Rand Index:\", adjusted_rand_score(iris.target, y_kmeans))\n",
    "print(\"Normalized Mutual Information:\", normalized_mutual_info_score(iris.target, y_kmeans))\n",
    "\n",
    "# Define the centroids of the clusters\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Create a mapping function. This is only here to make the plots consistent in color.\n",
    "def map_labels(y_kmeans, y_true):\n",
    "    mapping = {}\n",
    "    for k in range(n_clusters):\n",
    "        k_indices = y_kmeans == k\n",
    "        k_true = y_true[k_indices]\n",
    "        mapping[k] = np.bincount(k_true).argmax()\n",
    "    return np.array([mapping[k] for k in y_kmeans])\n",
    "\n",
    "# Apply the mapping\n",
    "y_kmeans_mapped = map_labels(y_kmeans, iris.target)\n",
    "\n",
    "# Plot comparison with true labels\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans_mapped, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "plt.title('K-means Clustering')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.subplot(122)\n",
    "scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=iris.target, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "plt.title('True Labels')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077800a",
   "metadata": {},
   "source": [
    "Now let's see K-Means in action on a more complex dataset. We'll use the Labeled Faces in the Wild dataset, which is a dataset containing images of faces. The dataset contains 13,233 sample images. We'll use K-Means to cluster the faces into different groups based on their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c1c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn import cluster, decomposition\n",
    "\n",
    "lfw_people = fetch_lfw_people()\n",
    "lfw_people.data.shape\n",
    "n_samples, n_features = lfw_people.data.shape\n",
    "\n",
    "# Global centering (focus on one feature, centering all samples)\n",
    "faces_centered = lfw_people.data - lfw_people.data.mean(axis=0)\n",
    "\n",
    "# Local centering (focus on one sample, centering all features)\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536b487",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (62, 47)\n",
    "\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=n_row,\n",
    "        ncols=n_col,\n",
    "        figsize=(2.0 * n_col, 2.3 * n_row),\n",
    "        facecolor=\"white\",\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n",
    "    fig.set_edgecolor(\"black\")\n",
    "    fig.suptitle(title, size=16)\n",
    "    for ax, vec in zip(axs.flat, images):\n",
    "        vmax = max(vec.max(), -vec.min())\n",
    "        im = ax.imshow(\n",
    "            vec.reshape(image_shape),\n",
    "            cmap=cmap,\n",
    "            interpolation=\"nearest\",\n",
    "            vmin=-vmax,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2556e04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_gallery(\"Faces from dataset\", faces_centered[:n_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a32779",
   "metadata": {},
   "source": [
    "Now let's apply K-Means to the faces dataset. We can then examine the cluster centroids. This centroid will be like the \"average\" face in the cluster, for each of our clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6f543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans_estimator = cluster.MiniBatchKMeans(\n",
    "    n_clusters=24,\n",
    "    tol=1e-3,\n",
    "    batch_size=1000,\n",
    "    max_iter=1000,\n",
    "    random_state=355,\n",
    ")\n",
    "kmeans_estimator.fit(faces_centered)\n",
    "plot_gallery(\n",
    "    \"Cluster centers - MiniBatchKMeans\",\n",
    "    kmeans_estimator.cluster_centers_[:n_components],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b0c57",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of features in a dataset. This can be useful for a number of reasons, including:\n",
    "* Reducing the computational cost of working with high-dimensional data\n",
    "* Reducing the noise in the data\n",
    "* Visualizing high-dimensional data in a lower-dimensional space\n",
    "\n",
    "Principal Component Analysis (PCA) is a popular dimensionality reduction technique often used for the first two reasons above. It works by finding the directions (or principal components) in which the data varies the most. These directions are then used to transform the data into a lower-dimensional space. PCA is often used before applying other machine learning algorithms to the data, including supervised learning algorithms.\n",
    "\n",
    "Why is PCA used this way? Imagine you could construct your ideal ML data set for some phenomenon youâre studying. Three things youâd want:\n",
    "* High variance features that are\n",
    "* Uncorrelated, and are also\n",
    "* Few in number.\n",
    "\n",
    "PCA makes your data more aligned with these three goals. It replaces your original features with new ones that are uncorrelated and ordered by how much variance they explain. So you can keep only the few features that explain most of the variance, and throw away the rest. PCA is implicitly lossy compression.\n",
    "\n",
    "![pca](img/pca.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda08cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming faces_centered is already defined as per your previous code\n",
    "\n",
    "# Perform PCA on the entire dataset\n",
    "pca = PCA().fit(faces_centered)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Define the variance levels we want to visualize\n",
    "variance_levels = [0.5, 0.75, 0.9, 0.95, 0.99, 1.0]\n",
    "\n",
    "# Find the number of components needed for each variance level\n",
    "n_components_list = [np.argmax(cumulative_variance_ratio >= level) + 1 for level in variance_levels]\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Function to reconstruct and plot a face\n",
    "def plot_face(ax, face, n_components, variance):\n",
    "    if n_components == 1:\n",
    "        n_components = face.shape[0]\n",
    "    pca_partial = PCA(n_components=n_components)\n",
    "    face_pca = pca_partial.fit_transform(faces_centered)\n",
    "    face_approximation = pca_partial.inverse_transform(pca_partial.transform(face.reshape(1, -1)))\n",
    "    ax.imshow(face_approximation.reshape(lfw_people.images[0].shape), cmap='gray')\n",
    "    ax.set_title(f'{variance:.0%} variance\\n({n_components} components)')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Choose a random face\n",
    "random_face_index = np.random.randint(faces_centered.shape[0])\n",
    "face = faces_centered[random_face_index]\n",
    "\n",
    "# Plot the face at different variance levels\n",
    "for ax, n_components, variance in zip(axes, n_components_list, variance_levels):\n",
    "    plot_face(ax, face, n_components, variance)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print out the number of components for each variance level\n",
    "for variance, n_components in zip(variance_levels, n_components_list):\n",
    "    print(f\"{variance:.0%} variance explained by {n_components} components\")\n",
    "\n",
    "# Print the total number of samples and features\n",
    "print(f\"\\nDataset consists of {faces_centered.shape[0]} faces\")\n",
    "print(f\"Each face has {faces_centered.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276e1c7",
   "metadata": {},
   "source": [
    "#### T-SNE\n",
    "\n",
    "Dimensionality reduction is also used to visualize high-dimensional data in a lower-dimensional space. One popular technique for this is t-distributed Stochastic Neighbor Embedding (t-SNE). It works by modeling the similarity between data points in the high-dimensional space and the low-dimensional space. t-SNE is often used to visualize high-dimensional data in two or three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54700204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Convert target to integer\n",
    "y = y.astype(int)\n",
    "\n",
    "# Take a subset of the data to speed up computation\n",
    "n_samples = 5000\n",
    "random_idx = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "X_subset = X[random_idx]\n",
    "y_subset = y[random_idx]\n",
    "\n",
    "# Normalize the data\n",
    "X_normalized = X_subset / 255.0\n",
    "\n",
    "# Apply t-SNE\n",
    "print(\"Applying t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=1000, learning_rate='auto', init='pca')\n",
    "X_tsne = tsne.fit_transform(X_normalized)\n",
    "\n",
    "# Create a color map\n",
    "num_classes = len(np.unique(y))\n",
    "colors = plt.cm.jet(np.linspace(0, 1, num_classes))\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(6, 5))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_subset, cmap=cmap, alpha=0.7, s=5)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE visualization of MNIST dataset\")\n",
    "plt.xlabel(\"t-SNE feature 1\")\n",
    "plt.ylabel(\"t-SNE feature 2\")\n",
    "\n",
    "# Add a legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=f'Digit {i}', \n",
    "                   markerfacecolor=colors[i], markersize=10) for i in range(num_classes)]\n",
    "plt.legend(handles=legend_elements, loc='best', title=\"Digits\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to plot some example digits\n",
    "def plot_example_digits(X, y, num_examples=10):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(num_examples):\n",
    "        ax = plt.subplot(1, num_examples, i + 1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray_r')\n",
    "        plt.title(f\"Digit: {y[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot example digits\n",
    "plot_example_digits(X_subset, y_subset)\n",
    "\n",
    "print(\"t-SNE visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4a444",
   "metadata": {},
   "source": [
    "### 4.3 Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5b51c",
   "metadata": {},
   "source": [
    "#### 4.3.1 Supervised Learning Metrics\n",
    "\n",
    "How do we evaluate the performance of a machine learning model? There are many different metrics that can be used, depending on the type of problem and the goals of the model. The choice of metric will almost always depend heavily on the type of problem you are trying to solve.\n",
    "\n",
    "But how do we apply these metrics? We can split our data into a training set and a test set. We train our model on the training set and then evaluate its performance on the test set. If we're tuning hyperparameters, we might use a validation set as well.\n",
    "\n",
    "![train_test](img/tr_ts.png)\n",
    "\n",
    "Even better than a single train/test split is to use *cross-validation*. In cross-validation, the data is split into K folds, and the model is trained and evaluated K times, each time using a different fold as the test set. This gives us a more robust estimate of the model's performance.\n",
    "\n",
    "![cross_val](img/cvl.png)\n",
    "\n",
    "##### Regression Metrics\n",
    "\n",
    "In a regression problem, we are trying to predict a continuous value. Some common metrics for evaluating regression models include:\n",
    "* Mean Squared Error (MSE) - the average of the squared differences between the predicted and actual values\n",
    "* Root Mean Squared Error (RMSE) - the square root of the MSE. More interpretable than MSE due to being in the same units as the target variable\n",
    "* Mean Absolute Error (MAE) - the average of the absolute differences between the predicted and actual values\n",
    "* R-squared (R2) - a measure of how well the model fits the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display metrics\n",
    "print(\"Regression Metrics for California Housing Dataset:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# Display feature names and their coefficients\n",
    "feature_names = housing.feature_names\n",
    "coefficients = model.coef_\n",
    "\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "for name, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638bed4",
   "metadata": {},
   "source": [
    "##### Classification Metrics\n",
    "\n",
    "In a classification problem, we are trying to predict a label. Some common metrics for evaluating classification models include:\n",
    "* Accuracy - the proportion of correctly classified instances\n",
    "* Precision - the proportion of true positive predictions among all positive predictions\n",
    "* Recall - the proportion of true positive predictions among all actual positive instances\n",
    "* F1 Score - the harmonic mean of precision and recall\n",
    "* ROC AUC - the area under the Receiver Operating Characteristic curve\n",
    "\n",
    "<img src=\"img/prec_rec.png\" alt=\"precision_recall\" style=\"width:40%\">\n",
    "\n",
    "Note that all of the above *except* for ROC AUC depend on your choice of threshold for classification. ROC AUC is a metric that is threshold-independent, and is often used when you want to compare models across different thresholds.\n",
    "\n",
    "Let's look at this in action, after fitting a logistic regression model on the UCI Ionosphere dataset. In this dataset, we are trying to predict whether a radar return is \"good\" or \"bad\" based on features such as the signal's amplitude and frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Ionosphere dataset\n",
    "ionosphere = fetch_openml(name=\"ionosphere\", version=1, as_frame=True)\n",
    "X, y = ionosphere.data, ionosphere.target\n",
    "y = (y == 'g').astype(int)  # Convert to binary (good=1, bad=0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit a logistic regression model with reduced performance\n",
    "model = LogisticRegression(random_state=42, C=0.01, max_iter=1000) \n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Display metrics\n",
    "print(\"Classification Metrics for Ionosphere Dataset:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Display feature importances\n",
    "feature_importance = abs(model.coef_[0])\n",
    "feature_names = X.columns\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "for idx in sorted_idx[-5:]:\n",
    "    print(f\"{feature_names[idx]}: {feature_importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb4cb2",
   "metadata": {},
   "source": [
    "##### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model. It shows the number of true positives, false positives, true negatives, and false negatives. From the confusion matrix, we can calculate many different metrics, including accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix as a plot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(2, 2))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "            xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLWorkshop",
   "language": "python",
   "name": "rapids_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
