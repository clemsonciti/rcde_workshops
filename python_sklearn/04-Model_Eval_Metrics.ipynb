{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f46a2f7-458b-41df-827e-957195a16285",
   "metadata": {},
   "source": [
    "# Supervised Learning Model Evaluation Metrics\n",
    "\n",
    "How do we evaluate the performance of a machine learning model? There are many different metrics that can be used, depending on the type of problem and the goals of the model. The choice of metric will almost always depend heavily on the type of problem you are trying to solve.\n",
    "\n",
    "But how do we apply these metrics? We can split our data into a training set and a test set. We train our model on the training set and then evaluate its performance on the test set. If we're tuning hyperparameters, we might use a validation set as well.\n",
    "\n",
    "![train_test](img/tr_ts.png)\n",
    "\n",
    "Even better than a single train/test split is to use *cross-validation*. In cross-validation, the data is split into K folds, and the model is trained and evaluated K times, each time using a different fold as the test set. This gives us a more robust estimate of the model's performance.\n",
    "\n",
    "![cross_val](img/cvl.png)\n",
    "\n",
    "### 1. Regression Metrics\n",
    "\n",
    "In a regression problem, we are trying to predict a continuous value. Some common metrics for evaluating regression models include:\n",
    "* Mean Squared Error (MSE) - the average of the squared differences between the predicted and actual values\n",
    "* Root Mean Squared Error (RMSE) - the square root of the MSE. More interpretable than MSE due to being in the same units as the target variable\n",
    "* Mean Absolute Error (MAE) - the average of the absolute differences between the predicted and actual values\n",
    "* R-squared (R2) - a measure of how much of the variation in the target is captured by your model\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "Mean Squared Error (MSE) is a metric used to measure the average squared difference between actual values and predicted values. It is commonly used in regression tasks to quantify the accuracy of a predictive model. Mathematically, MSE is defined as:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where $ n $ is the number of data points, $ y_i $ represents the actual value of the $ i $-th data point, and $ \\hat{y}_i $ is the predicted value of the $ i $-th data point.\n",
    "\n",
    "For example, suppose a model predicts housing prices for four houses as $ \\hat{y} = [250\\mathrm{k}, 300\\mathrm{k}, 200\\mathrm{k}, 400\\mathrm{k}] $, while the actual prices are $ y = [260\\mathrm{k}, 310\\mathrm{k}, 190\\mathrm{k}, 390\\mathrm{k}] $. The MSE would be calculated as:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{4} \\left( (260\\mathrm{k} - 250\\mathrm{k})^2 + (310\\mathrm{k} - 300\\mathrm{k})^2 + (190\\mathrm{k} - 200\\mathrm{k})^2 + (390\\mathrm{k} - 400\\mathrm{k})^2 \\right) $$\n",
    "\n",
    "This results in:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{4} \\left( 10\\mathrm{k}^2 + 10\\mathrm{k}^2 + (-10\\mathrm{k})^2 + (-10\\mathrm{k})^2 \\right) = 100M $$\n",
    "\n",
    "Thus, the MSE for this prediction is 100M, meaning that, on average, the squared error between predicted and actual housing prices is 100M. Notice this is *not* on the scale of dollars, it is on the scale of dollars *squared*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490bacd-d3df-4bba-9a3a-36d0ab00e3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import create_answer_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2b554-d357-439b-bdb3-acd693f44a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display metrics\n",
    "print(\"Regression Metrics for California Housing Dataset:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "# Display feature names and their coefficients\n",
    "feature_names = housing.feature_names\n",
    "coefficients = model.coef_\n",
    "\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "for name, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7d904-05b7-4240-85ba-41638a10fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Try varying the size of the test set above and re-running the code a few times. What effect does that test set size appear to have on the model's performance in this case?\", \"04-03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eee84-e652-4ab9-b5d9-fa32a4197477",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"In general, people tend to use test set sizes between 0.1 and 0.3. But that's still a wide range. What considerations do you think might help determine what an appropriate test set size is in a particular case?\", \"04-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842c4f3-8589-40f3-8010-36c694c966ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Do you think, compared with some baseline model, a second model could simultaneously be better on some metric and worse on another? Why or why not?\", \"04-05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f53c62-57d9-4d32-b038-2843d3e774c0",
   "metadata": {},
   "source": [
    "### 2. Classification Metrics\n",
    "\n",
    "In a classification problem, we are trying to predict a label. Some common metrics for evaluating classification models include:\n",
    "* Accuracy - the proportion of correctly classified instances\n",
    "* Precision - the proportion of true positive predictions among all positive *predictions*\n",
    "* Recall - the proportion of true positive predictions among all actual positive *instances*\n",
    "* F1 Score - the harmonic mean of precision and recall $\\left(F_1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\\right)$\n",
    "* ROC AUC - the area under the Receiver Operating Characteristic curve\n",
    "\n",
    "<img src=\"img/prec_rec.png\" alt=\"precision_recall\" style=\"width:30%\">\n",
    "\n",
    "Note that all of the above *except* for ROC AUC depend on your choice of threshold for classification. ROC AUC is a metric that is threshold-independent, and is often used when you want to compare models across different thresholds. \n",
    "\n",
    "Let's look at this in action, after fitting a logistic regression model on the UCI Ionosphere dataset. In this dataset, we are trying to predict whether a radar return is \"good\" or \"bad\" based on features such as the signal's amplitude and frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0f21c-58c8-403d-9991-626fd621473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Ionosphere dataset\n",
    "ionosphere = fetch_openml(name=\"ionosphere\", version=1, as_frame=True)\n",
    "X, y = ionosphere.data, ionosphere.target\n",
    "y = (y == 'g').astype(int)  # Convert to binary (good=1, bad=0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit a logistic regression model with reduced performance\n",
    "model = LogisticRegression(random_state=42, C=0.01, max_iter=1000) \n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Display metrics\n",
    "print(\"Classification Metrics for Ionosphere Dataset:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea9ce2-b02e-4e65-9d7d-872e2e6c864c",
   "metadata": {},
   "source": [
    "The ROC curve shows the true positive rate (i.e., the proportion of actual positive cases correctly identified) against the false positive rate (i.e., the proportion of actual negative cases incorrectly classified as positive) at various classification thresholds. The ROC AUC is just the area under the curve -- the higher, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1031e8-e26b-4501-b232-9aaaf5422b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Accuracy is usually not considered a good metric when a dataset is highly *imbalanced*, containing many more of one class than of others. Why do you think this is?\", \"04-06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ed8d0-7f66-4a08-910f-a823e79d8098",
   "metadata": {},
   "source": [
    "##### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model. It shows the number of true positives, false positives, true negatives, and false negatives. From the confusion matrix, we can calculate many different metrics, including accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebee7ed-fcbb-4494-92b6-5dd6db98eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix as a plot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(2, 2))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "            xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
