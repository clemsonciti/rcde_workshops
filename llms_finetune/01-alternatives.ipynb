{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "from transformers import set_seed\n",
    "SEED = 355\n",
    "torch.manual_seed(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "model = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\")\n",
    "\n",
    "input_text = \"1. If I have 23 apples and I give 7 to my friend and sell the rest for $1.33 each, how much money do I have?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=200, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "\n",
    "for i, sample_output in enumerate(output):\n",
    "    print(f\"########\\nOUTPUT {i}:\\n {tokenizer.decode(sample_output[len(input_ids[0]):], skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure we're using the appropriate chat template for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "input_messages = [{\"role\":\"system\", \"content\":\"Answer the user's question.\"},\n",
    "                  {\"role\":\"user\", \"content\":input_text}]\n",
    "input_ids = tokenizer.apply_chat_template(input_messages, add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=200, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "\n",
    "for i, sample_output in enumerate(output):\n",
    "    print(f\"########\\nOUTPUT {i}:\\n {tokenizer.decode(sample_output[len(input_ids[0]):], skip_special_tokens=True)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "input_messages = [{\"role\":\"system\", \"content\":\"Answer the user's question. Think step-by-step and double-check your arithmetic. Enclose your final answer in delimiters like `<answer>{your answer here}</answer>`.\"},\n",
    "                  {\"role\":\"user\", \"content\":input_text}]\n",
    "input_ids = tokenizer.apply_chat_template(input_messages, add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=300, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "\n",
    "for i, sample_output in enumerate(output):\n",
    "    print(f\"########\\nOUTPUT {i}:\\n {tokenizer.decode(sample_output[len(input_ids[0]):], skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples for FSL\n",
    "example_1_question = \"If you have 41 pizzas and each pizza has 8 slices, and you sell the slices for 2.15 each, how much money do you make?\"\n",
    "example_1_answer = \"\"\"Let's solve this step by step, breaking down arithmetic steps to make them easier:\n",
    "\n",
    "Calculate the total number of slices: 41 * 8 = (40 * 8) + (1 * 8) = 320 + 8 = 328 slices\n",
    "Calculate the total money made: 328 * 2.15 = (300 * 2.15) + (20 * 2.15) + (8 * 2.15) = ((300 * 2) + (300 * 0.15)) + ((20 * 2) + (20 * 0.15)) + ((8 * 2) + (8 * 0.15)) = (600 + 45) + (40 + 3) + (16 + 1.2) = 645 + 43 + 17.2 = 705.2\n",
    "<answer>705.2</answer>\"\"\"\n",
    "example_2_question = \"If Sandy has 61 acorns, gives 7 to Bob, and then sells the remaining acorns for $1.20 per acorn, how much money does she make?\"\n",
    "example_2_answer = \"\"\"Let's solve this step by step, breaking down arithmetic steps to make them easier:\n",
    "\n",
    "Calculate the acorns left after giving some to Bob: 61 - 7 = 54 acorns\n",
    "Calculate the total money made: 54 * 1.20 = (50 * 1.20) + (4 * 1.20) = ((50 + 1) + (50 * 0.20)) + ((4 * 1) + (4 * 0.20)) = (60 + 10) + (4 + 0.8) = 70 + 4.8 = 74.8 \n",
    "<answer>74.8</answer>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "input_messages = [{\"role\":\"system\", \"content\":\"Answer the user's question. Think step-by-step and double-check your arithmetic. Enclose your final answer in delimiters like `<answer>{your answer here}</answer>`.\"},\n",
    "                    {\"role\":\"user\", \"content\":example_1_question},\n",
    "                    {\"role\":\"assistant\", \"content\":example_1_answer},\n",
    "                    {\"role\":\"user\", \"content\":example_2_question},\n",
    "                    {\"role\":\"assistant\", \"content\":example_2_answer},\n",
    "                    {\"role\":\"user\", \"content\":input_text}]\n",
    "input_ids = tokenizer.apply_chat_template(input_messages, add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=200, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "\n",
    "for i, sample_output in enumerate(output):\n",
    "    print(f\"########\\nOUTPUT {i}:\\n {tokenizer.decode(sample_output[len(input_ids[0]):], skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Example  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredXMLLoader\n",
    "\n",
    "loader = UnstructuredXMLLoader(\n",
    "    \"./can_tax.xml\",\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=8192, chunk_overlap=1024)\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F\"NUMBER OF CHUNKS: {len(chunked_docs)}\\n\\n\")\n",
    "print(chunked_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a Sentence Transformer model \n",
    "model_name = \"nomic-ai/modernbert-embed-base\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)  \n",
    "embeddings.client.to('cuda')  # Explicitly move the model to GPU\n",
    "\n",
    "# Create a FAISS vectorstore using the Sentence Transformer embeddings\n",
    "vectorstore = FAISS.from_documents(chunked_docs, embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity search on a query \n",
    "query = \"I'm a schoolteacher, can I deduct expenses for my classroom?\"\n",
    "results = vectorstore.similarity_search(query, k=2)  # Get top 2 similar documents\n",
    "\n",
    "# Print the results\n",
    "for i,result in enumerate(results):\n",
    "    print(f\"RESULT {i}:\\n{result.page_content}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Give the below context from the Canadian tax code, answer the user's question. Make sure that your answer is based solely in the context provided below.\\n\\n## Context:\\n{context}\"\n",
    "user_prompt = \"I'm a schoolteacher, can I deduct expenses for my classroom?\"\n",
    "context_docs = '/n'.join([doc.page_content for doc in retriever.get_relevant_documents(query)])\n",
    "\n",
    "prompt_template = [{\"role\":\"system\", \"content\":system_prompt.format(context=context_docs)}, {\"role\":\"user\", \"content\":user_prompt}]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(prompt_template, add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(input_ids, max_new_tokens=200, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id, do_sample=True)\n",
    "\n",
    "for i, sample_output in enumerate(output):\n",
    "    print(f\"########\\nOUTPUT {i}:\\n {tokenizer.decode(sample_output[len(input_ids[0]):], skip_special_tokens=True)}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLMsFT2)",
   "language": "python",
   "name": "llmsft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
