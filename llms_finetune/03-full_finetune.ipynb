{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "# Split the dataset into training and validation sets\n",
    "train_val_dataset = ds[\"test\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_dataset[\"train\"]\n",
    "eval_dataset = train_val_dataset[\"test\"]\n",
    "\n",
    " # This example only has a test split, so we use that, for demonstration purposes.\n",
    "\n",
    " # Load the model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# The model may not have a pad token set by default, so set it (using the EOS token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) anaconda3/2023.09-0   2) cuda/12.3.0\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_utils import tokenize_and_mask, tokenize_for_generation, generate_and_print, data_collator\n",
    "\n",
    "data_collator_fn = lambda features: data_collator(features, tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a969520ec9f48bcb1208529c286713b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b58857ac6a4ef7acd97ff5ceca6ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0d3b6aacf74cf1baedbfa87affde4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the formatting function over the dataset.\n",
    "# This applies the formatting function to each example in the dataset.\n",
    "# The result is that we have a dataset where each math problem is formatted as a prompt for the model,\n",
    "# and the solution is formatted as a response that the model should generate.\n",
    "# Each example is also tokenized\n",
    "# (If your dataset is large you might use batched=True; here we keep it simple.)\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "eval_dataset_tokenized = eval_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "# Get a sample dataset so we can examine model generations before and after training\n",
    "sample_dataset = eval_dataset.select(range(3))\n",
    "sample_dataset_tokenized = sample_dataset.map(tokenize_for_generation, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "train_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "eval_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "sample_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-25 10:39:34,352] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/cehrett/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: warning: libm.so.6, needed by /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/libstdc++.so.6: undefined reference to `fesetround@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/libstdc++.so.6: undefined reference to `fegetround@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 04:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.899900</td>\n",
       "      <td>1.035551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.977300</td>\n",
       "      <td>1.005583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.929400</td>\n",
       "      <td>0.998815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.906800</td>\n",
       "      <td>0.975086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>0.977636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>0.978490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>1.031475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.491700</td>\n",
       "      <td>1.017838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>1.045021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.455600</td>\n",
       "      <td>0.986534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>0.995112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>1.088563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>1.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.193500</td>\n",
       "      <td>1.132212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>1.143911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.176600</td>\n",
       "      <td>1.159629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>1.164687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=0.526299475403557, metrics={'train_runtime': 242.6472, 'train_samples_per_second': 5.564, 'train_steps_per_second': 0.705, 'total_flos': 1916930919720960.0, 'train_loss': 0.526299475403557, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set up training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned-math\",\n",
    "    per_device_train_batch_size=8,  # Adjust as needed\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    fp16=True,  # Use mixed precision if supported.\n",
    "    eval_strategy=\"steps\",  # Evaluate every eval_steps\n",
    "    eval_steps=10,  # Evaluate every x steps\n",
    "    save_total_limit=1, # Only save one checkpoint\n",
    "    load_best_model_at_end=True, # Load the best model at the end of training\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Set up the Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=eval_dataset_tokenized,\n",
    "    data_collator=data_collator_fn,\n",
    ")\n",
    "\n",
    "# Start fine-tuning.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen-finetuned-math-final/tokenizer_config.json',\n",
       " './qwen-finetuned-math-final/special_tokens_map.json',\n",
       " './qwen-finetuned-math-final/vocab.json',\n",
       " './qwen-finetuned-math-final/merges.txt',\n",
       " './qwen-finetuned-math-final/added_tokens.json',\n",
       " './qwen-finetuned-math-final/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model_path = \"./qwen-finetuned-math-final\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"./qwen-finetuned-math-final\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "Roslyn has ten boxes. Five of the boxes contain pencils, four of the boxes\n",
      "contain pens, and two of the boxes contain both pens and pencils. How many boxes\n",
      "contain neither pens nor pencils?\n",
      "\n",
      "True Solution:\n",
      "Of the 5 boxes with pencils, 2 have pens also, so $5-2=3$ have pencils only.\n",
      "Similarly, $4-2 =2$ of the boxes have pens only:   [asy] unitsize(0.05cm);\n",
      "label(\"Pencils\", (2,74)); label(\"Pens\", (80,74)); draw(Circle((30,45), 22));\n",
      "draw(Circle((58, 45), 22)); label(\"$2$\", (44, 45));\n",
      "label(scale(0.8)*\"$3$\",(28,58)); label(scale(0.8)*\"$2$\",(63,58)); [/asy]  That\n",
      "gives us $3+2+2=7$ boxes with pens, pencils, or both.  This leaves $10-7 =\n",
      "\\boxed{3}$ with neither.\n",
      "\n",
      "Model's Solution:\n",
      "The total number of boxes that contain pencils is $5-2=3$. The total number of\n",
      "boxes that contain pens is $4-1=3$. Therefore, there are $3+3=\\boxed{6}$ boxes\n",
      "that contain both pens and pencils.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "William Sydney Porter tried to perform the calculation $\\frac{-3+4i}{1+2i}$.\n",
      "However, he accidentally missed the minus sign, finding\n",
      "$\\frac{3+4i}{1+2i}=\\frac{11}{5}-\\frac{2}{5}i$. What answer should he have\n",
      "obtained?\n",
      "\n",
      "True Solution:\n",
      "To perform division in complex numbers, we multiply both the numerator and the\n",
      "denominator by the conjugate of the denominator. In this case, the conjugate of\n",
      "$1+2i$ is $1-2i$. Multiplying: \\begin{align*}\n",
      "\\frac{-3+4i}{1+2i}&=\\frac{(-3+4i)(1-2i)}{(1+2i)(1-2i)}\\\\\n",
      "&=\\frac{-3+4i+6i-8i^2}{1+2i-2i-4i^2}\\\\ &=\\frac{5+10i}{5}\\\\ &=\\boxed{1+2i}\n",
      "\\end{align*}\n",
      "\n",
      "Model's Solution:\n",
      "We can use the property that multiplying by a complex number is the same thing\n",
      "as multiplying by its magnitude and then taking the conjugate:\n",
      "\\[\\frac{3+4i}{1+2i} = \\frac{(3+4i)(1-2i)}{(1+2i)^2 (1-2i)}.\\]Note that $|1+2i| =\n",
      "1+\\sqrt{1^2 - 4(1)(2) } = \\sqrt{7},$ so we have\n",
      "\\[\\frac{(3+4i)(1-2i)}{\\sqrt{7}(1-2i) (1-2i)} =\n",
      "\\frac{(3+4i)(1-2i)}{\\sqrt{7}(1-2i) (1-2i)}.\\]Expanding this out gives us\n",
      "\\[\\frac{(3+4i)(1-2i)}{\\sqrt{7}(1-2i) (1-2i)} = \\frac{9-6i-10i+8}{\\sqrt{7}(1-2i)\n",
      "(1-2i)} = \\frac{9-16i}{\\sqrt{7}(1-2i) (1-2i) =\n",
      "\\boxed{\\frac{9-16i}{\\sqrt{7}}=\\boxed{-\\frac{3-16i}{7}=-\\frac{5}{7}}.\\]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "I have a deck of $54$ cards, and I deal all of the cards to $x$ players, with\n",
      "each player getting $y$ cards. If $x$ is at least $2$ and $y$ is at least $5$,\n",
      "then how many possible values of $x$ are there?\n",
      "\n",
      "True Solution:\n",
      "We want $xy=54=2 \\cdot 3^3$ such that $x$ is at least $2$ and $y$ is at least\n",
      "$5$. Thus, the possible combinations $(x,y)$ are $(2,27)$, $(3,18)$, $(6,9)$,\n",
      "and $(9,6)$. There are $\\boxed{4}$ such combinations.\n",
      "\n",
      "Model's Solution:\n",
      "There are 54 cards in total which means there are at most $18$ cards in the\n",
      "deck. Therefore, we need to find the number of nonnegative integer solutions for\n",
      "$x$ and $y$. Since $x \\ge 2$ and $y \\ge 5$, the number of nonnegative integer\n",
      "solutions for $x$ and $y$ is given by the binomial coefficient $\\binom{x+3}{y-5}\n",
      "= \\boxed{6}$.  Let us verify that this solution satisfies our conditions: if $x\n",
      "= 0$ or $y = 0$, there are no solutions; if $x > 0$ and $y > 5$, there are no\n",
      "solutions; if $x < 0$ and $y > 5$, there are no solutions. This leaves us with\n",
      "only one solution when both $x$ and $y$ are positive integers: $(0,5), (1,5),\n",
      "\\dots, (4,5)$, so there are $\\boxed{6}$ possible values of $x$.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and print model outputs after training\n",
    "generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "# Garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Trainer` class takes care of a lot of things under the hood. If you'd rather deal with these details directly yourself, you can avoid using the `Trainer` class and set up the training logic yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 113/113 [00:52<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Average training loss: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:01<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Average evaluation loss: 1.3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 113/113 [00:51<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Average training loss: 0.2716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 13/13 [00:01<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Average evaluation loss: 1.4568\n",
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define your dataloaders using the custom data_collator to pad variable-length sequences\n",
    "train_dataloader = DataLoader(train_dataset_tokenized, batch_size=4, shuffle=True, collate_fn=data_collator_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset_tokenized, batch_size=4, shuffle=False, collate_fn=data_collator_fn)\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "device = model.device\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move batch tensors to the right device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average training loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation loop (optional)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            # Move batch tensors to the right device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average evaluation loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
