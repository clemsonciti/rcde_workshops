{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "# Split the dataset into training and validation sets\n",
    "train_val_dataset = ds[\"test\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_dataset[\"train\"]\n",
    "eval_dataset = train_val_dataset[\"test\"]\n",
    "\n",
    " # This example only has a test split, so we use that, for demonstration purposes.\n",
    "\n",
    " # Load the model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map={'':0} # \"auto\"\n",
    ")\n",
    "\n",
    "# The model may not have a pad token set by default, so set it (using the EOS token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) anaconda3/2023.09-0   2) cuda/12.3.0\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workshop_utils import tokenize_and_mask, tokenize_for_generation, generate_and_print, data_collator\n",
    "\n",
    "data_collator_fn = lambda features: data_collator(features, tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e6af3602d94c34aba22fb21a07bc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292a13326d524c0c88b3dc31044fba86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e0f24655b444bca5ca72c2d68aac0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the formatting function over the dataset.\n",
    "# This applies the formatting function to each example in the dataset.\n",
    "# The result is that we have a dataset where each math problem is formatted as a prompt for the model,\n",
    "# and the solution is formatted as a response that the model should generate.\n",
    "# Each example is also tokenized\n",
    "# (If your dataset is large you might use batched=True; here we keep it simple.)\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "eval_dataset_tokenized = eval_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "# Get a sample dataset so we can examine model generations before and after training\n",
    "sample_dataset = eval_dataset.select(range(3))\n",
    "sample_dataset_tokenized = sample_dataset.map(tokenize_for_generation, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "train_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "eval_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "sample_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-28 13:58:59,233] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/cehrett/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: warning: libm.so.6, needed by /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/libstdc++.so.6: undefined reference to `fesetround@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/libstdc++.so.6: undefined reference to `fegetround@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 03:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.894042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.899300</td>\n",
       "      <td>0.853401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.843772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.416400</td>\n",
       "      <td>0.869267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.453800</td>\n",
       "      <td>0.837742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=0.6681024819089655, metrics={'train_runtime': 210.9524, 'train_samples_per_second': 4.266, 'train_steps_per_second': 0.54, 'total_flos': 1267302898667520.0, 'train_loss': 0.6681024819089655, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Set up training arguments.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-finetuned-math\",\n",
    "    per_device_train_batch_size=4,  # Adjust as needed\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=20,\n",
    "    save_steps=20,\n",
    "    fp16=True,  # Use mixed precision if supported.\n",
    "    eval_strategy=\"steps\",  # Evaluate every eval_steps\n",
    "    eval_steps=20,  # Evaluate every x steps\n",
    "    save_total_limit=1, # Only save one checkpoint\n",
    "    load_best_model_at_end=True, # Load the best model at the end of training\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Set up the Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=eval_dataset_tokenized,\n",
    "    data_collator=data_collator_fn,\n",
    ")\n",
    "\n",
    "# Start fine-tuning.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen-finetuned-math-final/tokenizer_config.json',\n",
       " './qwen-finetuned-math-final/special_tokens_map.json',\n",
       " './qwen-finetuned-math-final/vocab.json',\n",
       " './qwen-finetuned-math-final/merges.txt',\n",
       " './qwen-finetuned-math-final/added_tokens.json',\n",
       " './qwen-finetuned-math-final/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model_path = \"./qwen-finetuned-math-final\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"./qwen-finetuned-math-final\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "If the fourth term of an arithmetic sequence is $200$ and the eighth term is\n",
      "$500$, what is the sixth term?\n",
      "\n",
      "True Solution:\n",
      "The sixth term is exactly halfway between the fourth and the eighth in the\n",
      "arithmetic sequence, so it is the average of the two terms. Therefore, the sixth\n",
      "term is $(200 + 500)/2 = \\boxed{350}$.  We also could have found the common\n",
      "difference by noting that there are four steps between the fourth term and the\n",
      "eighth term.  So, if $d$ is the common difference, we have $4d = 500-200 = 300$.\n",
      "Therefore, we find $d=75$.  The sixth term is two steps after the fourth, or\n",
      "$200 + 2d = \\boxed{350}$.\n",
      "\n",
      "Model's Solution:\n",
      "The common difference between terms in an arithmetic sequence is constant.\n",
      "Therefore, if the first term is $a$, then the common difference is\n",
      "$\\frac{500-200}{8-4}=\\frac{300}{4}=75$.  Then, the sixth term is\n",
      "$a+6\\cdot75=1500$.  Thus, the sixth term is $\\boxed{1500}$.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "There are 360 people in my school.  15 take calculus, physics, and chemistry,\n",
      "and 15 don't take any of them.  180 take calculus.  Twice as many students take\n",
      "chemistry as take physics.  75 take both calculus and chemistry, and 75 take\n",
      "both physics and chemistry.  Only 30 take both physics and calculus.  How many\n",
      "students take physics?\n",
      "\n",
      "True Solution:\n",
      "Let $x$ be the number of students taking physics, so the number in chemistry is\n",
      "$2x$.  There are 15 students taking all three, and 30 students in both physics\n",
      "and calculus, meaning there are $30 - 15 = 15$ students in just physics and\n",
      "calculus.  Similarly there are $60$ students in just chemistry and calculus, and\n",
      "$60$ in physics and chemistry.  Since there are $x$ students in physics and $15\n",
      "+ 15 + 60 = 90$ students taking physics along with other classes, $x - 90$\n",
      "students are just taking physics.  Similarly, there are $2x - 135$ students\n",
      "taking just chemistry and $90$ students taking just calculus.  Knowing that\n",
      "there are 15 students not taking any of them, the sum of these eight categories\n",
      "is 360, the total number of people at the school:   \\[ (x - 90) + (2x - 135) +\n",
      "90 + 60 + 15 + 60 + 15 + 15 = 360. \\] We solve for $x$ and find that the number\n",
      "of physics students is $x = \\boxed{110}$.\n",
      "\n",
      "Model's Solution:\n",
      "Let $a,$ $b,$ and $c$ be the number of students who take calculus, physics, and\n",
      "chemistry, respectively.  Then we have \\begin{align*} a+b+c&=360\\\\\n",
      "2a&=b\\qquad\\Rightarrow\\\\ c&=75-a-b \\end{align*} From the first equation, we know\n",
      "that $a+b+c=360-75=\\boxed{185}$, so $c=75$.  We can substitute into the third\n",
      "equation to solve for $a$: \\begin{align*} c&=75\\\\ c+75&=360\\\\ 140&=360-a\n",
      "\\end{align*} Therefore, $a=185$, which means that $\\boxed{185}$ students take\n",
      "physics.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "Solve for the positive value of $x$ such that $\\sqrt[3]{x^2 - 4x + 4} = 16$.\n",
      "\n",
      "True Solution:\n",
      "We first cube each side of the equation to get $x^2 - 4x + 4 = 16^3$.  Notice\n",
      "that $x^2 - 4x + 4 = (x-2)^2.$  Therefore, we have that $x-2 = \\pm 16^{3/2} =\n",
      "\\pm 64$.  Therefore, the possible values of $x$ are $-62$ and $66,$ and the only\n",
      "positive value is therefore $\\boxed{66}$.\n",
      "\n",
      "Model's Solution:\n",
      "If we cube both sides, we get \\[\\sqrt[3]{x^8 - 4x^3 + 4x^2} = 2^{12}\\]so $x^8 -\n",
      "4x^3 + 4x^2 = 2^{12}$.  Since $x$ is positive and cubic, this simplifies to $x^8\n",
      "- 4x^3 + 4x^2 = (x^3 - 2)^2$.  So $x^8 - 4x^3 + 4x^2 = x^6 - 4x^3 + 4x^2$, which\n",
      "simplifies to $x^4 - 4x^3 + 4x^2 = 0$.  Then $x^4 - 4x^3 + 4x^2 = (x^2 - 2)(x^2\n",
      "+ 2x + 2)$, so $x^4 - 4x^3 + 4x^2 = 0$ if and only if $x^2 + 2x + 2 = 0$.  Using\n",
      "the quadratic formula, we find that $x = \\boxed{\\frac{-2 \\pm \\sqrt{2^2 -\n",
      "4(1)(2)}}{2}$.  OR  We can also write $x^8 - 4x^3 + 4x^2 = (x^4 - 2x^2)^2$ as\n",
      "$(x^4 - 2x^2)^2 = (x^2 - 2)^2$.  So by the same argument as above, $x^4 - 2x^2 =\n",
      "0$ if and only if $x^2 - 2 = 0$.  Thus, $x = \\boxed{\\frac{-2}{2}} = \\boxed{-1}$.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and print model outputs after training\n",
    "generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "# Garbage collection\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Trainer` class takes care of a lot of things under the hood. If you'd rather deal with these details directly yourself, you can avoid using the `Trainer` class and set up the training logic yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  99%|█████████▉| 223/225 [00:47<00:00,  4.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 26\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define your dataloaders using the custom data_collator to pad variable-length sequences\n",
    "train_dataloader = DataLoader(train_dataset_tokenized, batch_size=2, shuffle=True, collate_fn=data_collator_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset_tokenized, batch_size=2, shuffle=False, collate_fn=data_collator_fn)\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "device = model.device\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move batch tensors to the right device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average training loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation loop (optional)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            # Move batch tensors to the right device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average evaluation loss: {avg_eval_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLMsFT2)",
   "language": "python",
   "name": "llmsft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
