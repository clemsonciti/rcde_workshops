{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Libraries\n",
    "Install necessary libraries like wandb, transformers, and datasets. Then, import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import wandb\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcehrett\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do LoRA with W&B logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cehrett/Projects/RCD_Workshops/rcde_workshops/llms_finetune/wandb/run-20250228_144844-9acnb442</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cehrett/llms_finetune/runs/9acnb442' target=\"_blank\">golden-blaze-10</a></strong> to <a href='https://wandb.ai/cehrett/llms_finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cehrett/llms_finetune' target=\"_blank\">https://wandb.ai/cehrett/llms_finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cehrett/llms_finetune/runs/9acnb442' target=\"_blank\">https://wandb.ai/cehrett/llms_finetune/runs/9acnb442</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize WandB\n",
    "wandb.init(project=\"llms_finetune\", job_type=\"training\")\n",
    "\n",
    "# Access configuration\n",
    "config = wandb.config\n",
    "config.model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "config.dataset_name = \"HuggingFaceH4/MATH-500\"\n",
    "config.lora_r = 8\n",
    "config.lora_alpha = 32\n",
    "config.lora_dropout = 0.05\n",
    "config.per_device_train_batch_size = 8\n",
    "config.learning_rate = 1e-4\n",
    "config.num_train_epochs = 2\n",
    "config.fp16 = True\n",
    "config.bf16 = False\n",
    "config.save_steps = 10\n",
    "config.eval_steps = 10\n",
    "config.save_total_limit = 1\n",
    "config.optim = \"paged_adamw_32bit\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data (just as in previous noteboooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0203fd7b98f048bca64caf314349d3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d589106c5e04dd19350d2410a819404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8caa06188b416a91427eff3a5cafe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(config.dataset_name)\n",
    "train_val_dataset = ds[\"test\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_dataset[\"train\"]\n",
    "eval_dataset = train_val_dataset[\"test\"]\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = config.model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# The model may not have a pad token set by default, so set it (using the EOS token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "from workshop_utils import tokenize_and_mask, tokenize_for_generation, generate_and_print, data_collator\n",
    "\n",
    "data_collator_fn = lambda features: data_collator(features, tokenizer=tokenizer)\n",
    "\n",
    "# Map the formatting function over the dataset.\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "eval_dataset_tokenized = eval_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "# Get a sample dataset so we can examine model generations before and after training\n",
    "sample_dataset = eval_dataset.select(range(3))\n",
    "sample_dataset_tokenized = sample_dataset.map(tokenize_for_generation, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "train_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "eval_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "sample_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=config.lora_r, \n",
    "    lora_alpha=config.lora_alpha, \n",
    "    lora_dropout=config.lora_dropout,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # Replace with the target modules of your model\n",
    ")\n",
    "\n",
    "# Add LoRA adapter to the model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform training (while logging to W&B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-28 14:48:49,984] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/cehrett/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: warning: libm.so.6, needed by /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/libstdc++.so.6: undefined reference to `fesetround@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/libstdc++.so.6: undefined reference to `fegetround@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 00:42, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.720800</td>\n",
       "      <td>0.717406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.726600</td>\n",
       "      <td>0.700726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.663600</td>\n",
       "      <td>0.695522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.721800</td>\n",
       "      <td>0.692667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.741200</td>\n",
       "      <td>0.691015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.789300</td>\n",
       "      <td>0.689844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.709200</td>\n",
       "      <td>0.688991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.733100</td>\n",
       "      <td>0.688324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>0.688021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.687885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.705800</td>\n",
       "      <td>0.687827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅█▁▁▅▆▅▇▆▄▅</td></tr><tr><td>eval/samples_per_second</td><td>▄▁██▄▃▄▂▃▅▄</td></tr><tr><td>eval/steps_per_second</td><td>▄▁██▄▃▄▂▃▅▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▁▃▅▂▁▂▄▂▂▂</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>▅▅▂▅▆█▄▅▁▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.68783</td></tr><tr><td>eval/runtime</td><td>0.9019</td></tr><tr><td>eval/samples_per_second</td><td>55.437</td></tr><tr><td>eval/steps_per_second</td><td>7.761</td></tr><tr><td>total_flos</td><td>1266800085433344.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>114</td></tr><tr><td>train/grad_norm</td><td>0.73051</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7058</td></tr><tr><td>train_loss</td><td>0.71663</td></tr><tr><td>train_runtime</td><td>43.5867</td></tr><tr><td>train_samples_per_second</td><td>20.648</td></tr><tr><td>train_steps_per_second</td><td>2.615</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-blaze-10</strong> at: <a href='https://wandb.ai/cehrett/llms_finetune/runs/9acnb442' target=\"_blank\">https://wandb.ai/cehrett/llms_finetune/runs/9acnb442</a><br> View project at: <a href='https://wandb.ai/cehrett/llms_finetune' target=\"_blank\">https://wandb.ai/cehrett/llms_finetune</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250228_144844-9acnb442/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-lora-math\",          # Output directory\n",
    "    num_train_epochs=config.num_train_epochs,              # Number of training epochs\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,   # Batch size per device during training\n",
    "    optim=config.optim,        # Optimizer, you might need to install accelerate: pip install accelerate -U\n",
    "    save_steps=config.save_steps,                   # Save checkpoint every X updates steps\n",
    "    eval_steps=config.eval_steps,                   # Evaluate every X updates steps\n",
    "    eval_strategy=\"steps\",           # Evaluation strategy\n",
    "    save_total_limit=config.save_total_limit,              # Limit the total amount of checkpoints\n",
    "    load_best_model_at_end=True,     # Load the best model when finished training (default is True)\n",
    "    logging_steps=10,                # Log every X updates steps\n",
    "    learning_rate=config.learning_rate,              # Learning rate\n",
    "    fp16=config.fp16,                       # Use mixed precision training\n",
    "    bf16=config.bf16,                      # Use bfloat16 training\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=eval_dataset_tokenized,\n",
    "    data_collator=data_collator_fn,     # Data collator if needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Finish the WandB run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a WandB Sweep\n",
    "Define a sweep configuration with hyperparameters to tune. Use `wandb.sweep()` to create a sweep and `wandb.agent()` to run the sweep agent, optimizing the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the config we'll use for each run during the sweep\n",
    "\n",
    "config_defaults = {\n",
    "    \"model_name\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"dataset_name\": \"HuggingFaceH4/MATH-500\",\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"num_train_epochs\": 2,\n",
    "    \"fp16\": True,\n",
    "    \"bf16\": False,\n",
    "    \"save_steps\": 20,\n",
    "    \"eval_steps\": 20,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"save_total_limit\": 1,\n",
    "    \"epochs\": 2,\n",
    "    \"optim\": \"paged_adamw_32bit\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ciseumso\n",
      "Sweep URL: https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7vf3ial4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 16\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cehrett/Projects/RCD_Workshops/rcde_workshops/llms_finetune/wandb/run-20250228_144937-7vf3ial4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cehrett/llm-finetuning/runs/7vf3ial4' target=\"_blank\">crimson-sweep-1</a></strong> to <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cehrett/llm-finetuning/runs/7vf3ial4' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/runs/7vf3ial4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.2253922/ipykernel_4092456/1608663663.py:95: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [226/226 00:38, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.750669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>0.742140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.734700</td>\n",
       "      <td>0.738769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>0.737239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.694700</td>\n",
       "      <td>0.736443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.658600</td>\n",
       "      <td>0.735757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.747200</td>\n",
       "      <td>0.734871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.649800</td>\n",
       "      <td>0.734308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.734044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.771800</td>\n",
       "      <td>0.733845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.621800</td>\n",
       "      <td>0.733679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁██████████</td></tr><tr><td>eval/steps_per_second</td><td>▁██████████</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▂▂█▂▂▃▁▂▂▁▂▁▃▃▁▂▃▂▂▁▃</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▁▃█▅▅▃▅▄▂▃▄▅▁▂▃▇▃▆▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.73368</td></tr><tr><td>eval/runtime</td><td>0.5781</td></tr><tr><td>eval/samples_per_second</td><td>86.495</td></tr><tr><td>eval/steps_per_second</td><td>12.109</td></tr><tr><td>total_flos</td><td>1050475838421504.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>226</td></tr><tr><td>train/grad_norm</td><td>1.02329</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6218</td></tr><tr><td>train_loss</td><td>0.70939</td></tr><tr><td>train_runtime</td><td>38.8425</td></tr><tr><td>train_samples_per_second</td><td>23.171</td></tr><tr><td>train_steps_per_second</td><td>5.818</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">crimson-sweep-1</strong> at: <a href='https://wandb.ai/cehrett/llm-finetuning/runs/7vf3ial4' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/runs/7vf3ial4</a><br> View project at: <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250228_144937-7vf3ial4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u8d1dtkt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 4\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cehrett/Projects/RCD_Workshops/rcde_workshops/llms_finetune/wandb/run-20250228_145023-u8d1dtkt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cehrett/llm-finetuning/runs/u8d1dtkt' target=\"_blank\">fancy-sweep-2</a></strong> to <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cehrett/llm-finetuning/runs/u8d1dtkt' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/runs/u8d1dtkt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.2253922/ipykernel_4092456/1608663663.py:95: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 00:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.622900</td>\n",
       "      <td>0.747199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.715100</td>\n",
       "      <td>0.739966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.614600</td>\n",
       "      <td>0.737876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>0.736833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.724600</td>\n",
       "      <td>0.736279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁▅█▂▂</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁▇▇</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁▇▇</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▄▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▄▄▄▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅█▃▁▂▁▂▂▃▃▆</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▁▆▄▅▁▄▂▅▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.73628</td></tr><tr><td>eval/runtime</td><td>0.7221</td></tr><tr><td>eval/samples_per_second</td><td>69.242</td></tr><tr><td>eval/steps_per_second</td><td>9.694</td></tr><tr><td>total_flos</td><td>1297336262238720.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>114</td></tr><tr><td>train/grad_norm</td><td>1.87369</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6332</td></tr><tr><td>train_loss</td><td>0.70294</td></tr><tr><td>train_runtime</td><td>36.514</td></tr><tr><td>train_samples_per_second</td><td>24.648</td></tr><tr><td>train_steps_per_second</td><td>3.122</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-2</strong> at: <a href='https://wandb.ai/cehrett/llm-finetuning/runs/u8d1dtkt' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/runs/u8d1dtkt</a><br> View project at: <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250228_145023-u8d1dtkt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: il0iyxef with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_r: 8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cehrett/Projects/RCD_Workshops/rcde_workshops/llms_finetune/wandb/run-20250228_145109-il0iyxef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cehrett/llm-finetuning/runs/il0iyxef' target=\"_blank\">fragrant-sweep-3</a></strong> to <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/sweeps/ciseumso</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cehrett/llm-finetuning/runs/il0iyxef' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/runs/il0iyxef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/slurm.2253922/ipykernel_4092456/1608663663.py:95: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [226/226 00:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.872400</td>\n",
       "      <td>0.837065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.748700</td>\n",
       "      <td>0.811215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.792400</td>\n",
       "      <td>0.789239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.723900</td>\n",
       "      <td>0.775309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.728500</td>\n",
       "      <td>0.768237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>0.763917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.796200</td>\n",
       "      <td>0.761057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.875700</td>\n",
       "      <td>0.757795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.810600</td>\n",
       "      <td>0.756980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>0.756565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▄▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▁▂▁▂▂▂▂▁▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█▇█▇▇▇▆█▇█</td></tr><tr><td>eval/steps_per_second</td><td>▁█▇█▇▇▇▆█▇█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆▄▅█▅▅▇▂▅▃▁▃▁▅▆▂▂▄▃▂▁▄</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>▇▇▂▃█▅▅▃▅▃▁▂▃▅▁▂▃▇▂▅▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.75657</td></tr><tr><td>eval/runtime</td><td>0.7512</td></tr><tr><td>eval/samples_per_second</td><td>66.556</td></tr><tr><td>eval/steps_per_second</td><td>9.318</td></tr><tr><td>total_flos</td><td>1048893678598656.0</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>226</td></tr><tr><td>train/grad_norm</td><td>1.5932</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6544</td></tr><tr><td>train_loss</td><td>0.75857</td></tr><tr><td>train_runtime</td><td>45.8477</td></tr><tr><td>train_samples_per_second</td><td>19.63</td></tr><tr><td>train_steps_per_second</td><td>4.929</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-sweep-3</strong> at: <a href='https://wandb.ai/cehrett/llm-finetuning/runs/il0iyxef' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning/runs/il0iyxef</a><br> View project at: <a href='https://wandb.ai/cehrett/llm-finetuning' target=\"_blank\">https://wandb.ai/cehrett/llm-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250228_145109-il0iyxef/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a sweep configuration with hyperparameters to tune\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",  # Random search; other options include \"grid\", \"bayesian\", etc.\n",
    "    \"metric\": {\n",
    "        \"name\": \"eval/loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [1e-5, 1e-4, 1e-3]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [4, 8]\n",
    "        },\n",
    "        \"lora_r\": {\n",
    "            \"values\": [4, 8, 16]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"llm-finetuning\")\n",
    "\n",
    "# Define the training function\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Load the dataset\n",
    "    ds = load_dataset(wandb.config.dataset_name)\n",
    "    train_val_dataset = ds[\"test\"].train_test_split(test_size=0.1)\n",
    "    train_dataset = train_val_dataset[\"train\"]\n",
    "    eval_dataset = train_val_dataset[\"test\"]\n",
    "    \n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(config.model_name, device_map=\"auto\")\n",
    "\n",
    "    # The model may not have a pad token set by default, so set it (using the EOS token)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    from workshop_utils import tokenize_and_mask, tokenize_for_generation, generate_and_print, data_collator\n",
    "    \n",
    "    data_collator_fn = lambda features: data_collator(features, tokenizer=tokenizer)\n",
    "    \n",
    "    # Map the formatting function over the dataset.\n",
    "    train_dataset_tokenized = train_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    eval_dataset_tokenized = eval_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    \n",
    "    # Get a sample dataset so we can examine model generations before and after training\n",
    "    sample_dataset = eval_dataset.select(range(3))\n",
    "    sample_dataset_tokenized = sample_dataset.map(tokenize_for_generation, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "    \n",
    "    train_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "    eval_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "    sample_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    \n",
    "    # Define LoRA Config\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, \n",
    "        inference_mode=False, \n",
    "        r=config.lora_r, \n",
    "        lora_alpha=config.lora_alpha, \n",
    "        lora_dropout=config.lora_dropout,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"] # Replace with the target modules of your model\n",
    "    )\n",
    "    \n",
    "    # Add LoRA adapter to the model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=config.epochs,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        save_steps=config.save_steps,\n",
    "        eval_steps=config.eval_steps,\n",
    "        eval_strategy=config.eval_strategy,\n",
    "        save_total_limit=1,\n",
    "        learning_rate=config.learning_rate,\n",
    "        fp16=config.fp16,\n",
    "        bf16=config.bf16,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_tokenized,\n",
    "        eval_dataset=eval_dataset_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator_fn,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    # model.save_pretrained(f\"./model_{wandb.run.id}\")\n",
    "    # tokenizer.save_pretrained(f\"./model_{wandb.run.id}\")\n",
    "    \n",
    "    # # Log model checkpoint as artifact\n",
    "    # artifact = wandb.Artifact(f\"model_{wandb.run.id}\", type=\"model\")\n",
    "    # artifact.add_dir(f\"./model_{wandb.run.id}\")\n",
    "    # wandb.log_artifact(artifact)\n",
    "    \n",
    "    # Finish the WandB run\n",
    "    wandb.finish()\n",
    "\n",
    "# Run the sweep agent\n",
    "wandb.agent(sweep_id, function=train, count=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLMsFT2)",
   "language": "python",
   "name": "llmsft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
