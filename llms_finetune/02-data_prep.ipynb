{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div id=\"pdf-container\">\n",
       "        <iframe src=\"slides/Slides_part5.pdf\" width=\"1000\" height=\"800\" onerror=\"showFallback()\" id=\"pdf-frame\"></iframe>\n",
       "    </div>\n",
       "    <div id=\"fallback-message\" style=\"display: none;\">\n",
       "        <p><strong>⚠️ Unable to load PDF.</strong></p>\n",
       "        <p>Please <a href=\"slides/Slides_part5.pdf\" target=\"_blank\">click here</a> to open it in a new tab.</p>\n",
       "    </div>\n",
       "    <script>\n",
       "        function showFallback() {\n",
       "            document.getElementById(\"pdf-container\").style.display = \"none\";\n",
       "            document.getElementById(\"fallback-message\").style.display = \"block\";\n",
       "        }\n",
       "\n",
       "        // Timeout at 1.5s\n",
       "        setTimeout(function() {\n",
       "            var iframe = document.getElementById(\"pdf-frame\");\n",
       "            if (!iframe.contentDocument || iframe.contentDocument.body.childElementCount === 0) {\n",
       "                showFallback();\n",
       "            }\n",
       "        }, 1500);\n",
       "    </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from workshop_utils import display_pdf\n",
    "\n",
    "display_pdf(\"Slides_part5.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through an example of preparing data to use to fine-tune our model. Along the way, we'll develop functions that we can put in our `workshop_utils.py` script so we can easily import them into any script or notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "# Split the dataset into training and validation sets\n",
    "train_val_dataset = ds[\"test\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_dataset[\"train\"]\n",
    "eval_dataset = train_val_dataset[\"test\"]\n",
    "\n",
    " # This example only has a test split, so we use that, for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import random\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "sample = train_dataset[random.randint(0, len(train_dataset) - 1)]\n",
    "\n",
    "problem = wrapper.fill(sample['problem'])\n",
    "solution = wrapper.fill(sample['solution'])\n",
    "\n",
    "print('Problem:\\n', problem, '\\n\\n')\n",
    "print('Solution:\\n', solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# The model may not have a pad token set by default, so set it (using the EOS token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a system prompt to be prepended to the user prompt.\n",
    "# This is a simple example, but you could use a more complex system prompt.\n",
    "system_prompt = \"Solve the following math problem.\"\n",
    "\n",
    "# Define a function to format the prompt and apply loss masking.\n",
    "# This function builds a full text with a \"User:\" prompt and an \"Assistant:\" response.\n",
    "# It then computes which tokens belong to the prompt (to be masked in the loss)\n",
    "# The function assumes that each example has a `problem` and a `solution`, which is true for the MATH-500 dataset.\n",
    "def tokenize_and_mask(example, tokenizer, max_length=1024, system_prompt=system_prompt):\n",
    "    # Build a prompt with the system, user, and assistant messages.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": example['solution']}\n",
    "    ]\n",
    "\n",
    "    # Tokenize the prompt (without special tokens) to know its length.\n",
    "    prompt_ids = tokenizer.apply_chat_template(\n",
    "        messages[:-1],\n",
    "        return_tensors='pt',\n",
    "        return_dict=True,\n",
    "        add_special_tokens=False\n",
    "    )[\"input_ids\"][0]  # Remove batch dimension\n",
    "\n",
    "    # Tokenize the full conversation (with special tokens and truncation)\n",
    "    tokenized = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "    input_ids = tokenized[\"input_ids\"][0]  # Remove batch dimension\n",
    "    attention_mask = tokenized[\"attention_mask\"][0]  # Remove batch dimension\n",
    "\n",
    "    # Create labels as a copy of input_ids.\n",
    "    labels = input_ids.clone()  \n",
    "    prompt_length = len(prompt_ids)\n",
    "    labels[:prompt_length] = torch.tensor([-100] * prompt_length)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}\n",
    "\n",
    "def tokenize_for_generation(example, tokenizer):\n",
    "    # Build a prompt with the system and user messages only, for generation (not for training)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example['problem']}\n",
    "    ]\n",
    "\n",
    "    # Tokenize the full conversation\n",
    "    tokenized = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        add_generation_prefix=True, # Doesn't seem to work with Qwen2.5-0.5B-Instruct\n",
    "    )\n",
    "\n",
    "    generation_prefix = '<|im_start|>assistant\\n'\n",
    "    generation_prefix_tokenized = tokenizer(generation_prefix, return_tensors='pt')[\"input_ids\"]\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"][0]  # Remove batch dimension\n",
    "    attention_mask = tokenized[\"attention_mask\"][0]\n",
    "\n",
    "    # Unsqueeze generation_prefix_tokenized to match dimensions\n",
    "    generation_prefix_tokenized = generation_prefix_tokenized.squeeze(0)\n",
    "\n",
    "    # Add the generation prefix to the input_ids\n",
    "    input_ids = torch.cat([input_ids, generation_prefix_tokenized], dim=0)\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones_like(generation_prefix_tokenized)], dim=0)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the formatting function over the dataset.\n",
    "# This applies the formatting function to each example in the dataset.\n",
    "# The result is that we have a dataset where each math problem is formatted as a prompt for the model,\n",
    "# and the solution is formatted as a response that the model should generate.\n",
    "# Each example is also tokenized\n",
    "# (If your dataset is large you might use batched=True; here we keep it simple.)\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "eval_dataset_tokenized = eval_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "# Get a sample dataset so we can examine model generations before and after training\n",
    "sample_dataset = eval_dataset.select(range(3))\n",
    "sample_dataset_tokenized = sample_dataset.map(tokenize_for_generation, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "train_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "eval_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "sample_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer):\n",
    "    outputs = []\n",
    "    for sample in sample_dataset_tokenized:\n",
    "        input_ids_batch = sample['input_ids'].unsqueeze(0).to(model.device)\n",
    "        attention_mask_batch = sample['attention_mask'].unsqueeze(0).to(model.device)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids_batch,\n",
    "            attention_mask=attention_mask_batch,\n",
    "            max_new_tokens=512,  # change as needed\n",
    "        )\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        outputs.append(generated_text)\n",
    "\n",
    "    for i, sample in enumerate(sample_dataset_tokenized):\n",
    "        # Retrieve the original problem and solution from the un-tokenized dataset\n",
    "        original = sample_dataset[i]\n",
    "        print(\"Problem:\")\n",
    "        print(wrapper.fill(original[\"problem\"]))\n",
    "        print(\"\\nTrue Solution:\")\n",
    "        print(wrapper.fill(original[\"solution\"]))\n",
    "        print(\"\\nModel's Solution:\")\n",
    "        model_output = outputs[i].split(\"assistant\\n\")[-1].strip()\n",
    "        print(wrapper.fill(model_output))\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Generate and print model outputs before training\n",
    "generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple data collator\n",
    "def data_collator(features):\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [f[\"input_ids\"].clone().detach() for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        [f[\"labels\"].clone().detach() for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=-100,\n",
    "    )\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        [f[\"attention_mask\"].clone().detach() for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=0,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLMsFT2)",
   "language": "python",
   "name": "llmsft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
