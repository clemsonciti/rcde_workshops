{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "# Split the dataset into training and validation sets\n",
    "train_val_dataset = ds[\"test\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_dataset[\"train\"]\n",
    "eval_dataset = train_val_dataset[\"test\"]\n",
    "\n",
    " # This example only has a test split, so we use that, for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>solution</th>\n",
       "      <th>answer</th>\n",
       "      <th>subject</th>\n",
       "      <th>level</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Determine if the graph of the equation below i...</td>\n",
       "      <td>This looks like the equation of a circle, but ...</td>\n",
       "      <td>\\text{ellipse}</td>\n",
       "      <td>Intermediate Algebra</td>\n",
       "      <td>2</td>\n",
       "      <td>test/intermediate_algebra/860.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A class of 30 students recently took a test.  ...</td>\n",
       "      <td>From the given information, the total amount o...</td>\n",
       "      <td>84</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>3</td>\n",
       "      <td>test/prealgebra/846.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Compute $\\arcsin \\left( -\\frac{1}{2} \\right).$...</td>\n",
       "      <td>Since $\\sin \\left( -\\frac{\\pi}{6} \\right) = -\\...</td>\n",
       "      <td>-\\frac{\\pi}{6}</td>\n",
       "      <td>Precalculus</td>\n",
       "      <td>2</td>\n",
       "      <td>test/precalculus/1105.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What's the largest eight-digit base 2 integer?...</td>\n",
       "      <td>The largest eight-digit base 2 integer is 1 le...</td>\n",
       "      <td>255</td>\n",
       "      <td>Number Theory</td>\n",
       "      <td>3</td>\n",
       "      <td>test/number_theory/691.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Add 313.9 to 12.6. Express the result as a dec...</td>\n",
       "      <td>We have \\[\\n\\begin{array}{@{}c@{}c@{}c@{}c@{}c...</td>\n",
       "      <td>326.5</td>\n",
       "      <td>Prealgebra</td>\n",
       "      <td>2</td>\n",
       "      <td>test/prealgebra/1784.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             problem  \\\n",
       "0  Determine if the graph of the equation below i...   \n",
       "1  A class of 30 students recently took a test.  ...   \n",
       "2  Compute $\\arcsin \\left( -\\frac{1}{2} \\right).$...   \n",
       "3  What's the largest eight-digit base 2 integer?...   \n",
       "4  Add 313.9 to 12.6. Express the result as a dec...   \n",
       "\n",
       "                                            solution          answer  \\\n",
       "0  This looks like the equation of a circle, but ...  \\text{ellipse}   \n",
       "1  From the given information, the total amount o...              84   \n",
       "2  Since $\\sin \\left( -\\frac{\\pi}{6} \\right) = -\\...  -\\frac{\\pi}{6}   \n",
       "3  The largest eight-digit base 2 integer is 1 le...             255   \n",
       "4  We have \\[\\n\\begin{array}{@{}c@{}c@{}c@{}c@{}c...           326.5   \n",
       "\n",
       "                subject  level                           unique_id  \n",
       "0  Intermediate Algebra      2  test/intermediate_algebra/860.json  \n",
       "1            Prealgebra      3            test/prealgebra/846.json  \n",
       "2           Precalculus      2          test/precalculus/1105.json  \n",
       "3         Number Theory      3         test/number_theory/691.json  \n",
       "4            Prealgebra      2           test/prealgebra/1784.json  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(train_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      " Evaluate \\[\\sin (\\arcsin 0.4 + \\arcsin 0.5) \\cdot \\sin (\\arcsin 0.5 - \\arcsin\n",
      "0.4).\\] \n",
      "\n",
      "\n",
      "Solution:\n",
      " From the angle addition and subtraction formulas, \\begin{align*} \\sin (x + y) &=\n",
      "\\sin x \\cos y + \\cos x \\sin y, \\\\ \\sin (x - y) &= \\sin x \\cos y - \\cos x \\sin y,\n",
      "\\end{align*}so \\begin{align*} \\sin (x + y) \\sin (x - y) &= (\\sin x \\cos y + \\cos\n",
      "x \\sin y)(\\sin x \\cos y - \\cos x \\sin y) \\\\ &= \\sin^2 x \\cos^2 y + \\sin x \\cos x\n",
      "\\sin y \\cos y - \\sin x \\cos x \\sin y \\cos y - \\cos^2 x \\sin^2 y \\\\ &= \\sin^2 x\n",
      "(1 - \\sin^2 y) - (1 - \\sin^2 x) \\sin^2 y \\\\ &= \\sin^2 x - \\sin^2 x \\sin^2 y -\n",
      "\\sin^2 y + \\sin^2 x \\sin^2 y \\\\ &= \\sin^2 x - \\sin^2 y. \\end{align*}Taking $x =\n",
      "\\arcsin 0.5$ and $y = \\arcsin 0.4,$ we get \\begin{align*} \\sin (\\arcsin 0.5 +\n",
      "\\arcsin 0.4) \\cdot \\sin (\\arcsin 0.5 - \\arcsin 0.4) &= \\sin^2 (\\arcsin 0.5) -\n",
      "\\sin^2 (\\arcsin 0.4) \\\\ &= 0.5^2 - 0.4^2 \\\\ &= 0.09 = \\boxed{\\frac{9}{100}}.\n",
      "\\end{align*}\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import random\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "sample = train_dataset[random.randint(0, len(train_dataset) - 1)]\n",
    "\n",
    "problem = wrapper.fill(sample['problem'])\n",
    "solution = wrapper.fill(sample['solution'])\n",
    "\n",
    "print('Problem:\\n', problem, '\\n\\n')\n",
    "print('Solution:\\n', solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# The model may not have a pad token set by default, so set it (using the EOS token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a system prompt to be prepended to the user prompt.\n",
    "# This is a simple example, but you could use a more complex system prompt.\n",
    "system_prompt = \"Solve the following math problem.\"\n",
    "\n",
    "# Define a function to format the prompt and apply loss masking.\n",
    "# This function builds a full text with a \"User:\" prompt and an \"Assistant:\" response.\n",
    "# It then computes which tokens belong to the prompt (to be masked in the loss)\n",
    "# The function assumes that each example has a `problem` and a `solution`, which is true for the MATH-500 dataset.\n",
    "def tokenize_and_mask(example, tokenizer, max_length=1024, system_prompt=system_prompt):\n",
    "    # Build a prompt with the system, user, and assistant messages.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example['problem']},\n",
    "        {\"role\": \"assistant\", \"content\": example['solution']}\n",
    "    ]\n",
    "\n",
    "    # Tokenize the prompt (without special tokens) to know its length.\n",
    "    prompt_ids = tokenizer.apply_chat_template(\n",
    "        messages[:-1],\n",
    "        return_tensors='pt',\n",
    "        return_dict=True,\n",
    "        add_special_tokens=False\n",
    "    )[\"input_ids\"][0]  # Remove batch dimension\n",
    "\n",
    "    # Tokenize the full conversation (with special tokens and truncation)\n",
    "    tokenized = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "    input_ids = tokenized[\"input_ids\"][0]  # Remove batch dimension\n",
    "    attention_mask = tokenized[\"attention_mask\"][0]  # Remove batch dimension\n",
    "\n",
    "    # Create labels as a copy of input_ids.\n",
    "    labels = input_ids.clone()  \n",
    "    prompt_length = len(prompt_ids)\n",
    "    labels[:prompt_length] = torch.tensor([-100] * prompt_length)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels, \"attention_mask\": attention_mask}\n",
    "\n",
    "def tokenize_for_generation(example, tokenizer):\n",
    "    # Build a prompt with the system and user messages only, for generation (not for training)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example['problem']}\n",
    "    ]\n",
    "\n",
    "    # Tokenize the full conversation\n",
    "    tokenized = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        add_generation_prefix=True, # Doesn't seem to work with Qwen2.5-0.5B-Instruct\n",
    "    )\n",
    "\n",
    "    generation_prefix = '<|im_start|>assistant\\n'\n",
    "    generation_prefix_tokenized = tokenizer(generation_prefix, return_tensors='pt')[\"input_ids\"]\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"][0]  # Remove batch dimension\n",
    "    attention_mask = tokenized[\"attention_mask\"][0]\n",
    "\n",
    "    # Unsqueeze generation_prefix_tokenized to match dimensions\n",
    "    generation_prefix_tokenized = generation_prefix_tokenized.squeeze(0)\n",
    "\n",
    "    # Add the generation prefix to the input_ids\n",
    "    input_ids = torch.cat([input_ids, generation_prefix_tokenized], dim=0)\n",
    "    attention_mask = torch.cat([attention_mask, torch.ones_like(generation_prefix_tokenized)], dim=0)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5c6e830cc1457a8a1d01892e921fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6491dcbc7a634dc48f785d050ec42afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b6946b1c2491cbf440e1dfda7e3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the formatting function over the dataset.\n",
    "# This applies the formatting function to each example in the dataset.\n",
    "# The result is that we have a dataset where each math problem is formatted as a prompt for the model,\n",
    "# and the solution is formatted as a response that the model should generate.\n",
    "# Each example is also tokenized\n",
    "# (If your dataset is large you might use batched=True; here we keep it simple.)\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "eval_dataset_tokenized = eval_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "# Get a sample dataset so we can examine model generations before and after training\n",
    "sample_dataset = eval_dataset.select(range(3))\n",
    "sample_dataset_tokenized = sample_dataset.map(tokenize_for_generation, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "train_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "eval_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "sample_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "Find the remainder when $(5x + 9)^{611} + (x + 5)^{11} + (x - 1)^{11} + 3x^2 +\n",
      "1$ is divided by $x + 2.$\n",
      "\n",
      "True Solution:\n",
      "By the Remainder Theorem, to find the remainder, we set $x = -2.$  This gives us\n",
      "\\[(-1)^{611} + 3^{11} + (-3)^{11} + 3(-2)^2 + 1 = \\boxed{12}.\\]\n",
      "\n",
      "Model's Solution:\n",
      "To solve the problem of finding the remainder when \\((5x + 9)^{611} + (x +\n",
      "5)^{11} + (x - 1)^{11} + 3x^2 + 1\\) is divided by \\(x + 2\\), we can use\n",
      "polynomial long division or the Remainder Theorem. However, a more efficient\n",
      "approach involves recognizing that the given expression can be simplified using\n",
      "the fact that \\(x = -2\\) is a root of the divisor.  First, let's substitute \\(x\n",
      "= -2\\) into the expression:  \\[ (5(-2) + 9)^{611} + ((-2) + 5)^{11} + ((-2) -\n",
      "1)^{11} + 3(-2)^2 + 1 \\]  Calculating each term separately:  1. \\((5(-2) +\n",
      "9)^{611} = (-10 + 9)^{611} = (-1)^{611} = -1\\) 2. \\((-2 + 5)^{11} = (3)^{11}\\)\n",
      "3. \\((( -2 ) - 1)^{11} = (-3)^{11}\\) 4. \\(3(-2)^2 = 3 \\cdot 4 = 12\\)  So,\n",
      "substituting these values back in, we get:  \\[ -1 + (3^{11}) + (-3^{11}) + 12 +\n",
      "1 = 12 - 1 - 3^{11} \\]  Since \\(3^{11}\\) is much larger than 1, we have:  \\[ 12\n",
      "- 1 - 3^{11} = 11 - 3^{11} \\]  Now, we need to find the remainder when \\(11 -\n",
      "3^{11}\\) is divided by \\(x + 2\\). According to the Remainder Theorem, if \\(x =\n",
      "-2\\) is a root of the divisor, then the remainder must be equal to the value of\n",
      "the polynomial at \\(x = -2\\):  \\[ 11 - 3^{11} \\equiv 0 \\pmod{x+2} \\]  Thus, the\n",
      "remainder when \\((5x + 9)^{611} + (x +\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "A regular pentagon is rotated counterclockwise about its center. What is the\n",
      "minimum number of degrees it must be rotated until it coincides with its\n",
      "original position?\n",
      "\n",
      "True Solution:\n",
      "Each of the five marked angles measures $360/5=72$ degrees, so $\\boxed{72}$\n",
      "degrees is the minimum angle through which the pentagon may be rotated so that\n",
      "it coincides with its original position.  [asy] size(150);\n",
      "defaultpen(linewidth(0.7)); int i; for(i=0;i<=4;++i)  { draw(origin--\n",
      "dir(18+72*i)--dir(18+72*(i+1)));\n",
      "draw(anglemark(dir(18+72*i),origin,dir(18+72*(i+1)),3+fmod(i,3))); } [/asy]\n",
      "\n",
      "Model's Solution:\n",
      "To determine the minimum number of degrees a regular pentagon must be rotated to\n",
      "coincide with its original position, we need to understand that a rotation by\n",
      "\\(360^\\circ\\) (a full circle) will bring the pentagon back to its original\n",
      "position.  A regular pentagon has five equal sides and angles. When a regular\n",
      "pentagon is rotated, each vertex will return to its original position after\n",
      "exactly one full rotation around its center. This means that rotating the\n",
      "pentagon by \\(360^\\circ\\) brings it back to its starting point.  Therefore, the\n",
      "minimum number of degrees the pentagon must be rotated before it coincides with\n",
      "its original position is \\(\\boxed{360}\\).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "A number is selected at random from 1 through 100, inclusive.  What is the\n",
      "probability that the number is a multiple of 3?\n",
      "\n",
      "True Solution:\n",
      "There are 100 numbers possible between 1 and 100. There are 33 multiples of 3\n",
      "between 1 and 100: $(3,6,9,\\ldots,99)=(1\\times 3,2\\times 3,3\\times\n",
      "3,\\ldots,33\\times 3)$.  So the probability that a randomly selected number is a\n",
      "multiple of 3 is $\\boxed{\\dfrac{33}{100}}$.\n",
      "\n",
      "Model's Solution:\n",
      "To determine the probability that a randomly selected number from 1 to 100 is a\n",
      "multiple of 3, we can follow these steps:  1. **Identify the total number of\n",
      "possible outcomes:**    The numbers from 1 to 100 are all the integers in this\n",
      "range. Therefore, there are \\(100\\) possible outcomes.  2. **Determine how many\n",
      "numbers are multiples of 3:**    A number is a multiple of 3 if it can be\n",
      "written as \\(3 \\times k\\) where \\(k\\) is an integer. We need to find all such\n",
      "numbers between 1 and 100.        To do this, we divide 100 by 3:    \\[    100\n",
      "\\div 3 = 33.33\\ldots    \\]    Since 33.33 cannot be an integer, we round up to\n",
      "the next whole number, which is 34. This means there are \\(34\\) multiples of 3\n",
      "within the range from 1 to 100.  3. **Calculate the probability:**    The\n",
      "probability \\(P\\) of selecting a number that is a multiple of 3 is the ratio of\n",
      "the number of favorable outcomes (multiples of 3) to the total number of\n",
      "possible outcomes (\\(100\\)).        \\[    P(\\text{multiple of 3}) =\n",
      "\\frac{\\text{Number of multiples of 3}}{\\text{Total number of integers from 1 to\n",
      "100}}    \\]     Substituting the values we found:    \\[    P(\\text{multiple of\n",
      "3}) = \\frac{34}{100}    \\]  4. **Simplify the fraction:**    \\[\n",
      "\\frac{34}{100} = \\frac{17}{50}    \\]  Therefore, the probability that a randomly\n",
      "selected number from 1 to 100 is a multiple of 3 is \\(\\boxed{\\frac{17}{50}}\\).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer):\n",
    "    outputs = []\n",
    "    for sample in sample_dataset_tokenized:\n",
    "        input_ids_batch = sample['input_ids'].unsqueeze(0).to(model.device)\n",
    "        attention_mask_batch = sample['attention_mask'].unsqueeze(0).to(model.device)\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids_batch,\n",
    "            attention_mask=attention_mask_batch,\n",
    "            max_new_tokens=512,  # change as needed\n",
    "        )\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        outputs.append(generated_text)\n",
    "\n",
    "    for i, sample in enumerate(sample_dataset_tokenized):\n",
    "        # Retrieve the original problem and solution from the un-tokenized dataset\n",
    "        original = sample_dataset[i]\n",
    "        print(\"Problem:\")\n",
    "        print(wrapper.fill(original[\"problem\"]))\n",
    "        print(\"\\nTrue Solution:\")\n",
    "        print(wrapper.fill(original[\"solution\"]))\n",
    "        print(\"\\nModel's Solution:\")\n",
    "        model_output = outputs[i].split(\"assistant\\n\")[-1].strip()\n",
    "        print(wrapper.fill(model_output))\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n",
    "# Generate and print model outputs before training\n",
    "generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple data collator\n",
    "def data_collator(features):\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [f[\"input_ids\"].clone().detach() for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        [f[\"labels\"].clone().detach() for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=-100,\n",
    "    )\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        [f[\"attention_mask\"].clone().detach() for f in features],\n",
    "        batch_first=True,\n",
    "        padding_value=0,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
