{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "# Split the dataset into training and validation sets\n",
    "train_val_dataset = ds[\"test\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_dataset[\"train\"]\n",
    "eval_dataset = train_val_dataset[\"test\"]\n",
    "\n",
    " # This example only has a test split, so we use that, for demonstration purposes.\n",
    "\n",
    " # Load the model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# The model may not have a pad token set by default, so set it (using the EOS token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from workshop_utils import tokenize_and_mask, tokenize_for_generation, generate_and_print, data_collator\n",
    "\n",
    "data_collator_fn = lambda features: data_collator(features, tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda1ed04b3464ea6b715695d664efed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bd4729118640efa1ecb06f401d8917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501e595b1e0d4106a60e8312d5f2c003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the formatting function over the dataset.\n",
    "# This applies the formatting function to each example in the dataset.\n",
    "# The result is that we have a dataset where each math problem is formatted as a prompt for the model,\n",
    "# and the solution is formatted as a response that the model should generate.\n",
    "# Each example is also tokenized\n",
    "# (If your dataset is large you might use batched=True; here we keep it simple.)\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "eval_dataset_tokenized = eval_dataset.map(tokenize_and_mask, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "# Get a sample dataset so we can examine model generations before and after training\n",
    "sample_dataset = eval_dataset.select(range(3))\n",
    "sample_dataset_tokenized = sample_dataset.map(tokenize_for_generation, batched=False, fn_kwargs={\"tokenizer\": tokenizer})\n",
    "\n",
    "train_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "eval_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\", \"attention_mask\"])\n",
    "sample_dataset_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 494,032,768 || all params: 494,032,768 || trainable%: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        all_params += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    print(f\"trainable params: {trainable_params:,d} || all params: {all_params:,d} || trainable%: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Define LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # Replace with the target modules of your model\n",
    ")\n",
    "\n",
    "# Add LoRA adapter to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-28 14:20:58,190] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/cehrett/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: warning: libm.so.6, needed by /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so, not found (try using -rpath or -rpath-link)\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `log2f@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/libstdc++.so.6: undefined reference to `fesetround@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/libstdc++.so.6: undefined reference to `fegetround@GLIBC_2.2.5'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `shm_open'\n",
      "/home/cehrett/.conda/envs/LLMsFT/compiler_compat/ld: /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.3.0/cuda-12.3.0-p2hoh7xwcu52zilqglv3nnc5bwnritue/lib64/libcufile.so: undefined reference to `shm_unlink'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 00:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.748200</td>\n",
       "      <td>0.686028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>0.691287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.706000</td>\n",
       "      <td>0.672091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.755900</td>\n",
       "      <td>0.704082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.680891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.701159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.775300</td>\n",
       "      <td>0.698023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.691800</td>\n",
       "      <td>0.678332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=0.7346053506895812, metrics={'train_runtime': 36.2842, 'train_samples_per_second': 37.206, 'train_steps_per_second': 4.713, 'total_flos': 996680617810944.0, 'train_loss': 0.7346053506895812, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-lora-math\",          # Output directory\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size per device during training\n",
    "    gradient_accumulation_steps=1,   # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    optim=\"paged_adamw_32bit\",        # Optimizer, you might need to install accelerate: pip install accelerate -U\n",
    "    save_steps=20,                   # Save checkpoint every X updates steps\n",
    "    eval_steps=20,                   # Evaluate every X updates steps\n",
    "    eval_strategy=\"steps\",           # Evaluation strategy\n",
    "    save_total_limit=1,              # Limit the total amount of checkpoints\n",
    "    load_best_model_at_end=True,     # Load the best model when finished training (default is True)\n",
    "    logging_steps=10,                # Log every X updates steps\n",
    "    learning_rate=1e-4,              # Learning rate\n",
    "    weight_decay=0.001,              # Weight decay\n",
    "    fp16=True,                       # Use mixed precision training\n",
    "    bf16=False,                      # Use bfloat16 training\n",
    "    max_grad_norm=0.3,               # Gradient clipping max norm\n",
    "    max_steps=-1,                    # If > 0: set total number of training steps to perform. Override num_train_epochs.\n",
    "    warmup_ratio=0.03,               # Linear warmup over warmup_ratio fraction of the total number of training steps.\n",
    "    group_by_length=True,            # Group sequences of roughly the same length together for more efficient training\n",
    "    lr_scheduler_type=\"cosine\",       # Learning rate scheduler type\n",
    "    report_to=\"none\",                # Disable logging\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=eval_dataset_tokenized,\n",
    "    data_collator=data_collator_fn,     # Data collator if needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen-lora-math-final/tokenizer_config.json',\n",
       " './qwen-lora-math-final/special_tokens_map.json',\n",
       " './qwen-lora-math-final/vocab.json',\n",
       " './qwen-lora-math-final/merges.txt',\n",
       " './qwen-lora-math-final/added_tokens.json',\n",
       " './qwen-lora-math-final/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the LORA model\n",
    "output_dir = \"./qwen-lora-math-final\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the LORA model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "Consider the function $z(x,y)$ describing the paraboloid \\[z = (2x - y)^2 - 2y^2\n",
      "- 3y.\\]Archimedes and Brahmagupta are playing a game.  Archimedes first chooses\n",
      "$x.$  Afterwards, Brahmagupta chooses $y.$  Archimedes wishes to minimize $z$\n",
      "while Brahmagupta wishes to maximize $z.$  Assuming that Brahmagupta will play\n",
      "optimally, what value of $x$ should Archimedes choose?\n",
      "\n",
      "True Solution:\n",
      "Expanding $z,$ we get \\begin{align*} z &= 4x^2 - 4xy + y^2 - 2y^2 - 3y \\\\ &=\n",
      "-y^2 - (4x + 3) y + 4x^2. \\end{align*}After Archimedes chooses $x,$ Brahmagupta\n",
      "will choose \\[y = -\\frac{4x + 3}{2}\\]in order to maximize $z.$  Then\n",
      "\\begin{align*} z &= -\\left( -\\frac{4x + 3}{2} \\right)^2 - (4x + 3) \\left(\n",
      "-\\frac{4x + 3}{2} \\right)^2 + 4x^2 \\\\ &= 8x^2 + 6x + \\frac{9}{4}. \\end{align*}To\n",
      "minimize this expression, Archimedes should choose $x = -\\frac{6}{16} =\n",
      "\\boxed{-\\frac{3}{8}}.$\n",
      "\n",
      "Model's Solution:\n",
      "We want to minimize or maximize the function $f(x) = z(x,y) = (2x-y)^2 -\n",
      "2y^2-3y$.  The derivative is  \\[\\frac{df}{dx} = 4(2x-y)(-1) + (-4y)-6y =\n",
      "-8x+4y-6y = -8x-2y.\\]To find the critical points, we set $\\frac{df}{dx}=0$,\n",
      "which gives us $-8x=2y$ or $x=-\\frac{1}{4}y$.  Since this is the only critical\n",
      "point, it must be an extremum.  If we plug in values for $x$ and $y$ into the\n",
      "original equation, we get \\[z(-\\frac{1}{4},y)=\\left(2(-\\frac{1}{4})-y\\right)^2 -\n",
      "2y^2-3y = \\frac{1}{8}-2y^2-2y=y(\\frac{-1}{4}-2y).\\]Since the derivative is\n",
      "negative when $y<-\\frac{1}{4}$, this means that when $y=\\boxed{\\frac{1}{4}}$,\n",
      "the minimum value of $z$ occurs at $x=-\\frac{1}{4}\\cdot \\frac{1}{4} =\n",
      "\\boxed{\\frac{1}{16}}$.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "Find the quotient when $x^6 - 3$ is divided by $x + 1.$\n",
      "\n",
      "True Solution:\n",
      "We can perform long division.  Alternatively, by the Remainder Theorem, the\n",
      "remainder upon division is $(-1)^6 - 3 = -2.$  Thus, we can write \\begin{align*}\n",
      "\\frac{x^6 - 3}{x + 1} &= \\frac{(x^6 - 1) - 2}{x + 1} \\\\ &= \\frac{x^6 - 1}{x + 1}\n",
      "- \\frac{2}{x + 1} \\\\ &= \\frac{(x^3 - 1)(x^3 + 1)}{x + 1} - \\frac{2}{x + 1} \\\\ &=\n",
      "\\frac{(x^3 - 1)(x + 1)(x^2 - x + 1)}{x + 1} - \\frac{2}{x + 1} \\\\ &= (x^3 -\n",
      "1)(x^2 - x + 1) - \\frac{2}{x + 1} \\\\ &= x^5 - x^4 + x^3 - x^2 + x - 1 -\n",
      "\\frac{2}{x + 1}. \\end{align*}Thus, the quotient is $\\boxed{x^5 - x^4 + x^3 - x^2\n",
      "+ x - 1}.$\n",
      "\n",
      "Model's Solution:\n",
      "We can divide out a factor of $(x+1)$ from both sides, so we have\n",
      "\\[\\frac{x^6-3}{x+1} = x^5 - 2.\\]Then we subtract 2 times the numerator from the\n",
      "denominator to get $\\frac{(x^6-3) - (2(x^5-2))}{x+1} = \\frac{x^6-3-2x^5+4}{x+1}\n",
      "= \\frac{-2x^5+x^6+1}{x+1} = \\frac{-(2x^5+x^6)+1}{x+1} =\n",
      "\\frac{-(x^6+2x^5)+1}{x+1} = \\frac{-(x^5+2x^4)+1}{x+1} =\n",
      "\\frac{-(x^4)(x+2)+1}{x+1}.\\]Therefore, the quotient is\n",
      "$\\boxed{\\frac{x^4}{x+2}}$.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "Let $a,$ $b,$ $c,$ $d$ be positive real numbers.  Find the minimum value of \\[(a\n",
      "+ b + c + d) \\left( \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\n",
      "\\right).\\]\n",
      "\n",
      "True Solution:\n",
      "By AM-GM, \\[a + b + c + d \\ge 4 \\sqrt[4]{abcd},\\]and \\[\\frac{1}{a} + \\frac{1}{b}\n",
      "+ \\frac{1}{c} + \\frac{1}{d} \\ge 4 \\sqrt[4]{\\frac{1}{abcd}},\\]so \\[(a + b + c +\n",
      "d) \\left( \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d} \\right) \\ge 4\n",
      "\\sqrt[4]{abcd} \\cdot 4 \\sqrt[4]{\\frac{1}{abcd}} = 16.\\]Equality occurs when $a =\n",
      "b = c = d,$ so the minimum value is $\\boxed{16}.$\n",
      "\n",
      "Model's Solution:\n",
      "By AM-GM, \\[ \\frac{(a + b + c + d)^2}{abcd} \\geq (a + b + c +\n",
      "d)^{\\frac{1}{4}}.\\] Therefore, $\\frac{1}{abcd} \\geq \\frac{1}{4}$ and so $(a + b\n",
      "+ c + d)^3 \\geq abcd.$  Now, let's apply this to our original expression.  We\n",
      "have \\begin{align*} (a + b + c + d) \\left( \\frac{1}{a} + \\frac{1}{b} +\n",
      "\\frac{1}{c} + \\frac{1}{d} \\right) &= a^2 + b^2 + c^2 + d^2 + ab + ac + ad + bc +\n",
      "bd + cd - abc - abd - acd - bcd \\\\ &= a^2 + b^2 + c^2 + d^2 + ab + ac + ad + bc\n",
      "+ bd + cd - (abc + abd + acd + bcd) \\\\ &= \\sum_{cyc} (a + b)(a + c) - (ab + ac +\n",
      "ad + bc + bd + cd). \\end{align*}Thus, we need to minimize $\\sum_{cyc} (a + b)(a\n",
      "+ c)$.  Note that by AM-GM, \\[ \\frac{a + b}{2} \\geq \\sqrt{ab}, \\quad \\text{so}\n",
      "\\quad \\frac{a + b}{2} \\leq \\sqrt{a^2 + b^2}. \\]  Similarly, \\[ \\frac{a + c}{2}\n",
      "\\leq \\sqrt{a^2 + c^2}, \\quad \\text{so} \\quad \\frac{a + c}{2} \\leq \\sqrt{a^2 +\n",
      "b^2 + c^2}. \\]  Multiplying these inequalities together gives us \\[ \\frac{a +\n",
      "b}{2} \\cdot \\frac{a + c}{2} \\leq (\\sqrt{a^2 + b^2})^2 = a^2 + b^2,\\] and\n",
      "similarly, \\[ \\frac{a + b}{2} \\cdot \\frac{a + c}{2} \\leq (\\sqrt{a^2 + c^2})^2 =\n",
      "a^2 + c\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and print model outputs after training\n",
    "generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\")\n",
    "# Split the dataset into training and validation sets\n",
    "train_val_dataset = ds[\"test\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_dataset[\"train\"]\n",
    "eval_dataset = train_val_dataset[\"test\"]\n",
    "\n",
    " # This example only has a test split, so we use that, for demonstration purposes.\n",
    "\n",
    "# Configuration for bitsandbytes (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, # Or torch.bfloat16 if supported\n",
    ")\n",
    "\n",
    " # Load the model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# The model may not have a pad token set by default, so set it (using the EOS token)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "# Prepare Model for QLoRA\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # Adjust for your model\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 00:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.749300</td>\n",
       "      <td>0.724248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.739596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.732500</td>\n",
       "      <td>0.719729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.790800</td>\n",
       "      <td>0.754252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.739400</td>\n",
       "      <td>0.729048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.749899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.798700</td>\n",
       "      <td>0.746554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.721600</td>\n",
       "      <td>0.726491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "/home/cehrett/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=0.756615131919147, metrics={'train_runtime': 58.6451, 'train_samples_per_second': 23.02, 'train_steps_per_second': 2.916, 'total_flos': 996680617810944.0, 'train_loss': 0.756615131919147, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-qlora-math\",          # Output directory\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    per_device_train_batch_size=8,   # Batch size per device during training\n",
    "    gradient_accumulation_steps=1,   # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    optim=\"paged_adamw_32bit\",       # Optimizer, you might need to install accelerate: pip install accelerate -U\n",
    "    save_steps=20,                   # Save checkpoint every X updates steps\n",
    "    eval_steps=20,                   # Evaluate every X updates steps\n",
    "    eval_strategy=\"steps\",           # Evaluation strategy\n",
    "    save_total_limit=1,              # Limit the total amount of checkpoints\n",
    "    load_best_model_at_end=True,     # Load the best model when finished training (default is True)\n",
    "    logging_steps=10,                # Log every X updates steps\n",
    "    learning_rate=1e-4,              # Learning rate\n",
    "    weight_decay=0.001,              # Weight decay\n",
    "    fp16=True,                       # Use mixed precision training\n",
    "    bf16=False,                      # Use bfloat16 training\n",
    "    max_grad_norm=0.3,               # Gradient clipping max norm\n",
    "    max_steps=-1,                    # If > 0: set total number of training steps to perform. Override num_train_epochs.\n",
    "    warmup_ratio=0.03,               # Linear warmup over warmup_ratio fraction of the total number of training steps.\n",
    "    group_by_length=True,            # Group sequences of roughly the same length together for more efficient training\n",
    "    lr_scheduler_type=\"cosine\",      # Learning rate scheduler type\n",
    "    report_to=\"none\",                # Disable logging\n",
    ")\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=eval_dataset_tokenized,\n",
    "    data_collator=data_collator_fn,     # Data collator if needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen-qlora-math-final/tokenizer_config.json',\n",
       " './qwen-qlora-math-final/special_tokens_map.json',\n",
       " './qwen-qlora-math-final/vocab.json',\n",
       " './qwen-qlora-math-final/merges.txt',\n",
       " './qwen-qlora-math-final/added_tokens.json',\n",
       " './qwen-qlora-math-final/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the QLoRA model\n",
    "output_dir = \"./qwen-qlora-math-final\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the QLoRA model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", device_map=\"auto\", quantization_config=bnb_config)\n",
    "model = PeftModel.from_pretrained(model, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "Consider the function $z(x,y)$ describing the paraboloid \\[z = (2x - y)^2 - 2y^2\n",
      "- 3y.\\]Archimedes and Brahmagupta are playing a game.  Archimedes first chooses\n",
      "$x.$  Afterwards, Brahmagupta chooses $y.$  Archimedes wishes to minimize $z$\n",
      "while Brahmagupta wishes to maximize $z.$  Assuming that Brahmagupta will play\n",
      "optimally, what value of $x$ should Archimedes choose?\n",
      "\n",
      "True Solution:\n",
      "Expanding $z,$ we get \\begin{align*} z &= 4x^2 - 4xy + y^2 - 2y^2 - 3y \\\\ &=\n",
      "-y^2 - (4x + 3) y + 4x^2. \\end{align*}After Archimedes chooses $x,$ Brahmagupta\n",
      "will choose \\[y = -\\frac{4x + 3}{2}\\]in order to maximize $z.$  Then\n",
      "\\begin{align*} z &= -\\left( -\\frac{4x + 3}{2} \\right)^2 - (4x + 3) \\left(\n",
      "-\\frac{4x + 3}{2} \\right)^2 + 4x^2 \\\\ &= 8x^2 + 6x + \\frac{9}{4}. \\end{align*}To\n",
      "minimize this expression, Archimedes should choose $x = -\\frac{6}{16} =\n",
      "\\boxed{-\\frac{3}{8}}.$\n",
      "\n",
      "Model's Solution:\n",
      "The minimum value of $z$ occurs when $y$ is at its maximum.  To find this, we\n",
      "take the partial derivative with respect to $y$: \\[\\frac{\\partial z}{\\partial y}\n",
      "= \\frac{4(2x - y)}{2}\\cdot(-1) = -4(2x - y).\\]Setting $\\displaystyle\n",
      "\\frac{\\partial z}{\\partial y} = 0$, we have $-4(2x - y) = 0$.  Since $y > 0,$\n",
      "this implies $y = 2x.$  Next, we find the maximum value of $z$ in terms of $x$.\n",
      "We can do this by taking the partial derivative with respect to $x$:\n",
      "\\[\\frac{\\partial z}{\\partial x} = \\frac{d(2x - y)^2}{dx} = 2(2x - y)\\cdot 2 =\n",
      "4(2x - y).\\]Taking the partial derivative with respect to $x$, we get $4(2x - y)\n",
      "= 4(2x - y)$.  We now compare the values of $z$ at $x = 0$ and $x = 2$:\n",
      "\\begin{align*} z(0) &= (2(0) - y)^2 - 2y^2 - 3y \\\\ &= y^2 - 2y^2 - 3y = -y^2 -\n",
      "3y \\\\ z(2) &= (2(2) - y)^2 - 2y^2 - 3y \\\\ &= 7 - y^2 - 3y. \\end{align*}  Since\n",
      "$z(0) < z(2),$ Archimedes should choose $x = 0.$\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "Find the quotient when $x^6 - 3$ is divided by $x + 1.$\n",
      "\n",
      "True Solution:\n",
      "We can perform long division.  Alternatively, by the Remainder Theorem, the\n",
      "remainder upon division is $(-1)^6 - 3 = -2.$  Thus, we can write \\begin{align*}\n",
      "\\frac{x^6 - 3}{x + 1} &= \\frac{(x^6 - 1) - 2}{x + 1} \\\\ &= \\frac{x^6 - 1}{x + 1}\n",
      "- \\frac{2}{x + 1} \\\\ &= \\frac{(x^3 - 1)(x^3 + 1)}{x + 1} - \\frac{2}{x + 1} \\\\ &=\n",
      "\\frac{(x^3 - 1)(x + 1)(x^2 - x + 1)}{x + 1} - \\frac{2}{x + 1} \\\\ &= (x^3 -\n",
      "1)(x^2 - x + 1) - \\frac{2}{x + 1} \\\\ &= x^5 - x^4 + x^3 - x^2 + x - 1 -\n",
      "\\frac{2}{x + 1}. \\end{align*}Thus, the quotient is $\\boxed{x^5 - x^4 + x^3 - x^2\n",
      "+ x - 1}.$\n",
      "\n",
      "Model's Solution:\n",
      "We can use long division to divide $x^6 - 3$ by $x + 1$. The quotient is\n",
      "$\\boxed{x^5 - x^4 + x^3 - x^2 + x - 1}.$\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "Let $a,$ $b,$ $c,$ $d$ be positive real numbers.  Find the minimum value of \\[(a\n",
      "+ b + c + d) \\left( \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\n",
      "\\right).\\]\n",
      "\n",
      "True Solution:\n",
      "By AM-GM, \\[a + b + c + d \\ge 4 \\sqrt[4]{abcd},\\]and \\[\\frac{1}{a} + \\frac{1}{b}\n",
      "+ \\frac{1}{c} + \\frac{1}{d} \\ge 4 \\sqrt[4]{\\frac{1}{abcd}},\\]so \\[(a + b + c +\n",
      "d) \\left( \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d} \\right) \\ge 4\n",
      "\\sqrt[4]{abcd} \\cdot 4 \\sqrt[4]{\\frac{1}{abcd}} = 16.\\]Equality occurs when $a =\n",
      "b = c = d,$ so the minimum value is $\\boxed{16}.$\n",
      "\n",
      "Model's Solution:\n",
      "We can write \\begin{align*} (a + b + c + d) \\left( \\frac{1}{a} + \\frac{1}{b} +\n",
      "\\frac{1}{c} + \\frac{1}{d} \\right) &= \\frac{(a+b+c+d)^2}{abcd} - abcd \\\\ &=\n",
      "\\frac{a^3 + b^3 + c^3 + d^3 + 6abc + 6abd + 6acd + 6bcd}{abcd}. \\end{align*}\n",
      "Since $a,$ $b,$ $c,$ and $d$ are all positive, we have $a^3 + b^3 + c^3 + d^3 >\n",
      "0.$  Thus, we want to minimize $\\frac{a^3 + b^3 + c^3 + d^3 + 6abc + 6abd + 6acd\n",
      "+ 6bcd}{abcd}.$  By AM-GM, we have \\[\\frac{a^3 + b^3 + c^3 + d^3}{abcd} =\n",
      "\\frac{a + b + c + d}{abcd}\\geq 6,\\] so the minimum value is achieved when $a = b\n",
      "= c = d = 1.$  Then $a + b + c + d = 4,$ so the minimum value is $\\boxed{24}.$\n",
      "This occurs when $a = b = c = d = 1.$\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate and print model outputs after training\n",
    "generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17,920 || all params: 494,050,688 || trainable%: 0.0036\n"
     ]
    }
   ],
   "source": [
    "from peft import PromptTuningConfig, get_peft_model\n",
    "\n",
    "# Define Prompt Tuning Config\n",
    "prompt_tuning_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    num_virtual_tokens=20, # Length of the prompt\n",
    "    prompt_tuning_init=\"TEXT\",\n",
    "    prompt_tuning_init_text=\"Solve the following math problem:\",\n",
    "    tokenizer_name_or_path=model_name,\n",
    ")\n",
    "\n",
    "# Add prompt tuning adapter to the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", device_map=\"auto\")\n",
    "model = get_peft_model(model, prompt_tuning_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cehrett/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.736900</td>\n",
       "      <td>0.689120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.697645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.717700</td>\n",
       "      <td>0.682443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.778800</td>\n",
       "      <td>0.713648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.687617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.668200</td>\n",
       "      <td>0.709285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.803600</td>\n",
       "      <td>0.705559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.719700</td>\n",
       "      <td>0.685136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=0.7510400850173325, metrics={'train_runtime': 33.8498, 'train_samples_per_second': 39.882, 'train_steps_per_second': 5.052, 'total_flos': 995177216596992.0, 'train_loss': 0.7510400850173325, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Arguments (example - adjust as needed)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-prompt-tuning-math\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=10,\n",
    "    learning_rate=5e-3,  # Smaller learning rate often helps\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_tokenized,\n",
    "    eval_dataset=eval_dataset_tokenized,\n",
    "    data_collator=data_collator_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./qwen-prompt-tuning-math-final/tokenizer_config.json',\n",
       " './qwen-prompt-tuning-math-final/special_tokens_map.json',\n",
       " './qwen-prompt-tuning-math-final/vocab.json',\n",
       " './qwen-prompt-tuning-math-final/merges.txt',\n",
       " './qwen-prompt-tuning-math-final/added_tokens.json',\n",
       " './qwen-prompt-tuning-math-final/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "output_dir = \"./qwen-prompt-tuning-math-final\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\", device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cehrett/.local/lib/python3.10/site-packages/peft/peft_model.py:1889: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem:\n",
      "Consider the function $z(x,y)$ describing the paraboloid \\[z = (2x - y)^2 - 2y^2\n",
      "- 3y.\\]Archimedes and Brahmagupta are playing a game.  Archimedes first chooses\n",
      "$x.$  Afterwards, Brahmagupta chooses $y.$  Archimedes wishes to minimize $z$\n",
      "while Brahmagupta wishes to maximize $z.$  Assuming that Brahmagupta will play\n",
      "optimally, what value of $x$ should Archimedes choose?\n",
      "\n",
      "True Solution:\n",
      "Expanding $z,$ we get \\begin{align*} z &= 4x^2 - 4xy + y^2 - 2y^2 - 3y \\\\ &=\n",
      "-y^2 - (4x + 3) y + 4x^2. \\end{align*}After Archimedes chooses $x,$ Brahmagupta\n",
      "will choose \\[y = -\\frac{4x + 3}{2}\\]in order to maximize $z.$  Then\n",
      "\\begin{align*} z &= -\\left( -\\frac{4x + 3}{2} \\right)^2 - (4x + 3) \\left(\n",
      "-\\frac{4x + 3}{2} \\right)^2 + 4x^2 \\\\ &= 8x^2 + 6x + \\frac{9}{4}. \\end{align*}To\n",
      "minimize this expression, Archimedes should choose $x = -\\frac{6}{16} =\n",
      "\\boxed{-\\frac{3}{8}}.$\n",
      "\n",
      "Model's Solution:\n",
      "The maximum value occurs when $x=1,$ so we want to find the minimum of the\n",
      "quadratic in $y.$  The vertex is at $(0,-\\frac{3}{2},-5),$ which corresponds to\n",
      "$y=\\boxed{-\\frac{3}{4}}.$.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "Find the quotient when $x^6 - 3$ is divided by $x + 1.$\n",
      "\n",
      "True Solution:\n",
      "We can perform long division.  Alternatively, by the Remainder Theorem, the\n",
      "remainder upon division is $(-1)^6 - 3 = -2.$  Thus, we can write \\begin{align*}\n",
      "\\frac{x^6 - 3}{x + 1} &= \\frac{(x^6 - 1) - 2}{x + 1} \\\\ &= \\frac{x^6 - 1}{x + 1}\n",
      "- \\frac{2}{x + 1} \\\\ &= \\frac{(x^3 - 1)(x^3 + 1)}{x + 1} - \\frac{2}{x + 1} \\\\ &=\n",
      "\\frac{(x^3 - 1)(x + 1)(x^2 - x + 1)}{x + 1} - \\frac{2}{x + 1} \\\\ &= (x^3 -\n",
      "1)(x^2 - x + 1) - \\frac{2}{x + 1} \\\\ &= x^5 - x^4 + x^3 - x^2 + x - 1 -\n",
      "\\frac{2}{x + 1}. \\end{align*}Thus, the quotient is $\\boxed{x^5 - x^4 + x^3 - x^2\n",
      "+ x - 1}.$\n",
      "\n",
      "Model's Solution:\n",
      "We can write $x^6 - 3 = (x+1)(x^5) + (x-2)(x^4)$, so we want to find $(x+1)(x^5)\n",
      "/ (x-2).$ Since $x+1$ divides both of these terms, it must divide their\n",
      "difference as well, and this gives us a remainder of $\\boxed{0}.$\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Problem:\n",
      "Let $a,$ $b,$ $c,$ $d$ be positive real numbers.  Find the minimum value of \\[(a\n",
      "+ b + c + d) \\left( \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}\n",
      "\\right).\\]\n",
      "\n",
      "True Solution:\n",
      "By AM-GM, \\[a + b + c + d \\ge 4 \\sqrt[4]{abcd},\\]and \\[\\frac{1}{a} + \\frac{1}{b}\n",
      "+ \\frac{1}{c} + \\frac{1}{d} \\ge 4 \\sqrt[4]{\\frac{1}{abcd}},\\]so \\[(a + b + c +\n",
      "d) \\left( \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d} \\right) \\ge 4\n",
      "\\sqrt[4]{abcd} \\cdot 4 \\sqrt[4]{\\frac{1}{abcd}} = 16.\\]Equality occurs when $a =\n",
      "b = c = d,$ so the minimum value is $\\boxed{16}.$\n",
      "\n",
      "Model's Solution:\n",
      "We have that $\\sum_{cyc}\n",
      "(a+b+c+d)(\\frac{1}{a}+\\frac{1}{b}+\\frac{1}{c}+\\frac{1}{d})=\\sum_{cyc}\n",
      "\\left(\\frac{ab+bc+ca+a^2}{abc}\\right)=\\sum_{cyc} \\frac{(a-b)^2}{abc}$, and since\n",
      "$(a-b)^2 \\geq 0$, we must have $\\sum_{cyc} \\frac{(a-b)^2}{abc} \\geq 0$.  This is\n",
      "true if and only if each of the denominators are nonnegative, which happens when\n",
      "$a=b=c=d$.  Hence, the minimum value is $\\boxed{\\frac{4}{3}}$.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate and print model outputs after training\n",
    "generate_and_print(sample_dataset, sample_dataset_tokenized, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLMsFT2)",
   "language": "python",
   "name": "llmsft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
