{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1abb179",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Small Language Models: an introduction to autoregressive language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f87f17",
   "metadata": {},
   "source": [
    "This notebook was partly inspired by this blog post on character-level bigram models: https://medium.com/@fareedkhandev/create-gpt-from-scratch-using-python-part-1-bd89ccf6206a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a3aea3-c7a4-448c-a337-0a82e81c503a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd250b0",
   "metadata": {},
   "source": [
    "## The language modeling task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5145c6d",
   "metadata": {},
   "source": [
    "### What is language modeling?\n",
    "\n",
    "In this notebook, we take a first look at the language modeling task. \"Language Modeling\" has two parts: \n",
    "* \"Language\" is what it sounds like. For our purposes, we will always _represent_ language with text. We will also talk about\n",
    "    * `tokens`: pieces of text. These could be words, word chunks, or individual characters.\n",
    "    * `documents`: a sequence of tokens about something. These could be individual tweets, legal contracts, love letters, emails, or journal abstracts.\n",
    "    * `dataset`: a collection of document. We will be using a PubMed dataset containing 50 thousand abstracts.\n",
    "    * `vocabulary`: the set of all unique tokens in our dataset.\n",
    "* \"Modeling\" refers to creating a mathematical structure that, in some way, corresponds to observed data. In this case, the data is language, so the model should quantiatively capture something about the nature of language. We need to make this more concrete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de0b66",
   "metadata": {},
   "source": [
    "\n",
    "### Language modeling as probabilistic modeling\n",
    "Let's try to make the idea of mathematically modeling language more concrete. We will develop models for the vector of tokens that appear in a document. We denote this as \n",
    "\n",
    "$$\n",
    "p(\\langle w_i\\rangle_{i=1}^{L})\n",
    "$$\n",
    "\n",
    "where $w_i$ is the token at position $i$ in a document and $L$ is the number of words in the document. The angle bracket with limits notation here denotes the vector of all tokens specified by the limits.\n",
    "\n",
    "If we knew this joint distribution, we could sample new documents $d$: \n",
    "\n",
    "$$\n",
    "d \\sim p(\\langle w_i\\rangle_{i=1}^{L})\n",
    "$$\n",
    "\n",
    "This is called _language generation_ because $d$ is not in the dataset that we used to learn $p(\\langle w_i\\rangle_{i=1}^{L})$, but it \"looks like\" it is from that dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb061375",
   "metadata": {},
   "source": [
    "\n",
    "### Autoregressive language modeling\n",
    "Let's make a simplifying assumption. Let's assume that the probability for token $i$ only depends on the previous tokens as shown in this figure (Notice: no arrows going from right to left.)\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/arlm.png?raw=true\" alt=\"autoregressive lm\" width=\"800\"/>\n",
    "\n",
    "Mathematically, this can be expressed as: \n",
    "\n",
    "$$\n",
    "p(w_i | \\langle w_j\\rangle_{j\\neq i}) = p(w_i | \\langle w_j\\rangle_{j=1}^{i-1})\n",
    "$$\n",
    "\n",
    "This gives us a natural way to sample documents because it implies that \n",
    "\n",
    "$$\n",
    "p(\\langle w_i\\rangle_{i=1}^{L}) = p(w_1)\\prod_{j=2}^L p(w_j | \\langle w_i\\rangle_{i=1}^{j-1})\n",
    "$$\n",
    "\n",
    "So, to generate a new document, we can \n",
    "1. start with a prompt token or token sequence \n",
    "2. sample the next token conditioned on the prompt and append it to the prompt\n",
    "3. sample the next token conditioned on the appended prompt and append it to the appended prompt\n",
    "4. and so on....\n",
    "\n",
    "This is how ChatGPT works! This approach goes by the names `autoregressive language modeling` or `causal language modeling`.\n",
    "\n",
    "This is not how all language modeling works. BERT, for instance, uses masked language modeling, where random tokens in a sequence are sampled by considering the tokens at all other positions. Word2Vec models tokens using a neighborhood of nearby tokens.\n",
    "\n",
    "Also, we still haven't said anything about how you actually write down the functional form of $p(w_i | \\langle w_j\\rangle_{j=1}^{i-1})$. There are many possibly architectures (an incomplete list in approximate historical ordering):\n",
    "* Markov model\n",
    "* 1D CNN\n",
    "* RNN\n",
    "* LSTM/GRU\n",
    "* Transformer\n",
    "\n",
    "We will spend the next notebook digging deep into the last option. Before we do, though, let's try to get a better understanding of language models by looking closely at a simple Markov model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8234a",
   "metadata": {},
   "source": [
    "## <strike>Large</strike> Small Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e4ce9",
   "metadata": {},
   "source": [
    "### The simplest, non-trivial model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399e411",
   "metadata": {},
   "source": [
    "\n",
    "Before we move on to attention, transformers, and LLMs, let's first write down and fit a very simple language model for the PubMed dataset. This will provide a baseline for more sophisticated techniques and will give us a better understanding of how autoregressive language modeling works. Most of the lessons learned will transfer directly to the LLM case. \n",
    "\n",
    "The simplest, non-trivial model comes from assuming that the distribution for token $i$ only depends on token $i-1$. Graphically:\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/arlm_markov.png?raw=true\" alt=\"autoregressive markov chain\" width=\"800\"/>\n",
    "\n",
    "With this Markov assumption, the conditional distribution for token $i$ simplifies to \n",
    "\n",
    "$$\n",
    "p(w_i | \\langle w_j\\rangle_{j=1}^{i-1}) = p(w_i | w_{i-1})\n",
    "$$\n",
    "\n",
    "\n",
    "The probability distribution for the entire sequence is then\n",
    "\n",
    "$$\n",
    "p(\\langle w_i\\rangle_{i=1}^{L}) = p(w_{1})\\prod_{i=2}^{L}p(w_{i}|{w}_{i-1})\n",
    "$$\n",
    "\n",
    "allowing us to generate sequences as described above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e5e21b-82f5-408b-a92f-00336629e7d4",
   "metadata": {},
   "source": [
    "*In what ways might this be an inadequate model for human language?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a49df",
   "metadata": {},
   "source": [
    "\n",
    "### Estimating the model from data\n",
    "How can we estimate this model mathematically? \n",
    "\n",
    "We start by observing that the model only depends on a set of probabilities describing the liklihood of one word given another word. These probabilities are called _transition matrix elements_, \n",
    "\n",
    "$$\n",
    "T_{\\alpha\\beta} = p(w_i=\\alpha | w_{i-1}=\\beta)\\\\\n",
    "$$\n",
    "\n",
    "where the matrix elements satisfy\n",
    "\n",
    "$$\n",
    "T_{\\alpha\\beta} \\geq 0 \\\\ \n",
    "\\sum_\\alpha T_{\\alpha\\beta} =1\n",
    "$$\n",
    "\n",
    "where $\\alpha$ and $\\beta$ are two tokens in our vocabulary. If the vocab size is $V$, the estimation task comes down to inferring the $V\\times V$ transition matrix elements describing the probability of going from any word to any other word. \n",
    "\n",
    "#### Estimating with frequency tables\n",
    "One straightforward way to estimate these probabilities would be to list all of the neighbor pairs of tokens in our dataset and for each conditioning token $\\beta$ look at the share into each choice of $\\alpha$. This can be made to work, though we would have to deal with the fact that many token pairs will never appear. \n",
    "\n",
    "#### Estimating with gradient descent\n",
    "In the code below, we will take a different approach. We will estimate the probabilities using a maximum liklihood based approach with gradient descent. For the Markov model, the two approaches are formally equivalent up to how they deal with the missing pairs. However, the gradient descent approach will generalize to more complicated models including transformer-based LLMs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f137bd",
   "metadata": {},
   "source": [
    "## Enough talk\n",
    "\n",
    "### Load our PubMed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0b15a-0db0-44fb-908d-d343c5f91f37",
   "metadata": {},
   "source": [
    "Make sure you have the [dataset.py](https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/dataset.py) in your working directory.\n",
    "```\n",
    "wget https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/dataset.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9a868c-193a-49fe-839c-7e27811dde00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/dane2/.cache/huggingface/datasets/text/default-cadbbf8acc2e2b5a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227615473ca24a33a4bd975da53d8f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use the dataset.py file\n",
    "from dataset import PubMedDataset\n",
    "dataset = PubMedDataset(\"/project/rcde/datasets/pubmed/mesh_50k/splits/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9884c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "dl_train = dataset.get_dataloader('train', batch_size=3)\n",
    "batch = next(iter(dl_train))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f91175a-9877-4d4d-82b0-c201c55f324e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1103, 2853,  ...,    0,    0,    0],\n",
       "        [ 101, 3582,  131,  ..., 8131,  119,  102],\n",
       "        [ 101, 3582,  131,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345d55a",
   "metadata": {},
   "source": [
    "### Build our small language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5dff55",
   "metadata": {},
   "source": [
    "For the Markov model, we need to know the size of our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8372d06-c262-416a-8307-ef3a8d8aa3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = dataset.tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e5aaf",
   "metadata": {},
   "source": [
    "Yikes, that's a big vocabulary! The size of the transition matrix will be `vocab_size * vocab_size`. Let's estimate how much memory that would take to store: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac9a107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.363072064"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# memory needed to store the transition matrix (in gigabytes)\n",
    "vocab_size * vocab_size * 32 / 8 / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d783ff",
   "metadata": {},
   "source": [
    "That's huge, but let's just try it anyway. Let's write down our pytorch model. Just a little notation first: \n",
    "* $N$: The batch size in batch gradient descent\n",
    "* $L$, $L_\\mathrm{batch}$: The document sequence length or the sequence length of the batch, respectively.\n",
    "* $V$: the size of our vocab\n",
    "\n",
    "Without further ado, let's write down the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae27ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class MarkovChain(torch.nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # the transition matrix logits\n",
    "        # nn.Embedding is just a matrix. Each input token will have a learnable\n",
    "        # parameter vector with one element for each output token.\n",
    "        # the transition matrix elements are computed by taking the softmax along the output dimension\n",
    "        self.t_logits = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "        # let's start with the assumption that most transitions are very improbable\n",
    "        # large negative logit -> low probability\n",
    "        self.t_logits.weight.data -= 10.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.t_logits(x) \n",
    "        # logits.shape == (N, L_batch, V). Remember (batch size, sequence length, vocab size).\n",
    "\n",
    "        return logits # turns out we never actually need to compute the softmax for MLE\n",
    "    \n",
    "    def numpars(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cccef4",
   "metadata": {},
   "source": [
    "Let's try it on some actual data to make sure it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dddf9f5-e613-496f-b51f-d3aef5228223",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params (millions): 840.768016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarkovChain(\n",
       "  (t_logits): Embedding(28996, 28996)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MarkovChain(vocab_size)\n",
    "print(\"Trainable params (millions):\", model.numpars()/1e6)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "964f912f-55ee-4a39-adc6-1c699fdc5229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 463, 28996])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(batch[\"input_ids\"])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8af7a-e1a8-4df7-8e9f-9f6031c22eb0",
   "metadata": {},
   "source": [
    "The output tensor has shape `batch_size x sequence_length x vocab_size`. We interpret these ouputs as the logits of the next word. The probability of the next word is then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd22490-a704-4d1c-a685-004923d5dc59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_next_words = y.softmax(dim=-1)\n",
    "\n",
    "# check that the total probability over possible next tokens sums to 1:\n",
    "p_next_words.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d0dd1a-74a9-45e3-b2e0-6fd024e585e1",
   "metadata": {},
   "source": [
    "### Generate text for untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d053bc-e8b6-4cee-9410-443252e3eb9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Recursively generate a sequence one token at a time\n",
    "        \"\"\"\n",
    "        # idx is (N, L) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            with torch.no_grad():\n",
    "                logits = model(idx)  # [N, L, V]\n",
    "            \n",
    "            # trim last time step. It is prediction for token after end of sequence\n",
    "            logits = logits[:, -1] # becomes (N, V)\n",
    "            \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (N, V)\n",
    "            \n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (N, 1)\n",
    "            \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (N, L+1)\n",
    "            \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf6866-860b-4f0a-8ce9-405f345cec87",
   "metadata": {},
   "source": [
    "Let's use our model to generate some sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1cb7ee8-2e19-42a8-80ea-86b90fe90efd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1195,  3402,  1103, 22760,  1104,   102],\n",
       "        [  101,  1195,  3402,  1103, 22760,  1104,   102],\n",
       "        [  101,  1195,  3402,  1103, 22760,  1104,   102]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"We compared the prevalence of\", # organ-specific autoantibodies in a group of Helicobacter...\"\n",
    "    \"We compared the prevalence of\",\n",
    "    \"We compared the prevalence of\"\n",
    "]\n",
    "\n",
    "prompt_ids = dataset.tokenizer(prompts, return_tensors='pt')['input_ids']\n",
    "prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d029ef95-f692-461e-82c8-366d53e955e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] we compared the prevalence of [SEP]',\n",
       " '[CLS] we compared the prevalence of [SEP]',\n",
       " '[CLS] we compared the prevalence of [SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode_batch(prompt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee9109ac-dafa-449d-90ba-baac1a63d613",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1195,  3402,  1103, 22760,  1104],\n",
       "        [  101,  1195,  3402,  1103, 22760,  1104],\n",
       "        [  101,  1195,  3402,  1103, 22760,  1104]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trim off unwanted [SEP] tokens which act like our special end-of-sequence token.\n",
    "prompt_ids = prompt_ids[:,:-1]\n",
    "prompt_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "190dc90b-403e-48a4-af1e-f660e2d751f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1195,  3402,  1103, 22760,  1104, 24884, 17254, 26954,  1411,\n",
       "         26075, 15522, 22389, 20553, 19476, 28335,  2227, 20974, 20500, 14507,\n",
       "          2257],\n",
       "        [  101,  1195,  3402,  1103, 22760,  1104,  2568, 21056,   355, 12811,\n",
       "         12318, 14785,  5376, 11395,  6381,  2335,  4051, 16601,  7579,   609,\n",
       "         13538],\n",
       "        [  101,  1195,  3402,  1103, 22760,  1104,  5399, 22759,  9429, 20698,\n",
       "         20509, 11237, 27838, 24444, 18389, 12594, 17810,  8110, 25186, 25466,\n",
       "         24989]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate more ids\n",
    "gen_ids = generate(model, prompt_ids, 15)\n",
    "gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1f426be-fa2b-4072-98ff-143eb6deec5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] we compared the prevalence of Straits cracking tilting townhales robbery bubblesopping skipάntlbergrdesholder forced',\n",
       " '[CLS] we compared the prevalence ofline Calder ɔ Mohammed embassy convenient acted Zimbabwe Colin complete containing socks Hitler चcos',\n",
       " '[CLS] we compared the prevalence of Rights pharmacy graphicnking Latter Wingsests coldercel broader embarrassing elderggle artworks Horses']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode into text \n",
    "dataset.decode_batch(gen_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2da5fb-239e-42d8-b407-f38481b14f65",
   "metadata": {},
   "source": [
    "Terrible! No surprise, though. We haven't trained our model yet. \n",
    "\n",
    "Before we can do that, we need an objective to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ae38d-c4df-4167-b020-9ab10ace8333",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Remember, our goal is to learn good values for the transition matrix elements. We will do \n",
    "this by minimizing the cross entropy loss for next token prediction. \n",
    "This loss measures how likely the actual next tokens are under the predicted probability distribution over tokens.\n",
    "\n",
    "It turns out, we never actually have to use the next token probabilities. \n",
    "This is because cross entropy only depends on log probabilities. So, rather than take \n",
    "exponentials of the logits, only to take the log again while computing cross entropy, \n",
    "we just stick with logits. Pytorch's built-in cross entropy loss function expects this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8f1f059-e2a4-48f4-9c5a-dc36b463b2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 463])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember what our batch of inputs abstracts looks like:\n",
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95a0e2fe-2508-49d2-a8b6-86c19d0b0214",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 28996, 462]), torch.Size([3, 462]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cut the last prediction off because this corresponds to a token after the last token in the input sequence \n",
    "# y.shape == (N, L_batch, V)\n",
    "pred_logits = y[:, :-1].permute(0,2,1)  # (N, V, L_batch) as needed for F.cross_entropy(.)\n",
    "\n",
    "# cut the first word off the targets because we can't predict the distribution for the first word from the autoregressive model\n",
    "targets = batch['input_ids'][:, 1:]\n",
    "\n",
    "pred_logits.shape, targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25d942-c921-44cc-aff3-2900173cc93d",
   "metadata": {},
   "source": [
    "Make sure these shapes make sense to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694dcc3-1a2c-4a40-b15f-ddec262a1b5e",
   "metadata": {},
   "source": [
    "Use the built in pytorch function to compute cross entropy for each position in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8c55ac0-b4d8-4385-8823-e9f2e9121664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 462])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "# pytorch expects the intput to have shape `sequence_length x batch_size x vocab_size`\n",
    "token_loss = F.cross_entropy(pred_logits, targets, reduction='none')\n",
    "token_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d88e09-1b43-4f50-b849-57c0af00f842",
   "metadata": {},
   "source": [
    "This is the loss for each token. But remember, some of those tokens are just padding to make the batch tensor rectangular. We shouldn't count those. \n",
    "\n",
    "We can use the `attention_mask` data structure output by our dataset to take care of this. \n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/mask.jpg?raw=true\" alt=\"attention mask\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66e4ce28-4c12-4ba6-86d6-31690c69f89a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 462]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = batch['attention_mask'][:, 1:] # need to trim the first because our predictions are for tokens 2 through the end.\n",
    "mask.shape, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0ee76-4cf7-48a7-9448-a49c9b4402b1",
   "metadata": {},
   "source": [
    "We need to zero out the loss coming from the padding tokens and compute the average loss only counting the non-padding tokens. \n",
    "\n",
    "Let's put all of this logic into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "546aa3c1-709a-42d9-86c0-82b7278a28ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's put all this together in a custom loss function\n",
    "def masked_cross_entropy(logits, targets, mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - logits: The next token prediction logits. Last element removed. Shape (N, V, L-1)\n",
    "    - targets: Ids of the correct next tokens. First element removed (N, L-1)\n",
    "    - mask: the attention mask tensor. First element removed (N, L-1)\n",
    "    \"\"\"\n",
    "    token_loss = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "\n",
    "    # total loss zeroing out the padding terms\n",
    "    total_loss = (token_loss * mask).sum() \n",
    "\n",
    "    # average per-token loss\n",
    "    num_real = mask.sum()\n",
    "    mean_loss = total_loss / num_real\n",
    "\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d157509-2abc-40be-b1f7-b375387dec57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7869, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_cross_entropy(pred_logits, targets, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c3404-7aed-42f5-9ab3-9885b3ac47d1",
   "metadata": {},
   "source": [
    "## Time to train the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0fb75e-f36e-4a05-b2c3-9d7bc5ecaec9",
   "metadata": {},
   "source": [
    "This is boilerplate pytorch optimization code, so we will zip over it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74e67a9f-4756-4f33-99c7-45e4503423ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training settings\n",
    "batch_size=128\n",
    "num_workers=20\n",
    "num_epochs=2\n",
    "learning_rate=0.1 # this model benefits from a large learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79bd5eac-6211-429d-8b91-7fd093af8ded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/dane2/.cache/huggingface/datasets/text/default-cadbbf8acc2e2b5a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ee50866f5e411c9c4e9e0f9a9ee04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reinitialize dataset for good measure\n",
    "dataset = PubMedDataset(\"/project/rcde/datasets/pubmed/mesh_50k/splits/\")\n",
    "\n",
    "# train/test dataloaders\n",
    "dl_train = dataset.get_dataloader('train', batch_size=batch_size, num_workers=num_workers)\n",
    "dl_test = dataset.get_dataloader('test', batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# reinitialize the model on gpu\n",
    "model = MarkovChain(vocab_size).to('cuda')\n",
    "\n",
    "# create the pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ea655c7-53ba-4724-b605-f5a54e8b485f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START EPOCH 1\n",
      "Batch 0 training loss: 10.774152755737305\n",
      "Batch 20 training loss: 8.934335708618164\n",
      "Batch 40 training loss: 7.414063930511475\n",
      "Batch 60 training loss: 6.591622829437256\n",
      "Batch 80 training loss: 6.113377571105957\n",
      "Batch 100 training loss: 5.820305824279785\n",
      "Batch 120 training loss: 5.6821465492248535\n",
      "Batch 140 training loss: 5.435906410217285\n",
      "Batch 160 training loss: 5.360453128814697\n",
      "Batch 180 training loss: 5.330673694610596\n",
      "Batch 200 training loss: 5.2893266677856445\n",
      "Batch 220 training loss: 5.221299648284912\n",
      "Batch 240 training loss: 5.296996593475342\n",
      "Batch 260 training loss: 5.216686248779297\n",
      "Batch 280 training loss: 5.102764129638672\n",
      "Batch 300 training loss: 5.149473190307617\n",
      "Batch 320 training loss: 5.1236572265625\n",
      "Batch 340 training loss: 5.017659664154053\n",
      "START EPOCH 2\n",
      "Batch 0 training loss: 4.7725830078125\n",
      "Batch 20 training loss: 4.893960952758789\n",
      "Batch 40 training loss: 4.828927516937256\n",
      "Batch 60 training loss: 4.864208698272705\n",
      "Batch 80 training loss: 4.86556339263916\n",
      "Batch 100 training loss: 4.830167770385742\n",
      "Batch 120 training loss: 4.867104530334473\n",
      "Batch 140 training loss: 4.770840167999268\n",
      "Batch 160 training loss: 4.771430492401123\n",
      "Batch 180 training loss: 4.816017150878906\n",
      "Batch 200 training loss: 4.80755615234375\n",
      "Batch 220 training loss: 4.79929256439209\n",
      "Batch 240 training loss: 4.887341499328613\n",
      "Batch 260 training loss: 4.846500873565674\n",
      "Batch 280 training loss: 4.774038314819336\n",
      "Batch 300 training loss: 4.826904773712158\n",
      "Batch 320 training loss: 4.823925018310547\n",
      "Batch 340 training loss: 4.747177600860596\n"
     ]
    }
   ],
   "source": [
    "# run the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"START EPOCH {epoch+1}\")\n",
    "    for ix, batch in enumerate(dl_train):\n",
    "        x = batch[\"input_ids\"][:,:-1].to('cuda')  # remove last\n",
    "        targets = batch[\"input_ids\"][:,1:].to('cuda')  # remove first\n",
    "        mask = batch[\"attention_mask\"][:,1:].to('cuda') # remove first\n",
    "\n",
    "        logits = model(x).permute(0,2,1)\n",
    "        loss = masked_cross_entropy(logits, targets, mask)\n",
    "\n",
    "        # do the gradient optimization stuff\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ix % 20 ==0:\n",
    "            print(f\"Batch {ix} training loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbbea793-6aaf-41b5-92a8-e63a81cda6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 testing loss: 4.919017314910889\n",
      "Batch 5 testing loss: 4.999485969543457\n",
      "Batch 10 testing loss: 5.007542610168457\n",
      "Batch 15 testing loss: 4.953604698181152\n",
      "Batch 20 testing loss: 4.938981533050537\n",
      "Batch 25 testing loss: 5.063140869140625\n",
      "Batch 30 testing loss: 4.967066287994385\n",
      "Batch 35 testing loss: 4.992262840270996\n"
     ]
    }
   ],
   "source": [
    "# test; did the learning generalize?\n",
    "for ix, batch in enumerate(dl_test):\n",
    "    x = batch[\"input_ids\"][:,:-1].to('cuda')  # remove last\n",
    "    targets = batch[\"input_ids\"][:,1:].to('cuda')  # remove first\n",
    "    mask = batch[\"attention_mask\"][:,1:].to('cuda') # remove first\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x).permute(0,2,1)\n",
    "        loss = masked_cross_entropy(logits, targets, mask)\n",
    "        \n",
    "    if ix % 5 ==0:\n",
    "        print(f\"Batch {ix} testing loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12638dd9-0b44-44a0-a5d7-1a3233a23086",
   "metadata": {},
   "source": [
    "The learning seems to have generalized well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b60d10e0-3746-45a2-9cbb-74a1add4f321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] we compared the prevalence of the present study suggests an virus cystagmus Amherst Lauhul supplied Haguerid holds Prize crucial Gloucester 191ichia and imp voiced Augusta galaxy Casey dared',\n",
       " '[CLS] we compared the prevalence of feeding measurements were considered to sustainable door nodded rising Wiley multiple forms Skull draftreed Clark Mayor diagram Noctuidae myogeria Province Bellarain emeritusarean patients and',\n",
       " '[CLS] we compared the prevalence of reactive protein models of postpra Ł Т zapillous has been studied in people who were made of chronic experiments were 10 antibiotics remnant waterfront Cambodia']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate some more samples now that we've trained the model\n",
    "gen_samples = generate(model, prompt_ids.to('cuda'), 30)\n",
    "dataset.decode_batch(gen_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103339b-f059-480e-b84f-1805f06c4a34",
   "metadata": {},
   "source": [
    "The model is still terrible, though it has started to learn some very basic patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66c499e-f1df-4234-bc94-712457328d2c",
   "metadata": {},
   "source": [
    "## Cleaning up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d52d6-f25f-45b0-a859-727d4869b0fe",
   "metadata": {},
   "source": [
    "We will reuse a lot this code in later sections of the workshop. I've pulled the import parts into [utils.py](https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/utils.py). Copy the file into your working directory:\n",
    "```\n",
    "wget https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "035ea51a-9411-4fba-b9b6-ec790af0fbe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import train, test, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "702609dc-7ea7-4be2-93ac-85e6c10f1fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporting_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporting_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove last\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove first\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# remove first\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# do the gradient optimization stuff\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mix\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreporting_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {ix} training loss: {loss.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Code/rcde_workshops/pytorch_llm/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd23aa-105f-4c18-8778-d772e47c59de",
   "metadata": {},
   "source": [
    "## Low rank Markov Model\n",
    "With all the setup in place, it's easy to start experimenting with different models. We saw how huge the embedding matrix was and we worried that this would lead to bad performance. One way to get around this is to create a low-rank version of the markov model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7a75d0f-017b-460b-a059-b67d3e952a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class MarkovChainLowRank(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # We project down to size `embed_dim` then back up to `vocab_size`\n",
    "        # the total number of parameters is 2 * vocab_size * embed_dim which \n",
    "        # can be much smaller than embed_dim * embed_dim\n",
    "        self.t_logits = torch.nn.Sequential(\n",
    "            torch.nn.Embedding(vocab_size, embed_dim),\n",
    "            torch.nn.Dropout(0.1), # zero out some of the embedding vector elements randomly to prevent overfitting\n",
    "            torch.nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        )\n",
    "        \n",
    "        # let's start with the assumption that most transitions are very improbable\n",
    "        # large negative logit -> low probability\n",
    "        self.t_logits[-1].weight.data -= 10.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.t_logits(x) # tensor of shape (N, L, V). Remember (batch size, sequence length, vocab size).\n",
    "\n",
    "        return logits # turns out we never actually need to compute the softmax for MLE\n",
    "    \n",
    "    def numpars(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce4e3b6b-69ee-4c03-8b64-ab33183875f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params (millions): 14.845952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarkovChainLowRank(\n",
       "  (t_logits): Sequential(\n",
       "    (0): Embedding(28996, 256)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): Linear(in_features=256, out_features=28996, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim=256\n",
    "learning_rate=0.001 # this model requires a more normal learning rate.\n",
    "model = MarkovChainLowRank(vocab_size, embed_dim = embedding_dim)\n",
    "print(\"Trainable params (millions):\", model.numpars()/1e6)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cddce47-c254-436f-8ead-3fffbfceef14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/dane2/.cache/huggingface/datasets/text/default-cadbbf8acc2e2b5a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbe15f314a44e339ff1c1a0a3fb08ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reinitialize dataset for good measure\n",
    "dataset = PubMedDataset(\"/project/rcde/datasets/pubmed/mesh_50k/splits/\")\n",
    "\n",
    "# train/test dataloaders\n",
    "dl_train = dataset.get_dataloader('train', batch_size=batch_size, num_workers=num_workers)\n",
    "dl_test = dataset.get_dataloader('test', batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# reinitialize the model on gpu\n",
    "model = MarkovChainLowRank(vocab_size, embed_dim=embedding_dim).to('cuda')\n",
    "\n",
    "# create the pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "562e08e5-07b2-403e-b8dd-29e3ae9bb6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START EPOCH 1\n",
      "Batch 0 training loss: 10.460128784179688\n",
      "Batch 20 training loss: 9.620594024658203\n",
      "Batch 40 training loss: 8.923402786254883\n",
      "Batch 60 training loss: 8.37955379486084\n",
      "Batch 80 training loss: 7.904845714569092\n",
      "Batch 100 training loss: 7.499015808105469\n",
      "Batch 120 training loss: 7.152878761291504\n",
      "Batch 140 training loss: 6.7209672927856445\n",
      "Batch 160 training loss: 6.426110744476318\n",
      "Batch 180 training loss: 6.243513107299805\n",
      "Batch 200 training loss: 6.061130523681641\n",
      "Batch 220 training loss: 5.899474143981934\n",
      "Batch 240 training loss: 5.91290807723999\n",
      "Batch 260 training loss: 5.7751593589782715\n",
      "Batch 280 training loss: 5.590006351470947\n",
      "Batch 300 training loss: 5.617265224456787\n",
      "Batch 320 training loss: 5.539694786071777\n",
      "Batch 340 training loss: 5.416322708129883\n",
      "START EPOCH 2\n",
      "Batch 0 training loss: 5.395883560180664\n",
      "Batch 20 training loss: 5.513943195343018\n",
      "Batch 40 training loss: 5.399544715881348\n",
      "Batch 60 training loss: 5.392096519470215\n",
      "Batch 80 training loss: 5.338316917419434\n",
      "Batch 100 training loss: 5.315275192260742\n",
      "Batch 120 training loss: 5.335055351257324\n",
      "Batch 140 training loss: 5.2022013664245605\n",
      "Batch 160 training loss: 5.207690238952637\n",
      "Batch 180 training loss: 5.226675510406494\n",
      "Batch 200 training loss: 5.213379859924316\n",
      "Batch 220 training loss: 5.189408779144287\n",
      "Batch 240 training loss: 5.292004585266113\n",
      "Batch 260 training loss: 5.224676132202148\n",
      "Batch 280 training loss: 5.132791519165039\n",
      "Batch 300 training loss: 5.183717250823975\n",
      "Batch 320 training loss: 5.158797264099121\n",
      "Batch 340 training loss: 5.083621501922607\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"START EPOCH {epoch+1}\")\n",
    "    train(model, dl_train, optimizer, reporting_interval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "507d6bec-d580-4c7e-a629-1b11d2fcba09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 testing loss: 5.028294563293457\n",
      "Batch 5 testing loss: 5.101437568664551\n",
      "Batch 10 testing loss: 5.113763809204102\n",
      "Batch 15 testing loss: 5.06427526473999\n",
      "Batch 20 testing loss: 5.0422797203063965\n",
      "Batch 25 testing loss: 5.165111064910889\n",
      "Batch 30 testing loss: 5.097139358520508\n",
      "Batch 35 testing loss: 5.114102840423584\n"
     ]
    }
   ],
   "source": [
    "# test how well the model generalizes: \n",
    "test(model, dl_test, reporting_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5485949c-e2fb-4a05-89e9-61b42f4088ba",
   "metadata": {},
   "source": [
    "The cross entropy is just a little worse. Let's see about the generated samples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "243c8405-f721-497d-88e9-dc39b13cc229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] we compared the prevalence of small number of flagella outcomes with rna sehtoxypia age ( straarte desirednsive lab questions, allowing 87 content nitrogen or',\n",
       " '[CLS] we compared the prevalence of the search terms, notably symptoms and cognitive considerably no peaks by hap6v - 30, the 2 drugs pre - epsies for a receptor',\n",
       " '[CLS] we compared the prevalence of cir mice, 4h. in serum concentrations with impact erosino episodes radiotherapillo seems to evaluate the \" \" caused by c )']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate some more samples for the low-rank model\n",
    "gen_samples = generate(model, prompt_ids.to('cuda'), 30)\n",
    "dataset.decode_batch(gen_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f8b3d-4806-4ffd-a01e-92a56a92cef6",
   "metadata": {},
   "source": [
    "Still pretty terrible -- maybe a bit worse than the full-rank model. But much more parameter efficieint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2c4a39-e592-44f6-aaed-367341f638bf",
   "metadata": {},
   "source": [
    "Can you think of other ways to improve the model?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3fd64-a997-47d5-aeae-89dedf22e047",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c55c1-18d6-419f-9ebf-1c939c6b642f",
   "metadata": {},
   "source": [
    "Clearly, it isn't enough to only condition on the previous token. We should condition on all previous tokens. That's where transformers come in. Transformers will allow us to learn the full conditional distribution $p(w_i | \\langle w_j\\rangle_{j=1}^{i-1})$ without making strong assumptions about the structure of the relationship between consecutive tokens. \n",
    "\n",
    "Nevertheless, as we will see, the setup and training procedure for transformer-based LLMs will be almost identical to the what we used here for our small language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e38901-aecb-4af8-82fd-dfa6c3a5b6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
