{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b226ea-6d9a-4449-90d7-5d021068208e",
   "metadata": {},
   "source": [
    "# Other LLM Topics\n",
    "In this workshop we focused on understanding the core ideas of the Language Modeling and the Transformer Architecture. The community has built many methods and tools around these core ideas. In this notebook we list some of the most important ideas. This is by no means complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896252d7-c1e8-44f5-9cc0-5445f601ee5b",
   "metadata": {},
   "source": [
    "## From here to ChatGPT\n",
    "How do we get from the humble language model we developed today to something as powerful as ChatGPT?\n",
    "\n",
    "There are 3 primary ingredients. The first two are boring:\n",
    "1. Scale and quality of the training data (~ Million times larger)\n",
    "2. Scale of the model (number and size of transformer blocks) (10s of thousands of times larger)\n",
    "3. Human preference training\n",
    "\n",
    "Obviously this all takes a lot more compute resources than we have here. For instance, Elon's new Grok LLM was trained using 8000 A100 gpus. \n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/rlhf.svg?raw=true\" alt=\"Reinforcement learning with human feedback\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83586500-76d6-415f-98a8-b9cd7eb00ca6",
   "metadata": {},
   "source": [
    "## Masked language modeling\n",
    "So far, we've only discussed Autoregressive Language Modeling. This is the kind used for generating text, like ChatGPT. Another important class of language models are \"Masked Language Models\" as introduced by [BERT](https://arxiv.org/abs/1810.04805) (see figure). Masked language models are useful when fine-tuning on small datasets. \n",
    "\n",
    "*How would we have to change our attention masking to accomodate masked language modeling?*\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/mlm.png?raw=true\" alt=\"BERT architecture\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529834ea-4eb6-4d58-8c0b-89b8580bc386",
   "metadata": {},
   "source": [
    "## ðŸ¤— Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7461eac-f3ac-42cd-b20d-a59e637fb90d",
   "metadata": {},
   "source": [
    "We used Huggingface tools for the data prep. It's way bigger than that. The [Huggingface Hub](https://huggingface.co/docs/hub/index) has become the de-facto place to upload pretrained LLMs. To date, they have more than 31,000 autoregressive language models uploaded. In addition, the Huggingface APIs make it easy to load any of these models and fine tune them on your own data. For instance the [AutoModelForCausalLM](https://huggingface.co/docs/transformers/v4.35.1/en/model_doc/auto#transformers.AutoModelForCausalLM) class allows you to download a model from the Hub for use as a Causal/Autoregressive language model. You can then use the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class to fit the model to your data. These tools make it much easier to experiment with a wide variety of new language models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed8cc4-5201-4102-bcb9-5964d8640df0",
   "metadata": {},
   "source": [
    "## Low-rank adaptation (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b632d8-7fe8-46aa-9461-fe4a58bff93e",
   "metadata": {},
   "source": [
    "Fine tuning refers to adapting a pre-trained LLM to a small, domain-specific dataset. As mentioned, Huggingface provides convenient tools for this fine tuning. However, some models are so large, that it is prohibitively computationally intensive to fine tune. There are a number of techniques to address this. Perhaps the most widely applied is [Low-rank adaption (LoRA)](https://arxiv.org/abs/2106.09685). The basic idea is simple. Rather than fine tune all of the network weight matrices, we will only fine tune some of them. In addition, rather than fine tune full-rank weight matrices, we fine-tune low-rank weight matrices. See picture for the idea. \n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/lora.png?raw=true\" alt=\"Low rank adaptation\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14785b73-d099-4a7f-99d3-b5dcd46c2318",
   "metadata": {},
   "source": [
    "## Model quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d41b33-e36b-437b-b3c1-b699911bbf88",
   "metadata": {},
   "source": [
    "Lora helps reduce the amount of compute needed to fine-tune an LLM. However, many LLMs may still be too large to fit in VRAM. Quantization reduces model size by re-expressing the trained LLM into lower-precision weights. This can lead to factor of 2 or 4 reductions in the memory footprint. Quantization and LoRA are often used together. Using these techniques, LLMs can be fine tuned on CPU, on with hybrid mixtures of CPU and GPU, and can even make use of Swap memory.  \n",
    "\n",
    "<img src=\"figs/quantization.png\" alt=\"Quantization\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18070328-c668-4598-97f6-1c1b8eaf3849",
   "metadata": {},
   "source": [
    "## Handling long context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96659fae-9bef-4c7f-bf09-346f0c6328dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
