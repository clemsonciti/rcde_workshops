{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ceb62e-dd68-4d37-bfc3-07677f681dfb",
   "metadata": {},
   "source": [
    "# Preparing data for LLM training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d188814-c88d-42c5-9c24-66a80d7b85d8",
   "metadata": {},
   "source": [
    "We are going to train our LLM using the [PubMed dataset](https://pubmed.ncbi.nlm.nih.gov/download/), which contains abstracts from biomedical journal articles. To keep things quick for the workshop, we will be working with a [small subset of 50k abstracts](https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification). There are a few key concepts to consider when preparing data for LLM training: \n",
    "1. **Tokenization**: how do we turn the raw text strings into units of analysis for our program?\n",
    "2. **Batching**: how do we batch multiple documents into a batch data structure for efficient model training?\n",
    "\n",
    "To address these issues, we will be using a [pre-built biomedical data tokenizer](https://huggingface.co/dmis-lab/biobert-base-cased-v1.2) available from the huggingface hub. Implementing tokenizers is a complicated topic on its own, and we will not deal with it in detail here.\n",
    "\n",
    "One pleasant difference between LLMs and the previous generation of NLP techniques is that we do not usually need to perform elaborate data preprocessing to acheive good results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e80803-3425-4288-923d-32cae738eba3",
   "metadata": {},
   "source": [
    "## PubMed Data\n",
    "The huggingface `datasets` library contains some useful utilities for loading and working with text data. We use this here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442806c-3503-4ede-8949-0b7f51717e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb701c-2196-448b-8940-104aeef04632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to folder containing \"train.txt\" and \"test.txt\" files containing train/test PubMed abstracts\n",
    "root = \"/project/rcde/datasets/pubmed/mesh_50k/splits/\"\n",
    "\n",
    "train_test_files = {\n",
    "    \"train\": root+\"train.txt\",\n",
    "    \"test\": root+\"test.txt\"\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files = train_test_files).with_format(\"torch\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb5aad-90c8-4edb-b471-1790a7fad7d4",
   "metadata": {},
   "source": [
    "Let's check the sizes of training and test sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a4cff-65e1-4c12-a409-d06048a3c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"train\"]), len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96693950-c371-4a6a-8a8e-ad978943065b",
   "metadata": {},
   "source": [
    "Look at a particular training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a092bf5-0e18-429e-8998-3828f55a2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][34799]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb97e8-afdb-4c7d-9708-b06f70d7b58a",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c387c2b-134e-4b06-92e3-e4f8db0851b7",
   "metadata": {},
   "source": [
    "Here, we will use the Huggingface `transformers` library to fetch a tokenizer purpose-built for biomedical data. The `AutoTokenizer` class allows us to provide the name of a model on the Huggingface Hub and automatically retrieve the associated tokenizer. We could experiment with different tokenizers to try to acheive better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6888f-879b-4f77-8627-9bc17b57cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71918d60-3713-49dd-8589-2bd5ee4074e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a pretrained tokenizer\n",
    "# https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69832f-3b73-49e4-b73d-bb7b0e3ef4e7",
   "metadata": {},
   "source": [
    "Let's tokenize some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ad0d9-978c-426a-bff0-110b3b4febac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(dataset[\"train\"][34799]['text'])\n",
    "tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692cb1d-25ba-4aa0-bfa9-d1a90a663b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(tokenized_example['input_ids']),len(tokenized_example['token_type_ids']),len(tokenized_example['attention_mask']))\n",
    "print(len(example['text'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dd5f9-bd70-49a6-9bfd-99468caccfdd",
   "metadata": {},
   "source": [
    "The `input_ids` list contains an encoded representation of our text. It is a sequence of integer IDs corresponding to the tokens that appear in the text we tokenized. The IDs refer to specific terms in a pre-defined vocabulary that came with the tokenizer. So the `input_ids` list can be decoded back into our original text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd86832-c78b-48a9-ac22-41408f284bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb0ae3-976c-4dc7-8032-5ff99b5ca980",
   "metadata": {},
   "source": [
    "We will usually want pytorch tensors, not lists, as output. For this we need to enable padding. \n",
    "\n",
    "*What do you think padding does?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8588c4-41bd-422f-95cb-43c11c062c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids = tokenizer(dataset[\"train\"][34799:34801]['text'], return_tensors='pt', padding=True)['input_ids']\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e23ce-27e9-481d-b45a-79db54c33219",
   "metadata": {},
   "source": [
    "*Can you see how padding appears in the tokenized text?* \n",
    "\n",
    "*Do you notice anything special about the first and last non-padding tokens?* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e54c9d-0f9f-4272-a491-ae705754be5b",
   "metadata": {},
   "source": [
    "Let's decode back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625648cb-edab-43d5-9a3b-f87ea324bf2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[tokenizer.decode(input_ids) for input_ids in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d2ecf-fb61-46ea-83de-2b0d1d484fb8",
   "metadata": {},
   "source": [
    "There is a minor cleanliness issue: the abstracts start and close with unneeded quotation marks. We will add a preprocessing step to remove these while batching samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae8226-720c-42b8-9005-a421c81eec2f",
   "metadata": {},
   "source": [
    "## Cleaning and batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5084ea-fbcc-4b45-b9ce-c1187e8c29cc",
   "metadata": {},
   "source": [
    "Here we will interface between the Huggingface tools and native Pytorch tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64bce3-960a-458c-a069-a8f8b7941e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf029f-11b7-4fd8-bd81-d306a93684f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text_batch):\n",
    "    \"\"\"\n",
    "    This method demonstrates how you can apply custom preprocessing logic while you load your data. \n",
    "    \n",
    "    It expects a list of plaintext abstracts as input. \n",
    "    \"\"\"\n",
    "    ## custom preprocessing\n",
    "    # get rid of unwanted opening/closing quotes\n",
    "    text_batch = [t[1:-1] for t in text_batch]\n",
    "    \n",
    "    ## tokenization\n",
    "    # we use the huggingface tokenizer as above\n",
    "    text_batch = tokenizer(text_batch, padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    return text_batch\n",
    "    \n",
    "def custom_collate(batch_list):\n",
    "    \"\"\"\n",
    "    This is for use with the pytorch DataLoader class. We use the default collate function\n",
    "    but add the cleaning and tokenization step. \n",
    "    \"\"\"\n",
    "    batch = default_collate(batch_list)\n",
    "    batch['text'] = clean_and_tokenize(batch['text'])\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17f2b4-4978-45cd-bf0f-773b8b243521",
   "metadata": {},
   "source": [
    "We can now use this collate function with the Pytorch DataLoader class to load, clean, tokenize and batch our text data. Once we can do this, we're ready to work on modeling our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a557c1-6a35-42cd-926b-c31820781d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset['train'], batch_size=3, collate_fn = custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b99ed-3f8d-4f4c-83b3-18f7d00e86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a batch\n",
    "batch = next(iter(dl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99db5c-c07c-4bfe-8374-edd501a2806f",
   "metadata": {},
   "source": [
    "## Saving code for later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da328a-80dd-4f68-8c06-739a912bef24",
   "metadata": {},
   "source": [
    "I've pulled the above code into a separate file called [dataset.py](https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/dataset.py). This will allow us to reuse the code in future notebooks. Copy the file into your working directory:\n",
    "```\n",
    "wget https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/dataset.py\n",
    "```\n",
    "\n",
    "Let's briefly look at the usage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a868c-193a-49fe-839c-7e27811dde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PubMedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8372d06-c262-416a-8307-ef3a8d8aa3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PubMedDataset(\n",
    "    root = \"/project/rcde/datasets/pubmed/mesh_50k/splits/\", \n",
    "    max_tokens = 20,\n",
    "    tokenizer_model = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddf9f5-e613-496f-b51f-d3aef5228223",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = dataset.get_dataloader(split='train', batch_size=3) # split can be \"train\" or \"test\"\n",
    "batch = next(iter(dl_train))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b7465-125e-4e9c-b964-c6fc98bb2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.decode_batch(batch['input_ids'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AttentionWorkshop",
   "language": "python",
   "name": "attentionworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
