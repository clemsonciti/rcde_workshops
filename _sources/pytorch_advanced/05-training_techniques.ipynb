{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n40xD862nhd"
   },
   "source": [
    "# Training Techniques\n",
    "The [Pytorch Lightning](https://www.pytorchlightning.ai/index.html) `Trainer` class implements many advanced features to improve training speed, convergence, reproducibility, etc. In this notebook, we apply a number of these features to our EMNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils import models\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = f\"/scratch/{os.environ['USER']}/data\"\n",
    "model_path = f\"/scratch/{os.environ['USER']}/model.pt\"\n",
    "\n",
    "# Model and Training\n",
    "epochs=2 # number of training epochs\n",
    "batch_size=128 #input batch size for training (default: 64)\n",
    "test_batch_size=1000 #input batch size for testing (default: 1000)\n",
    "num_workers=2 # parallel data loading to speed things up\n",
    "lr=0.1 #learning rate (default: 0.1)\n",
    "gamma=0.7 #Learning rate step gamma (default: 0.7)\n",
    "seed=42 #random seed (default: 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NGHwURr8or8"
   },
   "source": [
    "## EMNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import data\n",
    "\n",
    "# transforms (we may wish to experiment with these so leave as inputs)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "test_transforms = train_transforms\n",
    "\n",
    "train_loader = data.get_train_dataloader(data_dir, train_transforms, batch_size, num_workers)\n",
    "test_loader = data.get_test_dataloader(data_dir, test_transforms, test_batch_size, num_workers)\n",
    "\n",
    "# save a test batch for later testing\n",
    "image_gen = iter(test_loader)\n",
    "test_img, test_trg = next(image_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Training dataset:\", train_loader.dataset)\n",
    "print(\"Testing dataset:\", test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test batch\n",
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the classifier\n",
    "pt_model = models.Classifier() #models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# init the lazy layers\n",
    "with torch.no_grad():\n",
    "    pt_model(x)\n",
    "    \n",
    "# create the lightning model\n",
    "# Note: since the last notebook, we moved the LitModel logic into utils.models\n",
    "model = models.LitModel(pt_model, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "# Without this line, there's a warning that points us to using it. \n",
    "# This allows a slight tradeoff of precision for speed with our GPU's tensor cores. \n",
    "# See https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# a logger to save results\n",
    "csv_logger = pl_loggers.CSVLogger(save_dir=\"logs/\")\n",
    "\n",
    "# the trainer class has about a million arguments. For now, the defaults will suffice. See docs here: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html\n",
    "trainer = Trainer(max_epochs=epochs, logger=csv_logger)\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `logs/lightning_logs` for results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model profiling\n",
    "Profiling tools show you how much time each part of your training code is taking. This can help you identify areas where your program should be optimized in order to speed things up. \n",
    "\n",
    "[Pytorch has a built in profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html). We can easily turn this on in Lightning by setting the `profiler` argument in the trainer. Note that the Pytorch profiler forces synchronous cuda execution. That makes things take longer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the classifier\n",
    "pt_model = models.Classifier() #models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# init the lazy layers\n",
    "with torch.no_grad():\n",
    "    pt_model(x)\n",
    "    \n",
    "# create the lightning model\n",
    "model = models.LitModel(pt_model, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just need to set the profiler argument\n",
    "trainer = Trainer(max_epochs=epochs, logger=csv_logger, profiler='simple') # or 'pytorch', etc\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightning supports [several other performance profilers](https://pytorch-lightning.readthedocs.io/en/1.6.2/advanced/profiler.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging with Weights and Biases\n",
    "Logging is simply the act of recording data throughout model training and evaluation that can be used to make decisions about model development. Earlier, we used the CSV logger to record experimental results. [Weights and Biases (WandB)](https://wandb.ai/site) is an online platform for logging training experiments. It provides a range of data collection and visualization tools to help you understand how your training is going. WandB is free for academic use cases. It's also very easy to integrate with Pytorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the classifier\n",
    "pt_model = models.Classifier() #models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# init the lazy layers\n",
    "with torch.no_grad():\n",
    "    pt_model(x)\n",
    "    \n",
    "# create the ligtning model\n",
    "model = models.LitModel(pt_model, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name='initial_run')\n",
    "print(wandb_logger.experiment.url)\n",
    "\n",
    "# just need to set the profiler argument\n",
    "trainer = Trainer(max_epochs=epochs, logger=wandb_logger)\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "# indicate that the run has finished\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Automatic mixed precision (AMP)\n",
    "By default, PyTorch uses 32-bit floating point numbers. These means that each element of a tensor, takes up 32 bits / 4 Bytes of memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.randn(3).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32-bit floats can store about 8 digits of precision. This level of precision may not be necessary for many of the computations performed by the neural network. Pytorch supports several other floating point formats, that we can make use of. For instance, we can allocate 16-bit tensors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.randn(3, dtype=torch.float16).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16-bit floats take up half the memory of 32 bit, so this may allow us to train larger models on the same GPU hardware. In addition, modern GPU architectures can perform some calculations more efficiently with 16-bit numbers. \n",
    "\n",
    "Unfortunately, it turns out that its usually not a good idea to convert all aspects of our computation into 16-bit floats. The research community has come up with good approaches to mixing 32-bit and 16-bit computation to get the benefits of using lower-precision without hurting model convergence. Manually setting all of this up is a headache, so Pytorch supports \"Automatic Mixed Precision\" to perform the conversion automatically under the hood. \n",
    "\n",
    "To use this functionality, you use the `autocast` context manager: \n",
    "```python\n",
    "# Creates model and optimizer in default precision\n",
    "model = Net().cuda()\n",
    "optimizer = optim.SGD(model.parameters(), ...)\n",
    "\n",
    "# Creates a GradScaler once at the beginning of training.\n",
    "# this improves convergence by preventing underflow\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in epochs:\n",
    "    for input, target in data:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Runs the forward pass with autocasting.\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMP in Pytorch Lightning\n",
    "Fortunately for us, it is extremely easy to implement AMP now that we have our model set up in Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init the classifier\n",
    "pt_model = models.Classifier() #models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# init the lazy layers\n",
    "with torch.no_grad():\n",
    "    pt_model(x)\n",
    "    \n",
    "# create the ligtning model\n",
    "model = models.LitModel(pt_model, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name='AMP run')\n",
    "\n",
    "trainer = Trainer(max_epochs=epochs, logger=wandb_logger, \n",
    "                  precision='16-mixed') #<-- this\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model checkpointing\n",
    "Simply put, checkpointing is the process of saving a model periodically based on a metric that you monitor. If the metric has improved, save the model. In addition to saving the model weights, we need to save the hyperparameters. We do this by calling `self.save_hyperparameters()` in the initializer for the lightning model:\n",
    "```python\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, pytorch_model, lr, gamma):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        ...\n",
    "```\n",
    "Unlike the previous options, we need to use a Callback method to set up checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the classifier\n",
    "pt_model = models.Classifier() #models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# init the lazy layers\n",
    "with torch.no_grad():\n",
    "    pt_model(x)\n",
    "     \n",
    "# create the ligtning model\n",
    "model = models.LitModel(pt_model, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# need to import the checkpoint callback object\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(\"checkpoints/\", save_top_k=3, monitor='val_loss')\n",
    "\n",
    "# we add log_model='all' to save models on wandb\n",
    "wandb_logger = WandbLogger(name='Run with Checkpointing', log_model='all')\n",
    "\n",
    "# need to pass the checkpoint callback\n",
    "trainer = Trainer(max_epochs=epochs, logger=wandb_logger, precision='16-mixed', callbacks = [checkpoint_callback])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model from a local checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a model from a checkpoint, we use the `load_from_checkpoint`, passing in the path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LitModel.load_from_checkpoint('checkpoints/epoch=1-step=1764.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when creating a trainer just for validation, we don't need to fuss over the arguments.\n",
    "trainer = Trainer()\n",
    "trainer.validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "Early stopping, as the name implies, is a technique for stopping training early if a monitored metric is not improving. This can save lots of time and compute resources. We set this up in Lightning using a callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the classifier\n",
    "pt_model = models.Classifier() #models.make_resnet18_model(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# init the lazy layers\n",
    "with torch.no_grad():\n",
    "    pt_model(x)\n",
    "    \n",
    "# create the ligtning model\n",
    "model = models.LitModel(pt_model, lr, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# need to import the early stop callback object\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "earlystop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.005, patience=3, verbose=False)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\"checkpoints/\", save_top_k=3, monitor='val_loss')\n",
    "wandb_logger = WandbLogger(name='Early stopping run')\n",
    "\n",
    "\n",
    "# need to pass the checkpoint callback\n",
    "trainer = Trainer(max_epochs=20, logger=wandb_logger, precision=16, \n",
    "                  callbacks = [earlystop_callback, checkpoint_callback])\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "wandb_logger.experiment.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.response import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    \"Early stopping uses min_delta=0.005 and patience=3. How would you tune these \"\n",
    "    \"hyperparameters for different scenarios: (a) a small dataset with noisy \"\n",
    "    \"validation loss, (b) a large dataset where you expect smooth convergence, (c) a model \"\n",
    "    \"known to have long plateaus before improvement?\", \n",
    "    \"05-01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-gpu\n",
    "\n",
    "### Single-node, multi-gpu\n",
    "Pytorch Lightning makes this very easy. In fact, Lightning will automatically use all available gpus by default. However, it is tricky to get this working in Jupyter notebooks. To demonstrate, we have create a script that you can download [here](https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_advanced/multi_gpu.py). \n",
    "\n",
    "### Multi-node, multi-gpu\n",
    "This is a bit trickier because it involves setting up communication across nodes. We have an example of how to set this up in our Palmetto Examples repository [here](https://github.com/clemsonciti/palmetto-examples/tree/master/PyTorch/Slurm/distributed_data_parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Thank you for attending Advanced Deep Learning in Pytorch! \"\n",
    "                  \"Please take a moment to describe any changes could make this workshop more \"\n",
    "                  \"productive in your view.\", \"05-02\")\n",
    "create_answer_box(\"Please describe any other workshop topics you would like to see covered \"\n",
    "                  \"related to AI and machine learning in future CCIT workshops.\", \"05-03\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21179021fdd249dca874a2cb3215d3e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f00f27414ed4d80ace97ee0bcea32d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "430518b6f83343e0b5e22d32b348d605": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b765f713ef5440ea85c5f12540ebba75",
      "max": 561753746,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_674919a7e1d14f738a6d70ad66a89f4b",
      "value": 561753746
     }
    },
    "4b3178c079fb4eb3800e3b8311caf412": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60c4d3e66e2146cdb1ed443d133ae9ec",
      "placeholder": "​",
      "style": "IPY_MODEL_c7cb437d14a24ecd9551c8f9516577c8",
      "value": " 561753746/561753746 [00:37&lt;00:00, 15679465.54it/s]"
     }
    },
    "60c4d3e66e2146cdb1ed443d133ae9ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "674919a7e1d14f738a6d70ad66a89f4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7907db13bf594b6188ee1321502e01f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f00f27414ed4d80ace97ee0bcea32d6",
      "placeholder": "​",
      "style": "IPY_MODEL_87f7464c25c44cde8088e97126f4190e",
      "value": "100%"
     }
    },
    "87f7464c25c44cde8088e97126f4190e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a594b0a61f9c471f927d1e84bb6b660e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7907db13bf594b6188ee1321502e01f3",
       "IPY_MODEL_430518b6f83343e0b5e22d32b348d605",
       "IPY_MODEL_4b3178c079fb4eb3800e3b8311caf412"
      ],
      "layout": "IPY_MODEL_21179021fdd249dca874a2cb3215d3e9"
     }
    },
    "b765f713ef5440ea85c5f12540ebba75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7cb437d14a24ecd9551c8f9516577c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
