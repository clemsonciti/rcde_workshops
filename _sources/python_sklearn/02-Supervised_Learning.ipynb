{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fce16e-a2ac-4319-b845-6f2a784b533c",
   "metadata": {},
   "source": [
    "# Introduction to ML Concepts\n",
    "\n",
    "Time to dive into some of the core concepts of machine learning! We'll start with a high-level overview of different types of machine learning, then move on to some common machine learning algorithms.\n",
    "\n",
    "## 1. Types of Machine Learning\n",
    "- Supervised Learning: Learning from labeled data\n",
    "  - Examples: classification, regression\n",
    "- Unsupervised Learning: Finding patterns in unlabeled data\n",
    "  - Examples: clustering, dimensionality reduction\n",
    "- Reinforcement Learning: Learning through interaction with an environment\n",
    "  - Examples: game playing, robotics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd707d69-2e17-4eec-83a2-adb7773f3689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import create_answer_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d1656-1f07-4129-8690-be0de9a028a2",
   "metadata": {},
   "source": [
    "## 2. Common ML Algorithms\n",
    "\n",
    "### 2.1. Supervised Learning Algorithms\n",
    "\n",
    "**Supervised learning:** learning a function which approximates the relationship between an input and output from a set of labeled training examples.\n",
    "\n",
    "E.g.:\n",
    "\n",
    "![Home price data](img/hp.png)\n",
    "\n",
    "Our *target* (or dependent variable, or output) is the variable we would like to predict/estimate.\n",
    "\n",
    "Our *features* (or regressors, or independent variables, or inputs, or covariates) are the variables we can use to make our prediction/estimate.\n",
    "\n",
    "In this case, $\\mathrm{Home \\$}\\approx f(\\mathrm{Sq. ft., \\#bed, \\#bath,}\\ldots)$\n",
    "\n",
    "##### Regression vs. Classification\n",
    "\n",
    "Supervised learning can be divided into regression and classification. \n",
    "In the case of **regression**, we estimate a *quantity*.\n",
    "\n",
    "![regression](img/reg.png)\n",
    "\n",
    "In the case of **classification**, we predict a *label* (i.e. a category).\n",
    "\n",
    "![classification](img/class.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae279f9c-bbc9-43e0-bd77-a276e7764b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Have you used/studied linear regression before? Please describe your level of familiarity with it.\", \"02-01\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1df4326d-ec62-41df-959f-4f61bf8f9e75",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "Linear regression assumes that $f$ is just a weighted sum of the variables in the covariate matrix $X$:\n",
    "$$f(X)=\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_px_p + \\epsilon$$\n",
    "Which we can express as just $f(X)=X\\mathbf{\\beta} + \\epsilon$ (and so $Y=X\\mathbf{\\beta}+\\epsilon$).\n",
    "Turns out the best estimate of $\\beta$ is just $(X^TX)^{-1}X^TY$. This is called the Ordinary Least Squares (OLS) estimate. However, that expression sometimes cannot be calculated, and is not computationally efficient to use with large data.\n",
    "\n",
    "In order to apply OLS regression, our problem should obey certain assumptions.\n",
    "1. The linear model is correct.\n",
    "2. The error term Îµ has mean 0.\n",
    "3. The regressors (the $x$ terms) are linearly independent.\n",
    "4. The errors are homoscedastic and uncorrelated.\n",
    "5. The errors are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369285cb-4c59-4a2e-bee0-0ea83dba4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Other than the examples discussed above, what would be an example of a dataset that is appropriate for supervised learning? (If possible, please describe a dataset that you yourself do or may work with in your research!)\", \"02-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d36832-9695-43da-96da-e82d2fe22830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate some synthetic data - in a real problem, we would not know the true relationship described here.\n",
    "X = np.random.uniform(-4, 4, (100, 1))\n",
    "y = X**2 + np.random.normal(0, 3, X.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"R-squared score: {model.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d7906-8aca-4861-a6a0-fbd27a37011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data and the regression line\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X, y, alpha=0.5)\n",
    "plt.plot(X, model.predict(X), color='red')\n",
    "plt.title('Simple Linear Regression Example')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f196a-60c7-42fa-bb38-b7f80f259c8c",
   "metadata": {},
   "source": [
    "Notice a couple of things. One, our fit is abysmal.\n",
    "Two, there *does* nonetheless seem (visually) to be an interesting relationship between the variables. Maybe $y$ is related not directly to $x$, but to some function of $x$.\n",
    "\n",
    "In this case, we can get ideas from visualizing the data, but in most cases, a deep understanding of the data will be necessary to make a good model. E.g., suppose in this case we know that $X$ is wind speed, and $y$ is power generated by a wind turbine. An engineer might tell us that in practice $y$ is typically related to the *square* of $X$, rather than $X$ itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df5c7c-7a09-4ab9-b447-b09d3764686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model where y is regressed on X^2\n",
    "X_squared_train = X_train**2\n",
    "X_squared_test = X_test**2\n",
    "\n",
    "model_squared = LinearRegression()\n",
    "model_squared.fit(X_squared_train, y_train)\n",
    "\n",
    "print(f\"R-squared score (X^2 model): {model_squared.score(X_squared_test, y_test)}\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "# Plotting the data and the regression curve\n",
    "axes[0].scatter(X, y, alpha=0.5)\n",
    "X_curve = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "y_pred = model_squared.predict(X_curve**2)\n",
    "axes[0].plot(X_curve, y_pred, color='red')\n",
    "axes[0].set_title('Linear Regression with X^2')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "\n",
    "# Plotting y_train vs X_squared_train\n",
    "axes[1].scatter(X_squared_train, y_train, alpha=0.5)\n",
    "axes[1].plot(X_squared_train, model_squared.predict(X_squared_train), color='red')\n",
    "axes[1].set_title('y_train vs X_squared_train')\n",
    "axes[1].set_xlabel('X_squared_train')\n",
    "axes[1].set_ylabel('y_train')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d861c0-6323-45e3-b6a8-31c64b75162f",
   "metadata": {},
   "source": [
    "\"Linear\" regression is surprisingly flexible!\n",
    "#### Logistic Regression\n",
    "\n",
    "Despite its name, logistic regression is a powerful algorithm for *classification*. In a binary classification problem, our target can be thought of as being either 1 or 0. It is possible (but not advisable!) to use a regression algorithm, like linear regression, in such a case.\n",
    "\n",
    "Suppose that I have data where the target is a binary indicator for whether a student passed a certain class. The data is the student's score on a that class's first exam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea8451-1214-42ec-bc18-0dfa0a490453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate continuous covariate\n",
    "n_samples = 100\n",
    "covariate = np.random.uniform(0, 10, n_samples)\n",
    "\n",
    "# Generate binary data correlated with covariate\n",
    "probabilities = 1 / (1 + np.exp(-(covariate - 5)))\n",
    "binary_data = np.random.binomial(1, probabilities)\n",
    "\n",
    "# Fit linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(covariate.reshape(-1, 1), binary_data)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(covariate, binary_data, color='blue', alpha=0.6)\n",
    "\n",
    "# Plot linear regression line\n",
    "x_plot = np.linspace(0, 10, 100)\n",
    "y_plot = model.predict(x_plot.reshape(-1, 1))\n",
    "plt.plot(x_plot, y_plot, color='red', lw=2)\n",
    "\n",
    "plt.xlabel('Score on first exam')\n",
    "plt.ylabel('Pass (1) or fail (0)')\n",
    "plt.title('Binary Data vs Continuous Covariate\\nwith Linear Regression Fit')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear Regression Coefficient: {model.coef_[0]:.4f}\")\n",
    "print(f\"Linear Regression Intercept: {model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a141a21-3584-4a28-a7cc-829a4fedb86a",
   "metadata": {},
   "source": [
    "The idea of logistic regression is that instead of directly modeling the target, we instead model the *probability* that the target is 1 or 0. This is a specific type of *generalized linear model*, in which the target is transformed by a *link function*. In this case, the link function is the *logit* function, which is the inverse of the *logistic* function. The logistic function is defined as: $$\\mathrm{logit}(p)=\\log\\left(\\frac{p}{1-p}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e649931-0ae6-4e7a-9e25-4c668979ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit logistic regression\n",
    "logistic_model = LogisticRegression(C=0.999)\n",
    "logistic_model.fit(covariate.reshape(-1, 1), binary_data)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(covariate, binary_data, color='blue', alpha=0.6, label='Data')\n",
    "\n",
    "# Plot linear regression line\n",
    "\n",
    "x_plot = np.linspace(0, 10, 100)\n",
    "y_linear = model.predict(x_plot.reshape(-1, 1))\n",
    "plt.plot(x_plot, y_linear, color='red', lw=2, label='Lin. Reg.')\n",
    "\n",
    "# Plot logistic regression curve\n",
    "y_logistic = logistic_model.predict_proba(x_plot.reshape(-1, 1))[:, 1]\n",
    "plt.plot(x_plot, y_logistic, color='green', lw=2, label='Log. Reg.')\n",
    "\n",
    "plt.xlabel('Score on first exam')\n",
    "plt.ylabel('Pass (1) or fail (0)')\n",
    "plt.title('Binary Data vs Continuous Covariate:\\nLinear and Logistic Regression')\n",
    "plt.legend(loc='center left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(-0.1, 1.1)  # Set y-axis limits for better visualization\n",
    "plt.show()\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"Coefficient: {model.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {model.intercept_:.4f}\")\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(f\"Coefficient: {logistic_model.coef_[0][0]:.4f}\")\n",
    "print(f\"Intercept: {logistic_model.intercept_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bdf3b0-1dca-4250-9243-4923cbaf9d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_answer_box(\"Logistic regression can take a *regularization term*, `C`. Try giving a few values of `C` above (try values in the range `[0,2]`, by putting `C=[value]` in the call to `LogisticRegression()`) and examine the result on the model fit in the plot. What effect does C appear to have on the fit?\", \"02-03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06aa2a-b0ba-4959-bca4-f6ca5decea07",
   "metadata": {},
   "source": [
    "#### Decision Trees\n",
    "\n",
    "A totally different approach to modeling data is to use a decision tree. A decision tree is a tree-like model of the dataset. It is a simple model that is easy to interpret and understand. It is also a non-parametric model, which means that it makes no assumptions about the shape of the data - sometimes a big advantage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38b671e-d3fa-42de-822a-ed63469b853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's make some data.\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'income': np.random.randint(20000, 200000, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples),\n",
    "    'debt_to_income': np.random.uniform(0, 0.6, n_samples),\n",
    "    'employment_length': np.random.randint(0, 30, n_samples),\n",
    "    'loan_amount': np.random.randint(5000, 100000, n_samples)\n",
    "})\n",
    "\n",
    "# Create a rule-based target variable\n",
    "data['loan_approved'] = (\n",
    "    (data['credit_score'] > 700) & \n",
    "    (data['debt_to_income'] < 0.4) & \n",
    "    (data['income'] > 50000)\n",
    ").astype(int)\n",
    "\n",
    "# Prepare features and target\n",
    "X = data[['income', 'credit_score', 'debt_to_income', 'employment_length', 'loan_amount']]\n",
    "y = data['loan_approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e327408-98bb-4cee-8571-ef865c9d0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Decision Tree\n",
    "dt = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(6, 6))\n",
    "plot_tree(dt, filled=True, feature_names=X.columns.to_list(), class_names=['Denied', 'Approved'], rounded=True, impurity=False, proportion=True, precision=2)\n",
    "plt.title(\"Loan Approval Decision Tree\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Decision Tree created and visualized based on the loan approval data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729bebb-57db-46db-92f1-5db9b067b937",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "The decision tree is highly interpretable, which is sometimes a big advantage. But, it is a weak machine learning algorithm; not nearly as powerful as some others. One way to make it more powerful is to use a *random forest*. A random forest is an ensemble of decision trees, which means that it is a collection of decision trees that are trained separately and then combined to make a prediction. Essentially, a random forest is a collection of decision trees that are trained separately and then combined (by averaging or voting) to make a prediction. They can be used for both classification and regression tasks.\n",
    "\n",
    "You lose the interpretability of a single decision tree, but you gain a lot of predictive power. And random forests are very easy to use and very flexible, don't require much tuning, are very hard to overfit, and don't make many assumptions about the data. A good general-purpose algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7e9ac-b0ae-460e-b5eb-b9357058e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Create and train the Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=2, random_state=355)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f132ae-adba-4935-ba9d-9a484ae78ab9",
   "metadata": {},
   "source": [
    "Random Forests makes it easy to see which features are most important in making a prediction. This is because the algorithm can keep track of how much each feature contributes to the trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a043f-c56e-4fc2-a641-d4ba673ed464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importances\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.bar(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.title(\"Feature Importance in Random Forest\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323ebf4-f725-4112-b1d8-779c2250368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot two trees side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "# First tree\n",
    "plot_tree(rf.estimators_[0], \n",
    "          filled=True, \n",
    "          feature_names=X.columns.to_list(), \n",
    "          class_names=['Denied', 'Approved'], \n",
    "          rounded=True,\n",
    "          impurity=False,\n",
    "          proportion=False,\n",
    "          node_ids=False,\n",
    "          precision=0,\n",
    "          ax=axes[0])\n",
    "axes[0].set_title(\"Tree 1 from Random Forest\")\n",
    "\n",
    "# Second tree\n",
    "plot_tree(rf.estimators_[1], \n",
    "          filled=True, \n",
    "          feature_names=X.columns.to_list(), \n",
    "          class_names=['Denied', 'Approved'], \n",
    "          rounded=True,\n",
    "          impurity=False,\n",
    "          proportion=False,\n",
    "          node_ids=False,\n",
    "          precision=0,\n",
    "          ax=axes[1])\n",
    "axes[1].set_title(\"Tree 2 from Random Forest\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219edfa-f45f-46f6-95d8-276e7c698b4f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) are a powerful supervised learning algorithm used for classification or regression tasks. They are based on the idea of finding the hyperplane that best divides a dataset into two classes. The hyperplane is the line that best separates the two classes. The SVM algorithm finds the hyperplane that maximizes the margin between the two classes.\n",
    "\n",
    "![hyperplanes](img/svm_hp.png)\n",
    "\n",
    "In cases where the data cannot be linearly separated, SVMs can use a *kernel trick* to transform the data into a higher-dimensional space where it can be separated. This is a very powerful technique that allows SVMs to work well on a wide variety of datasets.\n",
    "\n",
    "![kernel](img/svm_hd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4d3dc-de92-4895-9010-2e5ba8a441eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's prepare some data.\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]  # We'll use petal length and width\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=355)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2f012-cec4-4461-8733-88508b578750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's train a Support Vector Machine (SVM) classifier.\n",
    "\n",
    "# Train SVM classifier\n",
    "svm_classifier = svm.SVC(kernel='rbf', random_state=42)\n",
    "svm_classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef54b8b5-cafe-4e02-b2f0-88224c0f7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's make predictions and evaluate the model.\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Print the accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Visualize the decision boundaries\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = plt.cm.RdYlBu\n",
    "\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, color=cmap(idx / len(np.unique(y))),\n",
    "                    marker=markers[idx], label=iris.target_names[cl])\n",
    "\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='none', alpha=1.0, linewidth=1, marker='o', \n",
    "                    s=55, edgecolors='black', label='test set')\n",
    "\n",
    "# Visualize the results\n",
    "X_combined = np.vstack((X_train_scaled, X_test_scaled))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X_combined, y_combined, classifier=svm_classifier, test_idx=range(len(y_train), len(y_combined)))\n",
    "\n",
    "plt.xlabel('Petal length (standardized)')\n",
    "plt.ylabel('Petal width (standardized)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('SVM Decision Regions - Iris Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372000b-e70b-4055-b37c-dcdbb9427cb9",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbors\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, easy-to-understand algorithm that can be used for both classification and regression tasks. It is a non-parametric algorithm, which means that it makes no assumptions about the shape of the data. The basic idea behind KNN is that similar data points are close to each other in the feature space. To make a prediction, KNN looks at the K-nearest neighbors of a data point and takes a majority vote (for classification) or an average (for regression) to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36ceb0-96b3-4b42-8175-1583ee8a9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# Train KNN classifier on the iris data\n",
    "k = 5  # number of neighbors\n",
    "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "knn_classifier.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0edd3c-3bd1-486d-b1a9-eedebc1b97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Print the accuracy and classification report\n",
    "print(f\"KNN (k={k}) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Visualize the decision boundaries\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = plt.cm.RdYlBu\n",
    "\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
    "                    alpha=0.8, color=cmap(idx / len(np.unique(y))),\n",
    "                    marker=markers[idx], label=iris.target_names[cl])\n",
    "\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c='none', alpha=1.0, linewidth=1, marker='o', \n",
    "                    s=55, edgecolors='black', label='test set')\n",
    "\n",
    "# Visualize the results\n",
    "X_combined = np.vstack((X_train_scaled, X_test_scaled))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X_combined, y_combined, classifier=knn_classifier, test_idx=range(len(y_train), len(y_combined)))\n",
    "\n",
    "plt.xlabel('Petal length (standardized)')\n",
    "plt.ylabel('Petal width (standardized)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(f'KNN (k={k}) Decision Regions - Iris Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed7bd1b-663c-4fc0-9f95-de9583d76114",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Try re-running the above two code cells a few times with different values of `k`. What effect do different values of `k` appear to have on the resulting fit, in general?\", \"02-04\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
