{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca451b6-a01f-4fe5-b6bf-0e2fe42dfe02",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Algorithms\n",
    "\n",
    "**Unsupervised learning:** finding patterns in data without any labels.\n",
    "\n",
    "Common types of unsupervised learning algorithms include:\n",
    "* Clustering: grouping similar data points together\n",
    "* Dimensionality reduction: reducing the number of features in a dataset\n",
    "* Anomaly detection: finding outliers in a dataset\n",
    "\n",
    "## 1. Clustering\n",
    "\n",
    "Clustering is a common way to discover patterns and subgroups that are interesting in our data. For example, we might want to group customers into segments based on their purchasing behavior, or group documents based on their content. There are many different clustering algorithms. See just a few of those implemented in Scikit-Learn:\n",
    "\n",
    "<img src=\"img/skl_cl.png\" alt=\"clustering\" width=\"85%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec4bd5-ccd7-43ae-b1f3-17832a18ffad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import create_answer_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ce66f-9337-4758-aa04-978c3f807707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_answer_box(\"Describe a dataset that would be appropriate for unsupervised learning. (If possible, describe a dataset you do or may work with in your research!)\", \"03-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d949f-223b-44d8-b310-23920cb74791",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "\n",
    "![By Chire - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=59409335](img/kmc.gif)\n",
    "\n",
    "K-Means is a simple and popular clustering algorithm that is used to partition a dataset into K clusters. The algorithm works by iteratively assigning data points to clusters based on the distance between the data point and the cluster center. The cluster center is then updated to be the mean of all the data points assigned to that cluster. This process is repeated until the algorithm converges.\n",
    "\n",
    "We can look at this in action using the Iris dataset, which is a dataset containing information about iris flowers. The dataset contains 150 samples and 4 features. The goal is to cluster the flowers into different groups based on their features. Though we actually know the flower species, for this example we imagine that we don't -- we just have the features, and we want to see if we can group the flowers into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231a53a-c641-4e27-9a7e-afcbf6f73088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]  # We'll use petal length and width\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform K-means clustering\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=355)\n",
    "y_kmeans = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Evaluate the clustering\n",
    "print(\"Silhouette Score:\", silhouette_score(X_scaled, y_kmeans))\n",
    "print(\"Calinski-Harabasz Index:\", calinski_harabasz_score(X_scaled, y_kmeans))\n",
    "\n",
    "# Compare with true labels\n",
    "print(\"\\nComparison with true labels:\")\n",
    "print(\"Adjusted Rand Index:\", adjusted_rand_score(iris.target, y_kmeans))\n",
    "print(\"Normalized Mutual Information:\", normalized_mutual_info_score(iris.target, y_kmeans))\n",
    "\n",
    "# Define the centroids of the clusters\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Create a mapping function. This is only here to make the plots consistent in color.\n",
    "def map_labels(y_kmeans, y_true):\n",
    "    mapping = {}\n",
    "    for k in range(n_clusters):\n",
    "        k_indices = y_kmeans == k\n",
    "        k_true = y_true[k_indices]\n",
    "        mapping[k] = np.bincount(k_true).argmax()\n",
    "    return np.array([mapping[k] for k in y_kmeans])\n",
    "\n",
    "# Apply the mapping\n",
    "y_kmeans_mapped = map_labels(y_kmeans, iris.target)\n",
    "\n",
    "# Plot comparison with true labels\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans_mapped, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "plt.title('K-means Clustering')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.subplot(122)\n",
    "scatter = plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=iris.target, cmap='viridis', alpha=0.7)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "plt.title('True Labels')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071027df-e4ba-4849-bbc8-3eb5703b1e76",
   "metadata": {},
   "source": [
    "Now let's see K-Means in action on a more complex dataset. We'll use the Labeled Faces in the Wild dataset, which is a dataset containing images of faces. The dataset contains 13,233 sample images. We'll use K-Means to cluster the faces into different groups based on their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688d857-b656-441a-b861-6960aa1ea705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn import cluster, decomposition\n",
    "\n",
    "lfw_people = fetch_lfw_people()\n",
    "lfw_people.data.shape\n",
    "n_samples, n_features = lfw_people.data.shape\n",
    "\n",
    "# Global centering (focus on one feature, centering all samples)\n",
    "faces_centered = lfw_people.data - lfw_people.data.mean(axis=0)\n",
    "\n",
    "# Local centering (focus on one sample, centering all features)\n",
    "faces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f755e54-eb4d-4578-ac29-3a9c36691f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd = lfw_people.data\n",
    "# # dd = lfw_people.data - lfw_people.data.mean(axis=0)\n",
    "# dd -= dd.mean(axis=1).reshape(n_samples, -1)\n",
    "# plot_gallery(\"Faces from dataset\", dd[:n_components])\n",
    "lfw_people.data.mean(axis=1).shape\n",
    "lfw_people.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec2c50-e5ef-4d32-acd9-3081af2b9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row, n_col = 2, 3\n",
    "n_components = n_row * n_col\n",
    "image_shape = (62, 47)\n",
    "\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=n_row,\n",
    "        ncols=n_col,\n",
    "        figsize=(2.0 * n_col, 2.3 * n_row),\n",
    "        facecolor=\"white\",\n",
    "        constrained_layout=True,\n",
    "    )\n",
    "    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n",
    "    fig.set_edgecolor(\"black\")\n",
    "    fig.suptitle(title, size=16)\n",
    "    for ax, vec in zip(axs.flat, images):\n",
    "        vmax = max(vec.max(), -vec.min())\n",
    "        im = ax.imshow(\n",
    "            vec.reshape(image_shape),\n",
    "            cmap=cmap,\n",
    "            interpolation=\"nearest\",\n",
    "            vmin=-vmax,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac9019-2e44-4f5e-8a33-d345edc4e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(\"Faces from dataset\", faces_centered[:n_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d06b7a-c72f-4d1b-980b-23978b721997",
   "metadata": {},
   "source": [
    "Now let's apply K-Means to the faces dataset. We can then examine the cluster centroids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b8419-582e-43c3-a735-19d2aa760c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_estimator = cluster.MiniBatchKMeans(\n",
    "    n_clusters=24,\n",
    "    tol=1e-3,\n",
    "    batch_size=1000,\n",
    "    max_iter=1000,\n",
    "    random_state=355,\n",
    ")\n",
    "kmeans_estimator.fit(faces_centered)\n",
    "plot_gallery(\n",
    "    \"Cluster centers - MiniBatchKMeans\",\n",
    "    kmeans_estimator.cluster_centers_[:n_components],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa7bdd-c1fb-4fd5-a35e-683462e80506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_answer_box(\"We're looking at the \\\"centroid\\\" for each cluster of faces. But what does that mean, in non-technical terms? What do these images represent about our data?\", \"03-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2413604f-e1f2-45ff-bc0f-4f1a8d3b6890",
   "metadata": {},
   "source": [
    "## 2. Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of features in a dataset. This can be useful for a number of reasons, including:\n",
    "* Reducing the computational cost of working with high-dimensional data\n",
    "* Reducing the noise in the data\n",
    "* Visualizing high-dimensional data in a lower-dimensional space\n",
    "\n",
    "### PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is a popular dimensionality reduction technique often used for the first two reasons above. It works by finding the directions (or principal components) in which the data varies the most. These directions are then used to transform the data into a lower-dimensional space. PCA is often used before applying other machine learning algorithms to the data, including supervised learning algorithms.\n",
    "\n",
    "Why is PCA used this way? Imagine you could construct your ideal ML data set for some phenomenon you’re studying. Three things you’d want:\n",
    "* High variance features that are\n",
    "* Uncorrelated, and are also\n",
    "* Few in number.\n",
    "\n",
    "PCA makes your data more aligned with these three goals. It replaces your original features with new ones that are uncorrelated and ordered by how much variance they explain. So you can keep only the few features that explain most of the variance, and throw away the rest. PCA is implicitly lossy compression.\n",
    "\n",
    "![pca](img/pca.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e0fd9-d44d-4541-b8d6-a4acae479e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming faces_centered is already defined as per your previous code\n",
    "\n",
    "# Perform PCA on the entire dataset\n",
    "pca = PCA().fit(faces_centered)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Define the variance levels we want to visualize\n",
    "variance_levels = [1.0, 0.99, 0.95, 0.9, 0.75, 0.5]\n",
    "\n",
    "# Find the number of components needed for each variance level\n",
    "n_components_list = [np.argmax(cumulative_variance_ratio >= level) + 1 for level in variance_levels]\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Function to reconstruct and plot a face\n",
    "def plot_face(ax, face, n_components, variance):\n",
    "    if n_components == 1:\n",
    "        n_components = face.shape[0]\n",
    "    pca_partial = PCA(n_components=n_components)\n",
    "    face_pca = pca_partial.fit_transform(faces_centered)\n",
    "    face_approximation = pca_partial.inverse_transform(pca_partial.transform(face.reshape(1, -1)))\n",
    "    ax.imshow(face_approximation.reshape(lfw_people.images[0].shape), cmap='gray')\n",
    "    ax.set_title(f'{variance:.0%} variance\\n({n_components} components)')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Choose a random face\n",
    "random_face_index = np.random.randint(faces_centered.shape[0])\n",
    "face = faces_centered[random_face_index]\n",
    "\n",
    "# Plot the face at different variance levels\n",
    "for ax, n_components, variance in zip(axes, n_components_list, variance_levels):\n",
    "    plot_face(ax, face, n_components, variance)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print out the number of components for each variance level\n",
    "for variance, n_components in zip(variance_levels, n_components_list):\n",
    "    print(f\"{variance:.0%} variance explained by {n_components} components\")\n",
    "\n",
    "# Print the total number of samples and features\n",
    "print(f\"\\nDataset consists of {faces_centered.shape[0]} faces\")\n",
    "print(f\"Each face has {faces_centered.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf8730c-dee5-4157-b288-43d68c7b9aee",
   "metadata": {},
   "source": [
    "### T-SNE\n",
    "\n",
    "Dimensionality reduction is also used to visualize high-dimensional data in a lower-dimensional space. One popular technique for this is t-distributed Stochastic Neighbor Embedding (t-SNE). It works by modeling the similarity between data points in the high-dimensional space and the low-dimensional space. t-SNE is often used to visualize high-dimensional data in two or three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604bf22c-38b1-44f2-b9c8-695b0d12e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='auto')\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# Convert target to integer\n",
    "y = y.astype(int)\n",
    "\n",
    "# Take a subset of the data to speed up computation\n",
    "n_samples = 5000\n",
    "random_idx = np.random.choice(X.shape[0], n_samples, replace=False)\n",
    "X_subset = X[random_idx]\n",
    "y_subset = y[random_idx]\n",
    "\n",
    "# Normalize the data\n",
    "X_normalized = X_subset / 255.0\n",
    "\n",
    "# Apply t-SNE\n",
    "print(\"Applying t-SNE...\")\n",
    "tsne = TSNE(n_components=2, random_state=355, max_iter=1000, learning_rate='auto', init='pca')\n",
    "X_tsne = tsne.fit_transform(X_normalized)\n",
    "\n",
    "# Create a color map\n",
    "num_classes = len(np.unique(y))\n",
    "colors = plt.cm.jet(np.linspace(0, 1, num_classes))\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(6, 5))\n",
    "scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_subset, cmap=cmap, alpha=0.7, s=5)\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"t-SNE visualization of MNIST dataset\")\n",
    "plt.xlabel(\"t-SNE feature 1\")\n",
    "plt.ylabel(\"t-SNE feature 2\")\n",
    "\n",
    "# Add a legend\n",
    "legend_elements = [plt.Line2D([0], [0], marker='o', color='w', label=f'Digit {i}', \n",
    "                   markerfacecolor=colors[i], markersize=10) for i in range(num_classes)]\n",
    "plt.legend(handles=legend_elements, loc='best', title=\"Digits\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to plot some example digits\n",
    "def plot_example_digits(X, y, num_examples=10):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(num_examples):\n",
    "        ax = plt.subplot(1, num_examples, i + 1)\n",
    "        plt.imshow(X[i].reshape(28, 28), cmap='gray_r')\n",
    "        plt.title(f\"Digit: {y[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot example digits\n",
    "plot_example_digits(X_subset, y_subset)\n",
    "\n",
    "print(\"t-SNE visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d46500-bfb6-4bff-86ab-4bd450c9e7e0",
   "metadata": {},
   "source": [
    "## Coding challenge\n",
    "Let's put in practice some of the skills you've learned so far. Load the California housing data, then perform PCA on it to keep just two dimensions of the features. Then, use this PCA-reduced dataset to perform linear regression with housing prices as the target (/dependent variable). When you're finished, report your resulting model R-squared score in the text box. (Feel free to make changes, e.g. data preprocessing steps etc., if you like.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48553b-6819-4f36-b35f-bf789fb0235f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Load the California housing dataset\n",
    "# Hint: from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Step 2: Split the data into features (X) and target (y)\n",
    "\n",
    "# Step 3: Apply PCA to reduce the feature set to 2 components\n",
    "# Hint: from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 4: Perform linear regression on the PCA-reduced dataset\n",
    "# Hint: from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Step 5: Evaluate the model using R-squared score\n",
    "# Hint: Use model.score(X_pca, y)\n",
    "\n",
    "# Step 6: Print the R-squared score\n",
    "# This is the value you will enter into the text box\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e245a3-1c4d-47e2-a08f-ceea44efb61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_answer_box(\"Enter your model's R-squared score.\", \"03-03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df7892f-4c04-4f24-8a3c-e79231538994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_answer_box(\"Try re-running your code with different numbers of PCA components. What do the results of this investigation tell you?\", \"03-04\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
