{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc8382d-f45f-403e-a803-d08b763705c5",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "So far we've assumed that we already have access to a dataset that is ready to be used for training a machine learning model. However, in practice, this is rarely the case. Most of the time, the dataset will need to be cleaned, transformed, and prepared before it can be used for training.\n",
    "\n",
    "**Stages of data preparation:**\n",
    "1. Data collection\n",
    "2. Data loading\n",
    "3. Data exploration\n",
    "4. Data cleaning\n",
    "5. Feature selection and engineering\n",
    "7. Encoding categorical variables\n",
    "8. Feature scaling\n",
    "9. Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7007a43c-61a5-4b07-b558-661f6c8fb09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346633e5-67fa-45f3-9dc6-11bc91592a8f",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Data collection is the process of gathering data from various sources. The data can be collected from databases, files, APIs, web scraping, etc. Outside of the scope of this notebook, but it's an important step in the data preparation process.\n",
    "\n",
    "## 2. Data Loading\n",
    "\n",
    "In this step, we load the data into the working environment. Your data might be in a CSV file, a JSON file, a SQL database, or any other format. We will use the `pandas` library to load the data into a DataFrame. Let's use the \"Heart Failure Clinical Records\" dataset from the UCI Machine Learning Repository. This dataset contains the medical records of patients who had heart failure, collected during their follow-up period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6975b1-ee0c-4339-b857-807a8f4cf4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv\"\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605a21f-7ba3-41ad-9049-c07c121e84f1",
   "metadata": {},
   "source": [
    "Note that we're lucky in this dataset, in that it is small enough to fit into memory. For larger datasets, we would need to use more advanced techniques to load the data in chunks. For example, we might use Dask rather than pandas, which would allow us to work with larger-than-memory datasets. (Dask is designed to be a drop-in replacement for pandas for larger-than-memory datasets, so the code would look very similar.)\n",
    "\n",
    "For very large datasets, when performing data exploration and when producing the code that will be used to prepare your data, you might want to work with a sample of the data rather than the full dataset. This will allow you to iterate more quickly and avoid long wait times. Once you have the code working with a sample, you can then run it on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43753a3-9c77-4e0a-bfd8-112abd1e0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load only a subset of the dataset\n",
    "small_df = pd.read_csv(url, nrows=20)\n",
    "\n",
    "# Display the shape of the datasets\n",
    "print(df.shape)\n",
    "print(small_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c8055-0e86-417d-9f28-e58a5df9a1bc",
   "metadata": {},
   "source": [
    "Working with a smaller subset dataset is more computationally efficient, but there's the danger that the smaller subset distorts the data in some way. Use `pandas`' method `.describe()` in order to check the mean and standard deviations of each of the columns of `df` and `small_df`. Are any of the columns of `small_df` glaringly different than their `df` counterparts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549cf21-594f-43de-9dc7-de8656e326ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code cell (and others if you want to create more) to investigate the dataframes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470a17c6-7938-483a-bb06-01e42712580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Does the data in `small_df` seem similar to that in `df`? Which column(s), if any, seem distorted in `small_df`?\", \"05-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb3d8a-92ed-4a85-84af-980c1e37964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll work with the full dataset, so let's delete the small_df\n",
    "del small_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e7038-efc8-45fb-94d8-87ca8cc93d22",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Data exploration is the process of getting to know the data. We look at the structure of the data, the summary statistics, and the distribution of the data. We also look for missing values, outliers, and anomalies in the data. This step is crucial for understanding the data and making decisions about how to clean and transform it.\n",
    "\n",
    "It cannot be overemphasized that there is no one-size-fits-all approach to data exploration. The process will depend on the dataset, the problem you are trying to solve, and the questions you are trying to answer. A thorough understanding of the data -- its sources, its structure, its quality -- is essential for building a successful machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1caba9-e589-40be-9aa8-835f181753bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some basic information about the dataset\n",
    "\n",
    "# Display the column names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1451a14-2c33-4954-abaf-0bc036d33991",
   "metadata": {},
   "source": [
    "If you were really working with this dataset for research purposes, you should know what each of these columns represents, as well as the units in which they are measured. That knowledge is crucial both for knowing how best to make use of the data, as well as for detecting problems in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81347f5-0360-4d50-b8fd-b772649e2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(df.info())\n",
    "print(\"\\nSample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91f86ad-1e61-4b1f-8106-0664782b0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b99f5-b87e-4191-8dd4-594a98c7bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real data is very clean already. Let's make it messier (artificially), so we can then talk about how to clean it up.\n",
    "\n",
    "# Let's introduce some missing values, and also some outliers.\n",
    "# We'll do this by randomly selecting some values and setting them to NaN, and also by adding some random noise to some values.\n",
    "\n",
    "# Randomly select 10% of the data and set them to NaN\n",
    "df_nan = df.copy()\n",
    "nan_indices = np.random.choice(df.index, size=int(len(df)*0.1), replace=False)\n",
    "df_nan.loc[nan_indices, 'age'] = np.nan\n",
    "df_nan.loc[nan_indices, 'serum_creatinine'] = np.nan\n",
    "df_nan.loc[nan_indices, 'ejection_fraction'] = np.nan\n",
    "\n",
    "# Randomly select 2% of the data and make them outliers\n",
    "df_noisy = df_nan.copy()\n",
    "noisy_indices = np.random.choice(df.index, size=int(len(df)*0.02), replace=False)\n",
    "df_noisy.loc[noisy_indices, 'serum_creatinine'] = df_noisy.loc[noisy_indices, 'serum_creatinine'] * 10\n",
    "df_noisy.loc[noisy_indices, 'ejection_fraction'] = df_noisy.loc[noisy_indices, 'ejection_fraction'] * 10\n",
    "\n",
    "# Let's also make our data less clean by making one of our categorical variables non-numeric.\n",
    "# Right now, \"sex\" is represented by 0 and 1 values. Let's change that to \"male\" and \"female\", \n",
    "# since that kind of string data type is a common feature of datasets.\n",
    "df_noisy[\"sex\"] = df_noisy[\"sex\"].map({0: 'male', 1: 'female'})\n",
    "\n",
    "# Display the first few rows of the noisy dataset\n",
    "print(\"\\nNoisy dataset:\")\n",
    "df_noisy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1796d4f-3f1f-4f99-8693-68290dfa976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df_noisy.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b1f30-7a89-4931-a637-f04c1d479836",
   "metadata": {},
   "source": [
    "Suppose that our target variable is the \"DEATH_EVENT\" column, which indicates whether the patient died during the follow-up period. We will explore the data to understand the relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3056a3e8-366f-4504-85d2-ca448e773188",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 2))\n",
    "df_noisy['DEATH_EVENT'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Death Events')\n",
    "plt.xlabel('Death Event')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Survived', 'Died'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Percentage of deaths:\", (df['DEATH_EVENT'].sum() / len(df)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e65e04-45f4-4b94-bf89-8aec6189b315",
   "metadata": {},
   "source": [
    "Let's look at the age distribution across the dataset. While we're at it, let's break it down by our target variable, `DEATH_EVENT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b55766-6394-4558-9dc7-8e377181957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "sns.histplot(data=df_noisy, x='age', hue='DEATH_EVENT', kde=True, multiple=\"stack\")\n",
    "plt.title('Age Distribution by Outcome')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(\"Average age of survivors:\", df_noisy[df_noisy['DEATH_EVENT'] == 0]['age'].mean())\n",
    "print(\"Average age of non-survivors:\", df_noisy[df_noisy['DEATH_EVENT'] == 1]['age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98da1c-7d38-41e2-a628-272db781b915",
   "metadata": {},
   "source": [
    "Very frequently, a correlation heatmap is a good way to get a quick overview of the relationships between the features in the dataset. Note that this assumes that the features are all numeric. If you have categorical features, you will need to encode them as numbers before you can use a correlation heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b78aa-21f4-446b-bbdb-163b19ebafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(df_noisy.drop(columns='sex').corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d82b2-6307-47b3-bc6f-b86682e72edb",
   "metadata": {},
   "source": [
    "You might want to look at scatterplots to see if you pick up on any patterns in the data (or verify that patterns you expect to be there are really there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a1d84-8b47-4e21-9353-5d9fab44a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "sns.scatterplot(data=df_noisy, x='ejection_fraction', y='serum_creatinine', hue='DEATH_EVENT')\n",
    "plt.title('Ejection Fraction vs. Serum Creatinine')\n",
    "plt.xlabel('Ejection Fraction (%)')\n",
    "plt.ylabel('Serum Creatinine (mg/dL)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5d5af4-f35b-43a0-94d9-8fba88179bf0",
   "metadata": {},
   "source": [
    "What! There are some datapoints that look like bad outliers, before we even check what they are. And in this case, they are percentages that are well above 100%. Something is definitely wrong with those datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e3318-8cb4-4ff6-ae86-cd73da47ad44",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "First we'll check for and handle outliers. We need to be thoughtful about this step! Every choice we make says something about how we expect the future data to look, and what we think is the reason why we have outliers in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4acdd-ccca-4225-9f6e-6d3d4365f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df_noisy[(df_noisy[column] < lower_bound) | (df_noisy[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# Example: Check outliers in 'serum_creatinine'\n",
    "creatinine_outliers = detect_outliers(df_noisy, 'serum_creatinine')\n",
    "print(\"Outliers in serum_creatinine:\")\n",
    "print(creatinine_outliers[['age', 'sex', 'serum_creatinine', 'DEATH_EVENT']])\n",
    "\n",
    "# Remove outliers (be cautious with this step in real-world scenarios!)\n",
    "df_cleaned = df_noisy[~df_noisy.index.isin(creatinine_outliers.index)]\n",
    "\n",
    "print(\"\\nDataset shape after cleaning:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075bb1e-426c-4d6a-89b8-971d0194ab40",
   "metadata": {},
   "source": [
    "Similarly, we must handle missing values in a way that is thoughtful about why the values might be missing. Are they missing at random? Are they missing because they are not applicable? Are they missing because they were never recorded? The answers to these questions will affect how we handle the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1463a-d438-4848-b6bb-f9303c74750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "\n",
    "# Fill missing values with the mean\n",
    "df_cleaned = df_cleaned.fillna(df_cleaned.select_dtypes(include='number').mean())\n",
    "\n",
    "# Alternatively, fill missing values with the median\n",
    "# df_imputed = df_cleaned.fillna(df_cleaned.select_dtypes(include='number').median())\n",
    "\n",
    "# Or we can fill missing values with the mode\n",
    "# df_imputed = df_cleaned.fillna(df_cleaned.mode().iloc[0])\n",
    "\n",
    "# We could also fit a machine learning model to predict missing values, but this is more complex and not always necessary.\n",
    "\n",
    "# Or, just drop the missing rows. Which strategy makes the most sense depends crucially on your particular problem! No one-size-fits-all solutions.\n",
    "\n",
    "# Check if there are any missing values left\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4d3f8-0c1f-4bc2-adc9-09e47d9ee6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Describe a situation in which it would make more sense to drop rows with missing values, rather than fill them with the mean(/median/mode).\", \"05-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c04bf9-e1c1-46a4-af57-e2c280cf4942",
   "metadata": {},
   "source": [
    "## 5. Feature Selection and Engineering\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features for use in model training. Feature engineering is the process of creating new features from the existing features in the dataset. Both of these processes are crucial for building a successful machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500145b5-f595-487a-a765-1bab2973ddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age groups\n",
    "df_cleaned['age_group'] = pd.cut(df_cleaned['age'], bins=[30, 50, 70, 100], labels=['Middle-aged', 'Senior', 'Elderly'])\n",
    "\n",
    "# Create a feature for multiple conditions\n",
    "df_cleaned['multiple_conditions'] = ((df_cleaned['diabetes'] + df_cleaned['high_blood_pressure'] + df_cleaned['smoking']) > 1).astype(int)\n",
    "\n",
    "# Log transform skewed features\n",
    "df_cleaned['log_creatinine'] = np.log1p(df_cleaned['creatinine_phosphokinase'])\n",
    "\n",
    "# Interaction terms\n",
    "df_cleaned['ef_creatinine_interaction'] = df_cleaned['ejection_fraction'] * df_cleaned['serum_creatinine']\n",
    "\n",
    "print(\"New features added:\")\n",
    "print(df_cleaned[['age_group', 'multiple_conditions', 'log_creatinine', 'ef_creatinine_interaction']].head())\n",
    "\n",
    "# Visualize the effect of a new feature\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='age_group', y='serum_creatinine', hue='DEATH_EVENT', data=df_cleaned)\n",
    "plt.title('Serum Creatinine by Age Group and Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc336e98-bf7e-4930-995d-78f83b32ba0d",
   "metadata": {},
   "source": [
    "## 6. Encoding Categorical Variables\n",
    "\n",
    "Most machine learning algorithms require that the input data be in numerical format. If the dataset contains categorical variables, we need to encode them into numerical format. One issue to which to be particularly sensitive is whether it is appropriate to treat a categorical variable as ordinal or nominal. If it is ordinal, then we should encode it as such. If it is nominal, then we should use \"one-hot encoding\".\n",
    "\n",
    "For example, suppose that our \"age group\" column were the only information about age that we have. If we treat it as ordinal, then we are saying that the different age groups are ordered in some way. If we treat it as nominal, then we are saying that the different age groups are not ordered in any way. Which seems appropriate here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c22314-7ede-4a7c-996a-3f3e73440571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the `age_group` feature\n",
    "df_encoded = df_cleaned.copy()\n",
    "df_encoded['age_group_ordinal'] = df_cleaned['age_group'].cat.codes\n",
    "\n",
    "# One-hot encode the `age_group` feature\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['age_group'], drop_first=False)\n",
    "\n",
    "df_encoded[[col for col in df_encoded.columns if 'age_group' in col]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095b47b3-8775-46d6-bd7f-16e6054f8390",
   "metadata": {},
   "source": [
    "Matters are simpler if a column has only two values, because then the column can be numerically encoded simply by converting it to a column of zeros and ones -- there is no difference between ordinal and nominal encodings here. Use the below code cell to verify that in this dataset, there are only two values of the column `sex`. Then, write a line of code that will replace the string categories with ones and zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b46b45-cb49-4aad-9dcc-bd577eb780be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many unique values there are in the `sex` column. You might want pandas' `.unique` method.\n",
    "\n",
    "\n",
    "# Write a line of code to replace the categories in the `sex` column with ones and zeros. There are many ways to do this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a2f6a-e4cb-450b-8f90-96b15b024663",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Please copy the line of code you used to convert the `sex` column to a numeric encoding.\", \"05-03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5465a9-0e7b-4e8b-8d0c-edbd9159fa15",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling\n",
    "\n",
    "Feature scaling is the process of standardizing the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. This is helpful for algorithms that rely on the magnitude of values, such as distance-based algorithms. Even when it doesn't help the algorithm, it rarely hurts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ec819-dc9f-439c-89da-67c12a5755a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = df_cleaned.copy()\n",
    "df_scaled[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time', 'log_creatinine', 'ef_creatinine_interaction']] = scaler.fit_transform(df_encoded[['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium', 'time', 'log_creatinine', 'ef_creatinine_interaction']])\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d919254-5081-41ee-9fc6-8e5f2834a634",
   "metadata": {},
   "source": [
    "## 8. Data Splitting\n",
    "\n",
    "On day 1 we discussed train/test splits and cross-validation. This is the final step of data preparation, and is often integrated into our model training/tuning process, especially when we are using cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_appt",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
