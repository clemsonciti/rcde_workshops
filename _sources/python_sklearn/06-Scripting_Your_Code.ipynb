{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d580e8-c362-4a0f-b1fd-08b441eb7339",
   "metadata": {},
   "source": [
    "# Scripting Your Code\n",
    "\n",
    "Once you have drafted the code to prepare your data, split it and train/evaluate your model, you should package your code into a script that you can run via SLURM.\n",
    "\n",
    "This will allow you to submit your job to the cluster and have it run asynchronously, without needing to keep your notebook open. It will also allow you to run your code on a larger dataset, or with more iterations, than you could do interactively.\n",
    "\n",
    "What is different about running your code as a script vs. in a notebook? You need to ensure each of the following:\n",
    "- Your code runs from top to bottom without needing any manual intervention\n",
    "- Your code load the full dataset, not just a sample\n",
    "- Your code checks for unexpected conditions and handles them gracefully\n",
    "- Your code logs information about what it is doing, so you can debug it later if needed\n",
    "- For long jobs, your code checkpoints its progress so that it can resume where it left off if it is interrupted\n",
    "- Make sure any desired outputs (plots, model files, etc.) are saved to disk (rather than merely displayed in the notebook)\n",
    "- Figure out what resources you need to request\n",
    "\n",
    "Let's look at a couple of ways of converting code you developed in Jupyter into a SLURM-submittable script, and then dive into some of the above considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8217283-f243-4629-995d-7544ddbaa983",
   "metadata": {},
   "source": [
    "## 1. Running your notebook directly as a script\n",
    "\n",
    "Probably the easiest route to converting your notebook is just to run it directly as a job on the cluster. This is possible with the command:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to notebook --execute --inplace [notebook_filename].ipynb\n",
    "```\n",
    "\n",
    "where you would replace `notebook_filename` with whatever your notebook filename is. In a full SLURM script, this command might appear as follows:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name my-job-name\n",
    "#SBATCH --nodes 1\n",
    "#SBATCH --cpus-per-task 4\n",
    "#SBATCH --gpus-per-node v100:1\n",
    "#SBATCH --mem 8gb\n",
    "#SBATCH --time 08:00:00\n",
    "\n",
    "module load anaconda\n",
    "\n",
    "cd /path/to/your/notebook\n",
    "\n",
    "jupyter nbconvert --to notebook --execute --inplace [notebook_filename].ipynb\n",
    "```\n",
    "\n",
    "Notice that in this case I've determined that my code can make use of 4 cores each on 2 nodes, as well as a V100 GPU on each node. I've also requested 8GB of memory and 8 hours of runtime. You should adjust these values based on your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b69719a-7b77-4381-a4a2-2ced71b79e75",
   "metadata": {},
   "source": [
    "## 2. Converting your notebook to a script\n",
    "\n",
    "The preferred coding practice would be to convert your notebook into a script yourself. If you have worked with Jupyter notebooks but not with .py scripts, you can think of the latter as being one big cell in a notebook. In fact, you can even make sure your code runs in a single Jupyter cell (including checkpoints, logging, etc.), and then simply copy that cell into a .py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544d19c-b824-4c0a-8e1f-117cea91867c",
   "metadata": {},
   "source": [
    "## 3. Checkpointing\n",
    "\n",
    "If your code is going to take a long time to run, you should consider checkpointing it. This means saving the state of your code at regular intervals, so that if it is interrupted, you can resume from the last checkpoint rather than starting over from the beginning. \n",
    "\n",
    "What exactly this looks like will depend on what you are doing. If you are searching over possible hyperparameters to find the best ones, then you should keep track of which hyperparameters you have tried and what the results were. See the below toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf493071-e499-401c-99f4-1a764df602f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862f4107-6501-4da5-a7db-71c236b69061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10, max_depth=None, accuracy=0.5300\n",
      "n_estimators=50, max_depth=None, accuracy=0.4350\n",
      "Results saved to results.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import csv\n",
    "import os\n",
    "\n",
    "X, y = np.random.rand(1000, 5), np.random.randint(0, 2, 1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Define hyperparameters to test\n",
    "params = [(10, 5), (10, None), (50, 5), (50, None)]\n",
    "\n",
    "# Define the results file, where we'll save the information about which hyperparameters we've already tested\n",
    "results_file = 'results.csv'\n",
    "\n",
    "# Load existing results we've already computed\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        done = set(tuple(row[:2]) for row in csv.reader(f))\n",
    "else:\n",
    "    done = set()\n",
    "\n",
    "# Train models and save results for each set of hyperparameters not already tested\n",
    "with open(results_file, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    if not done:\n",
    "        writer.writerow(['n_estimators', 'max_depth', 'accuracy'])\n",
    "    for n_estimators, max_depth in params:\n",
    "        if (str(n_estimators), str(max_depth)) not in done:\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)\n",
    "            accuracy = accuracy_score(y_test, model.fit(X_train, y_train).predict(X_test))\n",
    "            writer.writerow([n_estimators, max_depth, accuracy])\n",
    "            print(f\"n_estimators={n_estimators}, max_depth={max_depth}, accuracy={accuracy:.4f}\")\n",
    "\n",
    "print(f\"Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffa308-c0e2-478f-bb23-378f3239bfb9",
   "metadata": {},
   "source": [
    "If we have a long training run, we might want to save the model at regular intervals. This is especially important if we are training a model that takes a long time to train, or if we are training on a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66799348-648c-401d-bc9a-7ee73dcf9dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 1.0510294437408447\n",
      "New best model found with loss: 1.0510294437408447\n",
      "Epoch 2\n",
      "Loss: 1.0472089052200317\n",
      "New best model found with loss: 1.0472089052200317\n",
      "Epoch 3\n",
      "Loss: 1.001285433769226\n",
      "New best model found with loss: 1.001285433769226\n",
      "Epoch 4\n",
      "Loss: 0.9997748732566833\n",
      "New best model found with loss: 0.9997748732566833\n",
      "Epoch 5\n",
      "Loss: 1.0021926164627075\n",
      "Epoch 6\n",
      "Loss: 1.0016573667526245\n",
      "Epoch 7\n",
      "Loss: 0.999336302280426\n",
      "New best model found with loss: 0.999336302280426\n",
      "Epoch 8\n",
      "Loss: 0.9966704845428467\n",
      "New best model found with loss: 0.9966704845428467\n",
      "Epoch 9\n",
      "Loss: 0.994911253452301\n",
      "New best model found with loss: 0.994911253452301\n",
      "Epoch 10\n",
      "Loss: 0.994596004486084\n",
      "New best model found with loss: 0.994596004486084\n",
      "Epoch 11\n",
      "Loss: 0.9952884912490845\n",
      "Epoch 12\n",
      "Loss: 0.9958664178848267\n",
      "Epoch 13\n",
      "Loss: 0.9955121874809265\n",
      "Epoch 14\n",
      "Loss: 0.9943965077400208\n",
      "New best model found with loss: 0.9943965077400208\n",
      "Epoch 15\n",
      "Loss: 0.9931568503379822\n",
      "New best model found with loss: 0.9931568503379822\n",
      "Epoch 16\n",
      "Loss: 0.9923498034477234\n",
      "New best model found with loss: 0.9923498034477234\n",
      "Epoch 17\n",
      "Loss: 0.9921700954437256\n",
      "New best model found with loss: 0.9921700954437256\n",
      "Epoch 18\n",
      "Loss: 0.9922705888748169\n",
      "Epoch 19\n",
      "Loss: 0.9923542141914368\n",
      "Epoch 20\n",
      "Loss: 0.9921954870223999\n",
      "Training complete. Best model saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network model\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, inputs=10):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(inputs, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = NNModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Make some random data\n",
    "x = torch.randn(10000, 10)  # Input data\n",
    "y = torch.randn(10000)  # Target data\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x).squeeze(1)\n",
    "    loss = nn.functional.mse_loss(output, y)\n",
    "    \n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track best model\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best_model = model.state_dict()\n",
    "        print(f\"New best model found with loss: {best_loss}\")\n",
    "        torch.save({\n",
    "            'model': best_model,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': best_loss\n",
    "        }, 'best_model.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': loss.item()\n",
    "        }, f'checkpoint_{epoch+1}.pt')\n",
    "\n",
    "print(\"Training complete. Best model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03606038-a80f-4d10-afed-3b9e50c2e5d5",
   "metadata": {},
   "source": [
    "Now, you create a python script that you can submit as a slurm job. You'll need to make a python `fit_nn.py` file and a slurm batch script. The following can be your slurm batch script:\n",
    "```\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=workshop_fit_nn\n",
    "#SBATCH --output=logs/%x-%j.out   # Creates a log file in a logs/ directory\n",
    "#SBATCH --error=logs/%x-%j.err\n",
    "#SBATCH --time=00:05:00\n",
    "#SBATCH --partition=work1\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=8G\n",
    "\n",
    "# Load your environment \n",
    "module load anaconda3\n",
    "source activate hpc_ml  # or your preferred env\n",
    "\n",
    "# Run the Python script\n",
    "python fit_nn.py\n",
    "```\n",
    "\n",
    "To make the `fit_nn.py` file, develop your code in the below code cell. I've already put the extra `import` statements you'll need. Now copy my code from the cell above, and modify it so that it uses the California housing data instead of random `x,y` tensors. To do this, you'll need to:\n",
    "1. Copy/paste the above code and add it to the below import statements\n",
    "2. Load the California housing data (into regressors `X` and target `y`)\n",
    "3. (Optionally) use `StandardScaler()` to scale the `X` data (very advisable for training neural networks!)\n",
    "4. Convert `X` and `y` to torch tensors using e.g. `x = torch.tensor(X, dtype=torch.float32)`\n",
    "5. Make the neural network take 8 inputs (the code I wrote assumes 10 inputs)\n",
    "6. Run the code cell to make sure it works here\n",
    "7. Copy it to a new file `fit_nn.py`\n",
    "8. Create a new file `run_fit_nn.slurm` and copy the slurm batch script code above to it\n",
    "9. Open a terminal window\n",
    "10. Submit your neural network training job using `sbatch run_fit_nn.slurm`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "058adf75-1eee-455a-9d46-ecc0ef6e6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split # optional\n",
    "from sklearn.preprocessing import StandardScaler # optional\n",
    "# Copy/paste the above code cell here, and modify it to use the California housing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8759659f-3733-4f2e-8394-5f27d8b680a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Were you able to successfully submit your job? If so, put your jobid here! If not, please describe what difficulties you encountered."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1902b79f92cf4ac0af5c106ea34ed3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='500px'), placeholder='Type your answer here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9cfaaf11b644ef8737c08904b02fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d32471b444e4247ac48b1d3beacc1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_answer_box(\"Were you able to successfully submit your job? If so, put your jobid here! If not, please describe what difficulties you encountered.\", \"06-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98402a35-3b0e-4152-ae0c-ab6170c1351a",
   "metadata": {},
   "source": [
    "## 4. Resource allocation\n",
    "\n",
    "In order to effectively run your code on the cluster, you need to request the appropriate resources. This includes the number of nodes, the number of cores per node, the amount of memory, the amount of time, and the type of GPU.\n",
    "\n",
    "**When multiple cores help:**\n",
    "- When you are running multiple independent jobs\n",
    "- When you are running a single job that can be parallelized (check the documentation!)\n",
    "- When you are running a single job that can be parallelized, but the parallelization is not built into the code (e.g. you are running multiple instances of the code with different hyperparameters)\n",
    "\n",
    "**When multiple cores don't help:**\n",
    "- When you are running a single job that cannot be parallelized\n",
    "- When you haven't written your code to take advantage of multiple cores\n",
    "\n",
    "**When a GPU helps:**\n",
    "- When you are running a deep learning model\n",
    "- When you are running a model that can be accelerated by a GPU (check the documentation!)\n",
    "\n",
    "**When a GPU doesn't help:**\n",
    "- When you are running a model that is not accelerated by a GPU\n",
    "- When you haven't written your code to take advantage of a GPU\n",
    "\n",
    "**How much memory to request:**\n",
    "- This depends on the size of your dataset and the size of your model, as well as whether you are using a GPU. E.g., if you are using a large language model, you might need a big GPU but not much memory. If you are using a large dataset, you might need a lot of memory but not a big GPU.\n",
    "\n",
    "Be sure to use the `jobperf` command to check how much memory your job is using. E.g., `watch -n 2 jobperf [jobid]` will show you how much memory your job is using every 2 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f758cc4a-2e9b-40ef-8d52-0b2a58132ccf",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "Thank you for joining the workshop today! Please take a moment now to answer the below questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93c5b620-ab34-45a4-9c6e-de76ee5a8edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Are there areas of machine learning that you wish were represented more in this workshop? If so, what are they?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bbefa63a3d42ca9c892ec8c8ddbeda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='500px'), placeholder='Type your answer here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8db280b27a4e56a6def62b1a07b1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8deba7e53394d749556630388e680f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Are there other changes you would suggest for this workshop?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b670c45c654410b21337e50e560d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='500px'), placeholder='Type your answer here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d34f001be4f4c1785dbdd6ec85a500b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645eae1418ea4c51a837255869cca088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What other workshop topics related to AI or machine learning would you like to see from CCIT?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506af5104dc74f3aa837c61e72f608e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='500px'), placeholder='Type your answer here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72b4c6892e940f78c45e4f86cdf09b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b08484d24c409786ece26d699c9b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Please leave any additional comments or questions here."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13957de598954cf79abf539993df7f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='500px'), placeholder='Type your answer here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801c4b48de2a4dedb56ac6c392966116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7effc9e649a4a9b9f3b6ab67cd957d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_answer_box(\"Are there areas of machine learning that you wish were represented more in this workshop? If so, what are they?\", \"06-02\")\n",
    "create_answer_box(\"Are there other changes you would suggest for this workshop?\", \"06-03\")\n",
    "create_answer_box(\"What other workshop topics related to AI or machine learning would you like to see from CCIT?\", \"06-04\")\n",
    "create_answer_box(\"Please leave any additional comments or questions here.\", \"06-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d2e3c-294f-462a-962b-01a4e683e844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPC_ML",
   "language": "python",
   "name": "hpc_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
