{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92feeaf1",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- Define Retrieval-Augmented Generation (RAG) and why it matters for research.\n",
    "- Contrast RAG with fine-tuning and plain prompting; identify trade-offs.\n",
    "- Recognize common research use cases and pitfalls (stale sources, grounding).\n",
    "- Name the core components of a RAG system (retrieval, ranking, prompting, generation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef07b9d",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "*Part of the RCD Workshops series: RAG for Research Applications*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590542b-f870-41c5-8ad8-838e583f46ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    \"Please briefly describe your background with RAG, LLMs, and python. Are you very familiar with each of these? Entirely unfamiliar? Somewhere in between?Are there any particular things you're hoping to learn in today's workshop?\",\n",
    "    question_id='mod1_background'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a06d1",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 What is RAG and Why Do We Need It?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines a pretrained large language model (LLM) with an external data retrieval system so that answers are **grounded in up-to-date, relevant external knowledge**. Instead of relying solely on what the LLM knows, RAG-enabled systems can \"consult\" databases or document corpora to generate better, more trustworthy responses.\n",
    "\n",
    "### Motivations for RAG in Research\n",
    "- **Reduce hallucinations**: LLMs sometimes make up answers. RAG anchors model outputs using real documents.\n",
    "- **Extend knowledge**: LLMs have a training cutoff; RAG lets you search new/specialized info on demand.\n",
    "- **Enable citations & trust**: In research and academic settings, RAG allows citation of sources and provenance.\n",
    "- **Lower costs**: Augmenting a frozen LLM with retrieval is much cheaper than fine-tuning or retraining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002609d7",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Examples & Use Cases\n",
    "- **Open-domain QA**: e.g., “What did NASA’s latest exoplanet report reveal?” Without RAG, an LLM trained in 2023 might guess or generalize; with RAG, it can pull excerpts from NASA’s 2025 press release and summarize the confirmed findings.\n",
    "- **Research assistant**: e.g., A RAG system connected to PubMed retrieves and summarizes the newest trials on CRISPR therapies, grounding each summary in citations.\n",
    "- **Data-aware report generator**: e.g., A RAG pipeline for sustainability research can pull recent regional temperature data and generate a grounded report, rather than hallucinating statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbe6ea",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3 RAG vs. Other Approaches\n",
    "- **Versus Fine-tuning:** Fine-tuning builds new knowledge into the model weights (very costly/slow). RAG keeps the model fixed and only augments its inputs with up-to-date evidence.\n",
    "- **Versus traditional search:** Search finds docs, but RAG finds *and reads/summarizes* them for you, so you get the answer directly.\n",
    "- **Versus plain prompting:** Without retrieval, the LLM answers from memory; with RAG, it cites and grounds responses in retrieved evidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1_components",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4 Core Components of a RAG System\n",
    "> ![Diagram Placeholder: Schematic of a RAG pipeline. Show a query box, retrieval module fetching relevant docs from a database, passing to LLM with retrieved context, and final grounded answer.](rag_pipeline_graphviz.png)\n",
    "\n",
    "- **Retrieval:** Ingest and index documents (often via chunking + embeddings) and fetch top-k candidates for a query.\n",
    "- **Ranking:** Re-rank retrieved passages (e.g., BM25 + dense, or cross-encoder rerankers) to improve relevance.\n",
    "- **Prompting:** Build an instruction + query + selected context prompt that guides the LLM to ground answers and cite sources.\n",
    "- **Generation:** Use the LLM to produce an answer constrained by the provided evidence.\n",
    "\n",
    "> Other lifecycle pieces include ingestion (chunking/overlap), indexing refresh, evaluation, and observability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1_components_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    \"Why is \\\"prompting\\\" one of the core components of a RAG system that we need to build? After all, isn't it the user who will provide the prompt?\",\n",
    "    question_id='mod1_components'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1_pitfalls",
   "metadata": {},
   "source": [
    "\n",
    "## 1.5 Common Pitfalls and Mitigations\n",
    "- **Stale sources:** Indexes drift out of date; schedule refreshes and show source timestamps in answers.\n",
    "- **Irrelevant retrieval:** Poor embeddings or queries lead to noise; tune chunking, add query expansion, and apply reranking.\n",
    "- **Over/under-chunking:** Too large misses specifics; too small loses context; use overlap and validate chunk sizes.\n",
    "- **Context overflow:** Excess context gets truncated; limit k, compress or summarize context, and instruct concise citation.\n",
    "- **Prompt injection/untrusted content:** Retrieved text may contain instructions; sanitize inputs and instruct the model to ignore instructions from context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1_pitfalls_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    \"You are certain that your RAG source documents are good, but you're still getting terrible results -- the model is hallucinating some facts and getting other facts dead wrong. What could be the problem?\",\n",
    "    question_id='mod1_pitfalls'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80829bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    '**How might RAG be relevant to your research?**',\n",
    "    question_id='mod1_advantage_rag'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e152c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-Up\n",
    "\n",
    "You now understand what RAG is, why it's important for scientific/academic research, and how it compares with other ways of expanding a language model's knowledge.\n",
    "\n",
    "**Next:** In the following module, you'll get hands-on with document retrieval and see how modern vector-based retrieval (using embeddings) powers RAG systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cde33f-6599-4faf-bf6e-c290306e0ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
