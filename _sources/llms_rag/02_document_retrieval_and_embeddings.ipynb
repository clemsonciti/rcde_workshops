{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e186c37",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- Explain embeddings and similarity at a high level.\n",
    "- Chunk and embed a small corpus with a modern sentence encoder.\n",
    "- Index vectors and run top-k similarity search (FAISS or exact NN).\n",
    "- Assess retrieval quality and iterate on chunking/top_k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b1d61",
   "metadata": {},
   "source": [
    "# Module 2: Document Retrieval and Embeddings\n",
    "\n",
    "*Part of the RCD Workshops series: RAG for Research Applications*\n",
    "---\n",
    "\n",
    "In this module, we'll dive into how to fetch relevant documents for RAG, covering both \"classic\" (keyword) and modern (embedding) approaches, with hands-on practice for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cc490",
   "metadata": {},
   "source": [
    "## 2.1: From Keywords to Vectors: Why Classic Search Isn’t Enough\n",
    "\n",
    "Traditional document search relies on **keyword matching** — for example, using TF-IDF or BM25 — but this method misses synonyms and rephrasings. RAG leverages **embeddings** instead: both documents and queries are mapped to dense vectors that reflect semantic *meaning*, enabling discovery even if no words overlap.\n",
    "\n",
    "By \"dense vectors,\" we mean that each document or query is represented as a point in a high-dimensional space, where similar meanings are closer together. This allows us to find relevant documents based on their semantic content rather than just exact word matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ae504-c59a-4e94-b08a-f593f96bbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, faiss, torch\n",
    "print(\"NumPy\", numpy.__version__)\n",
    "print(\"FAISS\", faiss.__version__, \"(CPU) | Torch\", torch.__version__, \"CUDA\", torch.version.cuda, \"GPU\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d43c06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EXERCISE: Classic Keyword Search\n",
    "corpus = [\n",
    "    'Impacts of climate change on global economies are substantial.',\n",
    "    'Recent studies discuss worldwide financial losses due to global warming.',\n",
    "    'I learned to sew in my high school home economics class.'\n",
    "]\n",
    "query = 'climate economics'\n",
    "def keyword_search(query, docs):\n",
    "    return [d for d in docs if any(word.lower() in d.lower() for word in query.split())]\n",
    "keyword_search(query, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b50123",
   "metadata": {},
   "source": [
    "<img src=\"semantic_sim_venn.png\" alt=\"Semantic Similarity Venn Diagram\" width=\"500\"/>\n",
    "\n",
    "Above: Only exact (or near-exact) keyword matches will be found. Synonyms/non-obvious rephrasings are missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc1585",
   "metadata": {},
   "source": [
    "## 2.2: What Are Embeddings?\n",
    "\n",
    "Embeddings are vector representations of text such that meaningfully similar texts have vectors close together in space.\n",
    "\n",
    "<img src=\"embeddings.png\" alt=\"Image embeddings\" width=\"500\"/>\n",
    "\n",
    "Let's see a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae054ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary tool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "sentences = [\n",
    "    'Large language models can learn from research papers.',\n",
    "    'AI systems use documents to answer questions.',\n",
    "    'Bananas are yellow and tasty.'\n",
    "]\n",
    "embs = model.encode(sentences)\n",
    "\n",
    "# Now we have embeddings for each sentence. Let's take a look at the first chunk of each embedding.\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"Sentence {i}: {s}\")\n",
    "    print(f\"Embedding: {embs[i][:15]}...\\n\")  # Display first 15 elements of each embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the cosine similarities among our documents.\n",
    "def cosine(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "for i, s1 in enumerate(sentences):\n",
    "    for j, s2 in enumerate(sentences):\n",
    "        if i < j:\n",
    "            print(f\"Similarity('{s1}', '{s2}') = {cosine(embs[i], embs[j]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3d190",
   "metadata": {},
   "source": [
    "You should see higher similarity between topically related text, much lower for unrelated (e.g. the banana one).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581d08f",
   "metadata": {},
   "source": [
    "### Quick Check: In your own words\n",
    "Why do we use embeddings instead of plain keyword search when building a RAG system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6827a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"**Your Answer:** We use embeddings instead of only keywords because...\", question_id=\"mod2_why_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ba523",
   "metadata": {},
   "source": [
    "## 2.3: Preparing Documents for Retrieval: Chunking and Embedding\n",
    "\n",
    "Documents are often too long for models to process at once. We break them into chunks (by token/paragraph) before embedding.\n",
    "\n",
    "**Why chunk?**\n",
    "- Keeps each unit the right size for LLM input\n",
    "- Lets retrieval focus on topical sections — precision\n",
    "\n",
    "Let's practice chunking and embedding a custom document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6339d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: Manual chunking\n",
    "doc = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) augments LLMs by allowing retrieval from external sources. \\\n",
    "Chunking splits text into manageable parts; for example, splitting by paragraph.\n",
    "\n",
    "Embeddings allow searches to find relevant sections even if different words are used. Cosine similarity quantifies text closeness.\n",
    "\n",
    "Document retrieval pipelines (using tools like FAISS) depend on these steps working well together.\n",
    "\"\"\"\n",
    "chunks = [c.strip() for c in doc.split('\\n') if c.strip()]\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Chunk {i+1}: {chunk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1611da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed your chunks\n",
    "chunk_embs = model.encode(chunks)\n",
    "\n",
    "# Print the first 15 elements of each chunk embedding\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    print(f\"Embedding: {chunk_embs[i][:15]}...\\n\")  # Display first 15 elements of each embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ec63d",
   "metadata": {},
   "source": [
    "### Dataset: Demo Corpus\n",
    "\n",
    "We will use a tiny mixed-domain corpus (AI, Climate, Biomedical, Materials paper abstracts) stored in `data/demo_corpus.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703bc71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/demo_corpus.jsonl'\n",
    "df = pd.read_json(DATA_PATH, lines=True)\n",
    "docs = df.to_dict('records')\n",
    "print(f'Loaded {len(docs)} docs from {DATA_PATH}')\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc3fd2-3f23-4729-9c50-0ccd91624482",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732f4e1",
   "metadata": {},
   "source": [
    "## 2.4: Indexing Scientific Abstracts (with FAISS)\n",
    "We'll go end-to-end using the demo corpus of scientific abstracts: chunk abstracts → encode chunks → index → retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95146ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a tiny passage index from scientific abstracts with simple chunking\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def chunk_text(text, max_chars=400):\n",
    "    text = (text or '').strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return [text[i:i+max_chars].strip() for i in range(0, len(text), max_chars)]\n",
    "\n",
    "# Prepare chunk records from the loaded demo corpus (expects df/docs from above)\n",
    "chunk_texts = []\n",
    "chunk_meta = []\n",
    "for d in docs:\n",
    "    abs_text = d.get('abstract', '')\n",
    "    pieces = chunk_text(abs_text, max_chars=400)\n",
    "    for j, t in enumerate(pieces):\n",
    "        if not t:\n",
    "            continue\n",
    "        chunk_texts.append(t)\n",
    "        chunk_meta.append({'doc_id': d.get('id'), 'title': d.get('title'), 'chunk_id': j})\n",
    "\n",
    "# Encode and normalize for cosine similarity via inner product\n",
    "embs = model.encode(chunk_texts)\n",
    "embs = np.array([v/np.linalg.norm(v) for v in embs], dtype='float32')\n",
    "index = faiss.IndexFlatIP(embs.shape[1])\n",
    "index.add(embs)\n",
    "\n",
    "# Simple demo query over abstracts\n",
    "query = 'How do RAG systems combine LLMs with retrieval?'\n",
    "q = model.encode([query])[0]\n",
    "q = (q/np.linalg.norm(q)).astype('float32')\n",
    "D, I = index.search(np.array([q]), k=3)\n",
    "for rank, (idx, score) in enumerate(zip(I[0], D[0]), start=1):\n",
    "    m = chunk_meta[idx]\n",
    "    snippet = chunk_texts[idx][:160].replace('\\n',' ')\n",
    "    print(f'#{rank} score={score:.3f}| {m[\"title\"][:90]}...')\n",
    "    print(f'   {snippet}...\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5034557",
   "metadata": {},
   "source": [
    "## Quick Knowledge Check\n",
    "What would happen if you used a *very* long chunk size? Write a brief hypothesis about the kinds of results you'd get from using a retrieval module in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5e546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"**Your Hypothesis:**\\n- With a long chunk size..\", question_id=\"mod2_longchunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772e48b",
   "metadata": {},
   "source": [
    "# End of Module 2\n",
    "\n",
    "You've now practiced the core steps of document retrieval for RAG: classic vs. semantic search, embedding, chunking, and vector indexing.\n",
    "\n",
    "Next: We'll assemble these building blocks into a complete RAG pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
