{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mod3-title",
   "metadata": {},
   "source": [
    "# Module 3: Building a Simple RAG Pipeline\n",
    "\n",
    "*Part of the RCD Workshops series: Retrieval-Augmented Generation (RAG) for Advanced Research Applications*\n",
    "\n",
    "---\n",
    "\n",
    "In this module, we'll connect retrieval and generation to build a working RAG pipeline end-to-end.\n",
    "We'll use our small example corpus (from Module 2), a retrieval component, and an 8B LLM, to show how RAG works in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0462bb",
   "metadata": {},
   "source": [
    "![RAG pipeline](rag_pipeline_graphviz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39f2d1",
   "metadata": {},
   "source": [
    "### Dataset: Demo Corpus\n",
    "\n",
    "We will use a tiny mixed-domain corpus (AI, Climate, Biomedical, Materials) stored in `data/demo_corpus.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0d1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/demo_corpus.jsonl'\n",
    "df = pd.read_json(DATA_PATH, lines=True)\n",
    "docs = df.to_dict('records')\n",
    "print(f'Loaded {len(docs)} docs from {DATA_PATH}')\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-step-setup-llm",
   "metadata": {},
   "source": [
    "## 3.1 Setting up the LLM\n",
    "For RAG, we need a language model that can read our prompt and generate an answer using retrieved context. We'll use Qwen-8B (open-weight, Hugging Face) for this pipeline.\n",
    "\n",
    "> **Note:** You need a GPU (ideally A100 or similar) to load a larger model at usable speed.\n",
    "\n",
    "We'll use the `transformers` library. Loading may take a while.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-llm-load",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = 'Qwen/Qwen3-8B'\n",
    "custom_cache = '/project/rcde/cehrett/rag_workshop/models/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=custom_cache,\n",
    "    device_map='auto',\n",
    "    dtype='auto'\n",
    ")\n",
    "MODEL_READY = True\n",
    "print(f'Loaded LLM: {model_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-rag-pipeline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Build chunked passage index from abstracts\n",
    "def chunk_text(text, max_chars=400):\n",
    "    text = (text or '').strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return [text[i:i+max_chars].strip() for i in range(0, len(text), max_chars)]\n",
    "\n",
    "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "chunk_texts = []\n",
    "chunk_meta = []\n",
    "for d in docs:\n",
    "    abs_text = d.get('abstract', '')\n",
    "    pieces = chunk_text(abs_text, max_chars=400)\n",
    "    for j, t in enumerate(pieces):\n",
    "        if not t:\n",
    "            continue\n",
    "        chunk_texts.append(t)\n",
    "        chunk_meta.append({'doc_id': d.get('id'), 'title': d.get('title'), 'chunk_id': j})\n",
    "\n",
    "embs = encoder.encode(chunk_texts)\n",
    "embs = np.array([v/np.linalg.norm(v) for v in embs], dtype='float32')\n",
    "index = faiss.IndexFlatIP(embs.shape[1])\n",
    "index.add(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858cd17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example question for climate economics\n",
    "query = \"According to recent studies, how exactly does replanting trees to replenish forests help to fight against climate change?\"\n",
    "q = encoder.encode([query])[0]\n",
    "q = (q/np.linalg.norm(q)).astype('float32')\n",
    "D, I = index.search(np.array([q]), k=2)\n",
    "retrieved_indices = I[0]\n",
    "print('Retrieved chunk indices:', retrieved_indices)\n",
    "retrieved_texts = [chunk_texts[i] for i in retrieved_indices]\n",
    "retrieved_meta = [chunk_meta[i] for i in retrieved_indices]\n",
    "print('Top-1 Retrieved text snippet:', retrieved_texts[0][:160].replace('\\n',' '), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-compose-prompt",
   "metadata": {},
   "source": [
    "### Building the Prompt\n",
    "To maximize answer quality, prompt your LLM with clear instructions and insert the most relevant docs just before the user's question.\n",
    "A simple format is to list docs like [Document 1], [Document 2], then give the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d7796f-0fe7-4441-9ec8-d502e2938057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box('Please describe your level of familiarity with LLM prompting concepts, including: system prompts vs. user prompts, chat templates, and so on.', question_id='mod_3_prompt_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-prompt-code",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build messages for chat template aware models (e.g., Qwen3)\n",
    "system_msg = (\n",
    "    \"You are a research assistant. Ground your answer to the user's query in the provided documents. \"\n",
    "    \"Cite document numbers inline when useful. If unsure, say you don't know.\"\n",
    ")\n",
    "\n",
    "docs_lines = []\n",
    "for i, text in enumerate(retrieved_texts, start=1):\n",
    "    docs_lines.append(f'[Document {i}]\\n{text}\\n')\n",
    "context_block = \"\".join(docs_lines)\n",
    "user_msg = f\"## Context:\\n{context_block}\\n\\n## Question:\\n{query}\"\n",
    "\n",
    "messages = [\n",
    "    { 'role': 'system', 'content': system_msg },\n",
    "    { 'role': 'user',   'content': user_msg },\n",
    "]\n",
    "\n",
    "# Render with chat template\n",
    "rendered_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print('Prompt (rendered):\\n')\n",
    "print(rendered_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-llm-generate-explain",
   "metadata": {},
   "source": [
    "### LLM: Answering with Retrieved Information\n",
    "Now, send the composed prompt to your language model.\n",
    "> This step may be slow unless you're on a GPU-ready machine, but shows the full RAG loop!\n",
    "If working on CPU or want to skip, use a smaller LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-llm-generate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = tokenizer(rendered_prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=512, temperature=0.2, do_sample=False)\n",
    "answer = tokenizer.decode(outputs[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print('\\nGenerated Answer:', answer.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-try-yours",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "Modify the `query` above (in the RAG pipeline code cell) to something your document can answer -- or to something *none* of the docs cover.\n",
    "What happens? How does the retrieval affect the model's output?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-answer-box",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box('In the above code in this notebook, what does the line `q = encoder.encode([query])[0]` do?', question_id='mod_3_encoder_question')\n",
    "\n",
    "create_answer_box('In the above code in this notebook, what does the line `D, I = index.search(np.array([q]), k=2)` do?', question_id='mod_3_index_question')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-context-limits",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note on Prompt Lengths & Context:**\n",
    "Models like Qwen3 support long context windows (up to 32K tokens or more), but you often need to truncate or focus your retrieved docs.\n",
    "Too much, and the model may ignore key info; too little, and you could miss relevant context.\n",
    "\n",
    "That's why retrieval *quality* is just as important as the LLM itself!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-end-wrap",
   "metadata": {},
   "source": [
    "\n",
    "Congratulationsâ€”You now have a basic, working RAG pipeline!\n",
    "In the next module, we'll explore how to improve retrieval quality and tackle more advanced scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-streamlined-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Streamlined RAG (Library-Based)\n",
    "\n",
    "The above walkthrough showed a ground-up RAG pipeline. Below is a concise version using a popular orchestration library to wire up embeddings, a vector store, a retriever, and an LLM chain.\n",
    "\n",
    "This mirrors what many teams do in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed192a05-2232-4c23-81ab-d173b34a9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS as LCFAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "# --- You must define these beforehand: ---\n",
    "# docs: iterable of dicts with keys 'id','title','year','abstract'\n",
    "# model, tokenizer: loaded HF model + tokenizer (prefer an *Instruct/Chat* variant)\n",
    "# query: the user query string\n",
    "\n",
    "print('Building vector store with LangChain (auto-chunk + FAISS) ...')\n",
    "emb = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Wrap raw records as LangChain Documents\n",
    "raw_docs = []\n",
    "for d in docs:\n",
    "    text = (d.get('abstract') or '').strip()\n",
    "    if not text:\n",
    "        continue\n",
    "    md = {'doc_id': d.get('id'), 'title': d.get('title'), 'year': d.get('year')}\n",
    "    raw_docs.append(Document(page_content=text, metadata=md))\n",
    "print(f'- Loaded {len(raw_docs)} source documents')\n",
    "\n",
    "# Split text\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400, chunk_overlap=40, separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n",
    "docs_split = splitter.split_documents(raw_docs)\n",
    "print(f'- Created {len(docs_split)} chunks (chunk_size=400, overlap=40)')\n",
    "\n",
    "# Vector store\n",
    "vs = LCFAISS.from_documents(docs_split, embedding=emb)\n",
    "retriever = vs.as_retriever(search_type='similarity', search_kwargs={'k': 2})\n",
    "\n",
    "print('Retrieving context...')\n",
    "relevant_docs = retriever.get_relevant_documents(query)\n",
    "context_text = \"\\n\\n\".join(f\"[{i+1}] {d.page_content}\" for i, d in enumerate(relevant_docs))\n",
    "\n",
    "print('Wrapping Transformers model as an LLM pipeline...')\n",
    "# Ensure pad token is set (avoids generation quirks)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "gen = hf_pipeline(\n",
    "    task='text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    do_sample=False,\n",
    "    return_full_text=False,  # <- don't echo the prompt\n",
    ")\n",
    "\n",
    "# Build chat messages and render with the model's chat template\n",
    "system_msg = (\n",
    "    \"You are a research assistant. Use the provided context to answer. \"\n",
    "    \"Cite titles when helpful. If unsure, say you don't know.\"\n",
    ")\n",
    "user_msg = (\n",
    "    f\"## Context:\\n{context_text}\\n\\n\"\n",
    "    f\"## Question:\\n{query}\\n\\n\"\n",
    "    \"Provide a concise answer.\"\n",
    ")\n",
    "\n",
    "# If your tokenizer has a chat template (most *Instruct/Chat* models do), use it:\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_msg},\n",
    "    {\"role\": \"user\", \"content\": user_msg},\n",
    "]\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,  # ensures the model starts the assistant turn\n",
    ")\n",
    "\n",
    "print('Querying model...')\n",
    "out = gen(prompt_text)\n",
    "answer_text = out[0][\"generated_text\"].strip()\n",
    "\n",
    "print('\\nAnswer:\\n', answer_text)\n",
    "\n",
    "print('\\nTop sources:')\n",
    "for i, d in enumerate(relevant_docs, 1):\n",
    "    md = getattr(d, 'metadata', {}) or {}\n",
    "    title = md.get('title', '') or ''\n",
    "    doc_id = md.get('doc_id', '') or ''\n",
    "    print(f'- Source {i}: {title[:80]} (id={doc_id})')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
