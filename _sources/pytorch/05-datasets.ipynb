{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets and data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`torch.utils.data`](https://pytorch.org/docs/stable/data.html) subpackage is an important part of PyTorch for developing neural networks. The `Dataset` class represents a dataset and provides an interface to access the data samples. The `DataLoader` class helps fetch data from the dataset and prepare it for passing to your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Case study: ImageNet data\n",
    "\n",
    "The ImageNet-1000 image classification task has been a huge driver of progress in deep learning. Let's get to know this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Locate the images\n",
    "image_dir = '/project/rcde/datasets/imagenet/ILSVRC/Data/CLS-LOC/'\n",
    "os.listdir(image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagenet has 1000 different classes. Each class has its own sub-folder (test dataset is organized differently): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in os.listdir(image_dir):\n",
    "    dir_path = os.path.join(image_dir, d)\n",
    "    if os.path.isdir(dir_path):  # Check if the item is a directory\n",
    "        print(d, len(os.listdir(dir_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These classes have uninformative directory names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir(image_dir+'train')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a file that maps from these strange names to human-readable names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! head -n 5 '/project/rcde/datasets/imagenet/LOC_synset_mapping.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/project/rcde/datasets/imagenet/LOC_synset_mapping.txt') as f: \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    # we will need these two dictionaries\n",
    "    class2label = {l[:9].strip(): l[10:-1].strip() for l in lines}\n",
    "    class2ix = {l[:9].strip(): ix for ix, l in enumerate(lines)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most classes have 1300 training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cls in os.listdir(image_dir+'train')[::50]:\n",
    "    print(class2label[cls], len(os.listdir(f\"{image_dir}train/{cls}/\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most classes have only 50 validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for cls in os.listdir(image_dir+'val')[::50]:\n",
    "    print(class2label[cls], len(os.listdir(f\"{image_dir}val/{cls}/\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "num_images = 25\n",
    "sample_images = []\n",
    "image_classes = []\n",
    "for cls in os.listdir(image_dir+'train')[:num_images]:\n",
    "    sample_images.append(glob(f\"{image_dir}train/{cls}/*.*\")[0])\n",
    "    image_classes.append(cls)\n",
    "    \n",
    "fig, ax = plt.subplots(5, 5)\n",
    "fig.set_size_inches(8,8)\n",
    "for ix, a in enumerate(ax.flatten()):\n",
    "    a.imshow(img.imread(sample_images[ix]))\n",
    "    a.set_title(class2label[image_classes[ix]].split(',')[0])\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Map-style dataset](https://pytorch.org/docs/stable/data.html)\n",
    "Use this when you have a well-defined set of samples that you will use to train your model. This is the most common case and the natural choice for ImageNet because we have a well-defined set of images that we want to feed to our model. Let's see how to create a map-style dataset class for ImageNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision.transforms import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "# subclass Dataset\n",
    "class Imagenet(Dataset):\n",
    "    \n",
    "    def __init__(self, root_dir: str, split: str, class2ix: dict, tfms = None):\n",
    "        \"\"\"\n",
    "        The __init__ method is called when creating an instance of the class.\n",
    "        This is where we set up the dataset, initialize paths, and apply any data transformations.\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): Full path to the ImageNet CLS-LOC folder containing \"train\" and \"val\" subfolders.\n",
    "            split (str): Specifies which dataset split to use, either \"train\" or \"val\".\n",
    "            class2ix (dict): A dictionary mapping class names to their corresponding indices.\n",
    "            tfms (callable, optional): A function or transform that takes in a PIL image and returns a transformed version. Defaults to None.\n",
    "        \n",
    "        Attributes:\n",
    "            image_paths (list): A list containing the full paths to all the images in the specified split.\n",
    "            class2ix (dict): The provided class-to-index mapping dictionary.\n",
    "        \n",
    "        Raises:\n",
    "            AssertionError: If the split is not one of 'train' or 'val'.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.class2ix = class2ix\n",
    "        self.tfms = tfms\n",
    "        \n",
    "        # make sure split is supported\n",
    "        assert split in {'train', 'val'}, f\"Split must be one of 'train' or 'val', not {split}.\"\n",
    "        \n",
    "        # get a list of all the images\n",
    "        self.image_paths = list(Path(f\"{self.root_dir}/{self.split}\").rglob(\"*.JPEG\"))\n",
    "        \n",
    "        # create a list mapping from class to index\n",
    "        self.class2ix = class2ix\n",
    "                                \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Map-style datasets must define the __len__ method. These return the number of \n",
    "        samples in the dataset. \n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Map-style datasets must define __getitem__ which takes an index and returns\n",
    "        a sample. This puts the \"map\" in \"map-style dataset\" because it represents\n",
    "        a mapping from some keys (indices) to the actual data. Map must return \n",
    "        a pytorch tensor or numpy array (or a collection thereof).\n",
    "        \"\"\"\n",
    "        # the path to the selected image\n",
    "        path = self.image_paths[index].as_posix()\n",
    "        \n",
    "        # get the class index\n",
    "        # the class is the next-to-last location in the file path\n",
    "        y = self.class2ix[path.split('/')[-2]]\n",
    "        \n",
    "        # read the instance\n",
    "        x = read_image(path, mode = ImageReadMode.RGB)\n",
    "        \n",
    "        # scale to 0 to 1 range\n",
    "        x = x / 255\n",
    "        \n",
    "        if self.tfms:\n",
    "            x = self.tfms(x)\n",
    "        \n",
    "        # return the \n",
    "        return x, y\n",
    "        \n",
    "tfms = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std= [0.229, 0.224, 0.225]),\n",
    "    transforms.Resize((224,224), antialias=True)\n",
    "])\n",
    "        \n",
    "imagenet = Imagenet('/project/rcde/datasets/imagenet/ILSVRC/Data/CLS-LOC/', split='val', class2ix=class2ix, tfms=tfms)\n",
    "\n",
    "print(\"Number of samples:\", len(imagenet))\n",
    "x, y = imagenet[5]\n",
    "print(\"(x.shape, y)=\", x.shape, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [Regression and Classification notebook](https://clemsonciti.github.io/rcde_workshops/pytorch/02-regression_and_classification.html), we trained the model by computing the loss for the _entire_ dataset multiple times. Our training loop looked something like: \n",
    "```python\n",
    "for i in range(num_epochs):\n",
    "    # forward pass\n",
    "    y_hat = model(x)\n",
    "\n",
    "    # measure the loss\n",
    "    # this is the mean squared error\n",
    "    loss = loss_func(y_hat, y)\n",
    "\n",
    "    # gradient computation\n",
    "    loss.backward()\n",
    "\n",
    "    # parameter updates\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "Where `x` and `y` represented the _entire_ input data and target data, respectively. \n",
    "\n",
    "---\n",
    "\n",
    "**Question**: What problems would we run into if we applied this to ImageNet?\n",
    "\n",
    "---\n",
    "\n",
    "In most applications of deep learning, we will instead loop over mini-batches (small subsets) of our training data. Our modified training loop will look something like: \n",
    "```python\n",
    "for i in range(num_epochs):\n",
    "    # Now we have an inner loop over batches of data\n",
    "    for x_batch, y_batch in batches:\n",
    "        # forward pass\n",
    "        y_hat_batch = model(x_batch)\n",
    "\n",
    "        # measure the loss\n",
    "        # this is the mean squared error\n",
    "        loss_batch = loss_func(y_hat_batch, y_batch)\n",
    "\n",
    "        # gradient computation\n",
    "        loss_batch.backward()\n",
    "\n",
    "        # parameter updates\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "Where `batches` is an iterable that returns tuples of the form `(x_batch, y_batch)` representing samples of the full dataset. \n",
    "\n",
    "It turns out that using very large batches leads to worse performance. \n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/raw/master/pytorch/fig_imagenet_largebatch.png\" alt=\"large batch size\" width=\"500\"/>\n",
    "\n",
    "---\n",
    "\n",
    "**Question**: Why do you think large batch size leads to worse performance?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent with the `DataLoader` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` class is our interface to the individual samples within our dataset. The `DataLoader` utility class provides an interface to batches of data. It also supports multiprocessing out of the box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# the DataLoader takes the dataset class as input\n",
    "# batch_size: how many samples per mini batch\n",
    "# num_workers: how many parallel processes for data loading\n",
    "dl = DataLoader(imagenet, batch_size=256, num_workers=8)\n",
    "\n",
    "print(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch fetches the batches of data on the fly, so we have to request them one at a time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x,y=next(iter(dl)) # get the first batch\n",
    "print(x.shape, y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the new dimension. The dataloader has bundled up the samples into a single tensor. \n",
    "\n",
    "We're now ready to write our new training loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet Training/Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the datasets\n",
    "imagenet_train = Imagenet('/project/rcde/datasets/imagenet/ILSVRC/Data/CLS-LOC/', split='train', class2ix=class2ix, tfms=tfms)\n",
    "imagenet_val = Imagenet('/project/rcde/datasets/imagenet/ILSVRC/Data/CLS-LOC/', split='val', class2ix=class2ix, tfms=tfms)\n",
    "\n",
    "# create dataloaders for training and validation\n",
    "dl_train = DataLoader(imagenet_train, batch_size=256, num_workers=9)\n",
    "dl_val = DataLoader(imagenet_val, batch_size=256, num_workers=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question**: Why is it good to have separate training and validation sets? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "num_epochs=3\n",
    "# Take a look at `htop` and `nvidia-smi` when running this...\n",
    "for i in range(num_epochs):\n",
    "    print(f\"[Epoch {i}] Training...\")\n",
    "    for ix, (x,y) in enumerate(dl_train):\n",
    "        print(f\"\\r[Epoch {i}] Batch {ix}. x.shape={x.shape}\", end='')\n",
    "        \n",
    "        # move to device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # this is just a test, so break early\n",
    "        if ix==9:\n",
    "            break\n",
    "            \n",
    "    print(f\"\\n[Epoch {i}] Testing...\")\n",
    "    for ix, (x, y) in enumerate(dl_val):\n",
    "        print(f\"\\r[Epoch {i}] Batch {ix}. x.shape={x.shape}\", end='')\n",
    "        \n",
    "        # move to device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # this is where we put the model evaluation logic\n",
    "        \n",
    "        # this is just a test, so break early\n",
    "        if ix==3:\n",
    "            break\n",
    "            \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's actually evaluate a trained model\n",
    "Training takes too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# pretrained weights with advertised accuracy of 80.858% on the validation set\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val = DataLoader(imagenet_val, batch_size=256, num_workers=9)\n",
    "\n",
    "preds_ls = []\n",
    "targs_ls = []\n",
    "\n",
    "# put the model in eval mode\n",
    "model.eval()\n",
    "for ix, (x, y) in enumerate(dl_val):\n",
    "    \n",
    "    # move to device\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    # this is where we put the model evaluation logic\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x)\n",
    "    \n",
    "    # compute batch-level performance metrics\n",
    "    # For classification tasks, the model typically outputs a probability distribution \n",
    "    # over the classes for each sample in the batch. \n",
    "    # The most likely class is chosen by taking the argmax along the last dimension:\n",
    "    pred_cls = y_pred.argmax(-1)\n",
    "    top1_acc = (pred_cls == y).type(torch.float32).mean().item()\n",
    "    \n",
    "    # save preds for final acc calc\n",
    "    preds_ls.append(pred_cls.cpu().squeeze())\n",
    "    targs_ls.append(y.cpu().squeeze())\n",
    "    \n",
    "    print(f\"Batch {ix}. Accuracy={100*top1_acc:0.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = torch.concat(preds_ls)\n",
    "targs = torch.concat(targs_ls)\n",
    "mean_top1_acc = (preds==targs).type(torch.float32).mean()\n",
    "\n",
    "print(f\"Average Accuracy={100*mean_top1_acc:0.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PytorchWorkshop",
   "language": "python",
   "name": "pytorchworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
