{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnuMqEJsdmsZ"
   },
   "source": [
    "# Pytorch GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1FhZ4ALdmsi"
   },
   "source": [
    "The Palmetto cluster has [many GPU compute nodes](https://docs.rcd.clemson.edu/palmetto/compute/hardware#infiniband-phases). A crucial feature of PyTorch is the support of GPUs--short for Graphics Processing Unit. A GPU can perform many thousands of small operations in parallel, making it perfect for performing large matrix operations in neural networks. _You do not need to know much about GPU programming to use PyTorch on the GPU!_\n",
    "\n",
    "When comparing GPUs to CPUs, we can list the following main differences (credit: [Kevin Krewell, 2009](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/)) \n",
    "\n",
    "<left style=\"width: 100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/comparison_CPU_GPU.png?raw=1\" width=\"700px\"></left>\n",
    "\n",
    "CPUs and GPUs have both different advantages and disadvantages, which is why many computers contain both components and use them for different tasks. In case you are not familiar with GPUs, you can read up more details in this [NVIDIA blog post](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) or [here](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1FhZ4ALdmsi"
   },
   "source": [
    "GPUs can accelerate the training of your network up to a factor of $100$ which is essential for large neural networks. PyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries [CUDA](https://developer.nvidia.com/cuda-zone) and [cuDNN](https://developer.nvidia.com/cudnn)). First, let's check whether you have a GPU available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# And our input function\n",
    "from utils import create_answer_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Have you used GPUs for computing before? If so, what for?\", \"02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tw9IJPMdmsi",
    "outputId": "27f0fc2a-ed6b-46cc-98d5-a6588ba67601",
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f'Is the GPU available? {\"Yes\" if gpu_avail else \"No\"}')\n",
    "assert gpu_avail, 'No graphics card available!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can information about your GPU usage by opening a terminal and running the `nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBC-BCbWdmsi"
   },
   "source": [
    "By default, all tensors you create are stored on the CPU. We can push a tensor to the GPU by using the function `.to(...)`, or `.cuda()`. However, it is often a good practice to define a `device` object in your code which points to the GPU if you have one, and otherwise to the CPU. Then, you can write your code with respect to this device object, and it allows you to run the same code on both a CPU-only system, and one with a GPU. Let's try it below. We can specify the device as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elKByn7-dmsi",
    "outputId": "f28831f0-ddf9-42a0-f789-11ec32043c88",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a large tensor, push it to device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 1000, 1000).to(device) \n",
    "x.dtype, x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\n",
    "    \"PREDICTION: We just created a tensor of size (1000, 1000, 1000). How much GPU memory do you think this will use?\", \n",
    "    \"02-02\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del x\n",
    "\n",
    "# the gpu memory may not be freed right away\n",
    "# we can explicitly free\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors must be on the same device**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.randn(3,3, device = torch.device('cuda')) # GPU\n",
    "b = torch.randn(3,3) # default\n",
    "\n",
    "try:\n",
    "    print(a+b)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**This will happen to you a lot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # tensor is too big to fit on our GPU!\n",
    "    x = torch.randn(int(3e10), device=torch.device('cuda'))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution: find ways to use less memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How much does GPU actually help?**\n",
    "\n",
    "Let's test by multiplying a large matrix with itself. \n",
    "\n",
    "Before running the below code cell: we are multiplying two 10,000 x 10,000 matrices on CPU vs GPU. How much faster do you think the GPU will be? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRarDQ-jdmsj",
    "outputId": "8e427e6f-e547-48d8-d844-f21af0cf1e8b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = torch.randn(10_000, 10_000)\n",
    "\n",
    "## CPU version\n",
    "start_time = time.time()\n",
    "_ = torch.matmul(x, x)\n",
    "end_time = time.time()\n",
    "cpu_time = end_time - start_time\n",
    "print(f\"CPU time: {(cpu_time):6.5f}s\")\n",
    "\n",
    "## GPU version\n",
    "x = x.to(device)\n",
    "_ = torch.matmul(x, x)  # First operation to 'burn in' GPU\n",
    "# CUDA is asynchronous, so we need to use different timing functions\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "_ = torch.matmul(x, x)\n",
    "end.record()\n",
    "torch.cuda.synchronize()  # Waits for everything to finish running on the GPU\n",
    "gpu_time = start.elapsed_time(end) / 1000  # Milliseconds to seconds\n",
    "print(f\"GPU time: {gpu_time:6.5f}s\")  # Milliseconds to seconds\n",
    "\n",
    "# How much faster is the GPU?\n",
    "print(f\"The GPU is {cpu_time / gpu_time:6.1f}x faster than the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mu9ij8DNdmsh"
   },
   "source": [
    "## Computation Graph and Backpropagation\n",
    "\n",
    "One of the main reasons for using PyTorch in Deep Learning projects is that we can automatically get _gradients/derivatives_ of functions that we define. We will mainly use PyTorch for implementing neural networks, and they are just fancy functions. If we use weight matrices in our function that we want to learn, then those are called the _parameters_ or simply the _weights_. The ability to compute gradients is essential for optimizing (a.k.a. _training_) our networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have a `requires_grad` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0LdDse8eSHG",
    "outputId": "ced73056-1ffb-42b7-b445-b7e7c7bdd6be",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.ones((3,))\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VKSErNZeSHG"
   },
   "source": [
    "We can change this for an existing tensor using the function `requires_grad_()` (underscore indicating that this is a in-place operation). Alternatively, when creating a tensor, you can pass the argument `requires_grad=True` to most initializers we have seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onvIPOuqeSHG",
    "outputId": "f2aa9b48-d6bd-4e1d-89d7-88ee481574d4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LUDKGM_eSHG"
   },
   "source": [
    "In order to get familiar with the concept of a computation graph, we will create one for the following function:\n",
    "\n",
    "$$y = \\frac{1}{\\mathrm{dim}(x)}\\sum_i \\left[(x_i + 2)^2 + 3\\right]$$\n",
    "\n",
    "You could imagine that $x$ are our parameters, and we want to optimize (either maximize or minimize) the output $y$. For this, we want to obtain the gradients $\\partial y / \\partial \\mathbf{x}$. For our example, we'll use $\\mathbf{x}=[0,1,2]$ as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4IHsXE9eSHG",
    "outputId": "db63d40d-ec86-45be-edd8-534b844497e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.arange(3, dtype=torch.float32, requires_grad=True) # Only float tensors can have gradients\n",
    "print(\"X\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQxbW_9GeSHG"
   },
   "source": [
    "Now let's build the computation graph step by step. You can combine multiple operations in a single line, but we will separate them here to get a better understanding of how each operation is added to the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tle5CWjeSHG",
    "outputId": "6d0bf301-21fb-481f-96d0-0c3a5e062be3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = x + 2\n",
    "b = a ** 2\n",
    "c = b + 3\n",
    "y = c.mean()\n",
    "print(\"Y\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om7CAzKSeSHH"
   },
   "source": [
    "Using the statements above, we have created a computation graph that looks similar to the figure below:\n",
    "\n",
    "<center style=\"width: 100%\"><img src=\"https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch/forward_computation.svg\" width=\"200px\"></center>\n",
    "\n",
    "We calculate $a$ based on the inputs $x$ and the constant $2$, $b$ is $a$ squared, and so on. The visualization is an abstraction of the dependencies between inputs and outputs of the operations we have applied.\n",
    "\n",
    "Each node in the computation graph has automatically defined a function for calculating the gradients with respect to its inputs, `grad_fn`. Pytorch can use the chain rule to automatically compute the gradients of $y$ with respect to any of the inputs that have `requires_grad=True`. Pytorch does this by traversing the computation graph backward, applying the `grad_fn` defined at each operation. This is called \"backpropagation\". \n",
    "\n",
    "<center style=\"width: 100%\"><img src=\"https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch/backward_computation.svg\" width=\"200px\"></center>\n",
    "\n",
    "\n",
    "We can perform backpropagation on the computation graph by calling the function `backward()` on the last output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IEPtOT9eSHH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTKtEVIIeSHH"
   },
   "source": [
    "`x.grad` will now contain the gradient $\\partial y/ \\partial \\mathcal{x}$, and this gradient indicates how a change in $\\mathbf{x}$ will affect output $y$ given the current input $\\mathbf{x}=[0,1,2]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0fG6wmBeSHH",
    "outputId": "9af3254c-a2e0-4b95-9a2a-2b8c801891cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GOFlzi0eSHH"
   },
   "source": [
    "We can also verify these gradients by hand. We will calculate the gradients using the chain rule, in the same way as PyTorch did it:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial c_i}\\frac{\\partial c_i}{\\partial b_i}\\frac{\\partial b_i}{\\partial a_i}\\frac{\\partial a_i}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "Note that we have simplified this equation to index notation, and by using the fact that all operation besides the mean do not combine the elements in the tensor. The partial derivatives are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a_i}{\\partial x_i} = 1,\\hspace{1cm}\n",
    "\\frac{\\partial b_i}{\\partial a_i} = 2\\cdot a_i\\hspace{1cm}\n",
    "\\frac{\\partial c_i}{\\partial b_i} = 1\\hspace{1cm}\n",
    "\\frac{\\partial y}{\\partial c_i} = \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "Hence, with the input being $\\mathbf{x}=[0,1,2]$, our gradients are $\\partial y/\\partial \\mathbf{x}=[4/3,2,8/3]$. The previous code cell should have printed the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE CHALLENGE: Use pytorch to find the gradient of the multivariate function f(x, y) = x^2 + 2y^2 at the point (x, y) = (1, 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_answer_box(\"Copy/paste your code to find the gradient of f(x, y) = x^2 + 2y^2 at (1, 2)\", \"02-03\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PytorchWorkshop",
   "language": "python",
   "name": "pytorchworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
