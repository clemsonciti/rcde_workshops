{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c39f2d1",
   "metadata": {},
   "source": [
    "### Dataset: Demo Corpus\n",
    "\n",
    "We will use a tiny mixed-domain corpus (AI, Climate, Biomedical, Materials) stored in `data/demo_corpus.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae0d1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 docs from data/demo_corpus.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2508.05366</td>\n",
       "      <td>Can Language Models Critique Themselves? Inves...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[NLP, Retrieval, Language Model, Biomedical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2508.07326</td>\n",
       "      <td>Nonparametric Reaction Coordinate Optimization...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[ML, Climate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2508.07654</td>\n",
       "      <td>MLego: Interactive and Scalable Topic Explorat...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[Databases, IR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2508.07798</td>\n",
       "      <td>Generative Inversion for Property-Targeted Mat...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[Materials, ML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2508.08140</td>\n",
       "      <td>Data-Efficient Biomedical In-Context Learning:...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[NLP, Retrieval, Language Model, Biomedical]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  year  \\\n",
       "0  2508.05366  Can Language Models Critique Themselves? Inves...  2025   \n",
       "1  2508.07326  Nonparametric Reaction Coordinate Optimization...  2025   \n",
       "2  2508.07654  MLego: Interactive and Scalable Topic Explorat...  2025   \n",
       "3  2508.07798  Generative Inversion for Property-Targeted Mat...  2025   \n",
       "4  2508.08140  Data-Efficient Biomedical In-Context Learning:...  2025   \n",
       "\n",
       "                                         topics  \n",
       "0  [NLP, Retrieval, Language Model, Biomedical]  \n",
       "1                                 [ML, Climate]  \n",
       "2                               [Databases, IR]  \n",
       "3                               [Materials, ML]  \n",
       "4  [NLP, Retrieval, Language Model, Biomedical]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/demo_corpus.jsonl'\n",
    "df = pd.read_json(DATA_PATH, lines=True)\n",
    "docs = df.to_dict('records')\n",
    "print(f'Loaded {len(docs)} docs from {DATA_PATH}')\n",
    "display(df[['id','title','year','topics']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-title",
   "metadata": {},
   "source": [
    "# Module 3: Building a Simple RAG Pipeline\n",
    "\n",
    "*Part of the RCD Workshops series: Retrieval-Augmented Generation (RAG) for Advanced Research Applications*\n",
    "\n",
    "---\n",
    "\n",
    "In this module, we'll connect retrieval and generation to build a working RAG pipeline end-to-end.\n",
    "We'll use our small example corpus (from Module 2), a retrieval component, and a 7B LLM, to show how RAG works in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0462bb",
   "metadata": {},
   "source": [
    "![RAG pipeline](rag_pipeline_graphviz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-step-setup-llm",
   "metadata": {},
   "source": [
    "## 3.1 Setting up the LLM\n",
    "For RAG, we need a language model that can read our prompt and generate an answer using retrieved context. We'll use Qwen-7B (open-source, Hugging Face) for this pipeline.\n",
    "\n",
    "> **Note:** You need a GPU (ideally A100 or similar) to load a 7B model at usable speed.\n",
    "\n",
    "We'll use the `transformers` library. Loading may take a while (model is ~14GB in 16-bit mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mod3-llm-load",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a89fc62282b4aed8475c489f3484ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/huggingface_hub/utils/_hf_folder.py:95: UserWarning: A token has been found in `/home/cehrett/.huggingface/token`. This is the old path where tokens were stored. The new location is `/scratch/cehrett/hf_cache/token` which is configurable using `HF_HOME` environment variable. Your token has been copied to this new location. You can now safely delete the old token file manually or use `huggingface-cli logout`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770ee6b9a1624ca4ada360c85912bcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class QWenTokenizer does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQwen/Qwen-7B\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# Or Qwen/Qwen-7B-Chat for instruct\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      8\u001b[0m     model_name,\n\u001b[1;32m      9\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:724\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    725\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m         )\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer class QWenTokenizer does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install transformers accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = 'Qwen/Qwen-7B'   # Or Qwen/Qwen-7B-Chat for instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    torch_dtype='auto'\n",
    ")\n",
    "# Model and tokenizer are now ready for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed32a01",
   "metadata": {},
   "source": [
    "![RAG pipeline](rag_pipeline_graphviz.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-rag-pipeline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assume: encoder, docs, doc embeddings, index from Module 2\n",
    "# Example question for climate economics\n",
    "query = \"According to recent studies, how much could global GDP decline at 3¬∞C of warming, and which regions are hit hardest?\"\n",
    "query_vec = encoder.encode([query])\n",
    "query_vec = query_vec / np.linalg.norm(query_vec)  # normalize for cosine\n",
    "\n",
    "top_k = 2\n",
    "D, I = index.search(query_vec, k=top_k)\n",
    "retrieved_indices = I[0]\n",
    "print(\"Retrieved doc indices:\", retrieved_indices)\n",
    "retrieved_texts = [docs[i] for i in retrieved_indices]\n",
    "print(\"Top-1 Retrieved text snippet:\", retrieved_texts[0][:60], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-compose-prompt",
   "metadata": {},
   "source": [
    "### Building the Prompt\n",
    "To maximize answer quality, prompt your LLM with clear instructions and insert the most relevant docs just before the user's question.\n",
    "A simple format is to list docs like [Document 1], [Document 2], then give the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-prompt-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_intro = \"You are a research assistant. Use the following documents to answer the question.\\n\"\n",
    "docs_section = \"\"\n",
    "for idx, text in enumerate(retrieved_texts, start=1):\n",
    "    docs_section += f\"[Document {idx}]\\n{text}\\n\\n\"\n",
    "question_section = f\"Question: {query}\\nAnswer:\"\n",
    "\n",
    "prompt = prompt_intro + docs_section + question_section\n",
    "print(\"Prompt sent to LLM:\\n\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-llm-generate-explain",
   "metadata": {},
   "source": [
    "### LLM: Answering with Retrieved Information\n",
    "Now, send the composed prompt to your language model.\n",
    "> This step may be slow unless you're on a GPU-ready machine, but shows the full RAG loop!\n",
    "If working on CPU or want to skip, use a smaller LLM (optionally ask facilitator for alternatives).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-llm-generate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids, max_length=256,\n",
    "                         temperature=0.2, do_sample=False)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Answer:\\n\", answer[len(prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-try-yours",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "Modify the `query` above (in the RAG pipeline code cell) to something your document can answer -- or to something *none* of the docs cover.\n",
    "What happens? How does the retrieval affect the model's output?\n",
    "\n",
    "> *Reflection: What are the main components of a simple RAG pipeline? (List at least two)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-answer-box",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box('üìù **Your Answer:** The RAG pipeline consists of ...', question_id='mod3_pipeline_components')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-context-limits",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note on Prompt Lengths & Context:**\n",
    "Models like Qwen-7B support long context windows (up to 8K tokens or more), but you often need to truncate or focus your retrieved docs.\n",
    "Too much, and the model may ignore key info; too little, and you could miss relevant context.\n",
    "\n",
    "That's why retrieval *quality* is just as important as the LLM itself!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-end-wrap",
   "metadata": {},
   "source": [
    "\n",
    "Congratulations‚ÄîYou now have a basic, working RAG pipeline!\n",
    "In the next module, we'll explore how to improve retrieval quality and tackle more advanced scenarios.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
