{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Demo Corpus\n",
    "\n",
    "We will use a tiny mixed-domain corpus (AI, Climate, Biomedical, Materials) stored in `data/demo_corpus.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport pandas as pd\n\nDATA_PATH = 'data/demo_corpus.jsonl'\ndf = pd.read_json(DATA_PATH, lines=True)\ndocs = df.to_dict('records')\nprint(f'Loaded {len(docs)} docs from {DATA_PATH}')\ndisplay(df[['id','title','year','topics']].head())\n"
  },
  {
   "cell_type": "markdown",
   "id": "mod3-title",
   "metadata": {},
   "source": [
    "# Module 3: Building a Simple RAG Pipeline\n",
    "\n",
    "*Part of the RCD Workshops series: Retrieval-Augmented Generation (RAG) for Advanced Research Applications*\n",
    "\n",
    "---\n",
    "\n",
    "In this module, we'll connect retrieval and generation to build a working RAG pipeline end-to-end.\n",
    "We'll use our small example corpus (from Module 2), a retrieval component, and a 7B LLM, to show how RAG works in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG pipeline](rag_pipeline_graphviz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-step-setup-llm",
   "metadata": {},
   "source": [
    "## 3.1 Setting up the LLM\n",
    "For RAG, we need a language model that can read our prompt and generate an answer using retrieved context. We'll use Qwen-7B (open-source, Hugging Face) for this pipeline.\n",
    "\n",
    "> **Note:** You need a GPU (ideally A100 or similar) to load a 7B model at usable speed.\n",
    "\n",
    "We'll use the `transformers` library. Loading may take a while (model is ~14GB in 16-bit mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-llm-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install transformers accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = 'Qwen/Qwen-7B'   # Or Qwen/Qwen-7B-Chat for instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    torch_dtype='auto'\n",
    ")\n",
    "# Model and tokenizer are now ready for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG pipeline](rag_pipeline_graphviz.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-rag-pipeline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Assume: encoder, docs, doc embeddings, index from Module 2\n",
    "# Example question for climate economics\n",
    "query = \"According to recent studies, how much could global GDP decline at 3¬∞C of warming, and which regions are hit hardest?\"\n",
    "query_vec = encoder.encode([query])\n",
    "query_vec = query_vec / np.linalg.norm(query_vec)  # normalize for cosine\n",
    "\n",
    "top_k = 2\n",
    "D, I = index.search(query_vec, k=top_k)\n",
    "retrieved_indices = I[0]\n",
    "print(\"Retrieved doc indices:\", retrieved_indices)\n",
    "retrieved_texts = [docs[i] for i in retrieved_indices]\n",
    "print(\"Top-1 Retrieved text snippet:\", retrieved_texts[0][:60], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-compose-prompt",
   "metadata": {},
   "source": [
    "### Building the Prompt\n",
    "To maximize answer quality, prompt your LLM with clear instructions and insert the most relevant docs just before the user's question.\n",
    "A simple format is to list docs like [Document 1], [Document 2], then give the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-prompt-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_intro = \"You are a research assistant. Use the following documents to answer the question.\\n\"\n",
    "docs_section = \"\"\n",
    "for idx, text in enumerate(retrieved_texts, start=1):\n",
    "    docs_section += f\"[Document {idx}]\\n{text}\\n\\n\"\n",
    "question_section = f\"Question: {query}\\nAnswer:\"\n",
    "\n",
    "prompt = prompt_intro + docs_section + question_section\n",
    "print(\"Prompt sent to LLM:\\n\")\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-llm-generate-explain",
   "metadata": {},
   "source": [
    "### LLM: Answering with Retrieved Information\n",
    "Now, send the composed prompt to your language model.\n",
    "> This step may be slow unless you're on a GPU-ready machine, but shows the full RAG loop!\n",
    "If working on CPU or want to skip, use a smaller LLM (optionally ask facilitator for alternatives).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-llm-generate-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids, max_length=256,\n",
    "                         temperature=0.2, do_sample=False)\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nGenerated Answer:\\n\", answer[len(prompt):])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-try-yours",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "Modify the `query` above (in the RAG pipeline code cell) to something your document can answer -- or to something *none* of the docs cover.\n",
    "What happens? How does the retrieval affect the model's output?\n",
    "\n",
    "> *Reflection: What are the main components of a simple RAG pipeline? (List at least two)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-answer-box",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box('üìù **Your Answer:** The RAG pipeline consists of ...', question_id='mod3_pipeline_components')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-context-limits",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note on Prompt Lengths & Context:**\n",
    "Models like Qwen-7B support long context windows (up to 8K tokens or more), but you often need to truncate or focus your retrieved docs.\n",
    "Too much, and the model may ignore key info; too little, and you could miss relevant context.\n",
    "\n",
    "That's why retrieval *quality* is just as important as the LLM itself!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-end-wrap",
   "metadata": {},
   "source": [
    "\nCongratulations‚ÄîYou now have a basic, working RAG pipeline!\n",
    "In the next module, we'll explore how to improve retrieval quality and tackle more advanced scenarios.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchWorkshop",
   "language": "python",
   "name": "pytorchworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}