{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mod3-title",
   "metadata": {},
   "source": [
    "# Module 3: Building a Simple RAG Pipeline\n",
    "\n",
    "*Part of the RCD Workshops series: Retrieval-Augmented Generation (RAG) for Advanced Research Applications*\n",
    "\n",
    "---\n",
    "\n",
    "In this module, we'll connect retrieval and generation to build a working RAG pipeline end-to-end.\n",
    "We'll use our small example corpus (from Module 2), a retrieval component, and an 8B LLM, to show how RAG works in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0462bb",
   "metadata": {},
   "source": [
    "![RAG pipeline](rag_pipeline_graphviz.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39f2d1",
   "metadata": {},
   "source": [
    "### Dataset: Demo Corpus\n",
    "\n",
    "We will use a tiny mixed-domain corpus (AI, Climate, Biomedical, Materials) stored in `data/demo_corpus.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae0d1cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 docs from data/demo_corpus.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2508.05366</td>\n",
       "      <td>Can Language Models Critique Themselves? Inves...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[NLP, Retrieval, Language Model, Biomedical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2508.07326</td>\n",
       "      <td>Nonparametric Reaction Coordinate Optimization...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[ML, Climate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2508.07654</td>\n",
       "      <td>MLego: Interactive and Scalable Topic Explorat...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[Databases, IR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2508.07798</td>\n",
       "      <td>Generative Inversion for Property-Targeted Mat...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[Materials, ML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2508.08140</td>\n",
       "      <td>Data-Efficient Biomedical In-Context Learning:...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[NLP, Retrieval, Language Model, Biomedical]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  year  \\\n",
       "0  2508.05366  Can Language Models Critique Themselves? Inves...  2025   \n",
       "1  2508.07326  Nonparametric Reaction Coordinate Optimization...  2025   \n",
       "2  2508.07654  MLego: Interactive and Scalable Topic Explorat...  2025   \n",
       "3  2508.07798  Generative Inversion for Property-Targeted Mat...  2025   \n",
       "4  2508.08140  Data-Efficient Biomedical In-Context Learning:...  2025   \n",
       "\n",
       "                                         topics  \n",
       "0  [NLP, Retrieval, Language Model, Biomedical]  \n",
       "1                                 [ML, Climate]  \n",
       "2                               [Databases, IR]  \n",
       "3                               [Materials, ML]  \n",
       "4  [NLP, Retrieval, Language Model, Biomedical]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/demo_corpus.jsonl'\n",
    "df = pd.read_json(DATA_PATH, lines=True)\n",
    "docs = df.to_dict('records')\n",
    "print(f'Loaded {len(docs)} docs from {DATA_PATH}')\n",
    "display(df[['id','title','year','topics']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-step-setup-llm",
   "metadata": {},
   "source": [
    "## 3.1 Setting up the LLM\n",
    "For RAG, we need a language model that can read our prompt and generate an answer using retrieved context. We'll use Qwen-7B (open-source, Hugging Face) for this pipeline.\n",
    "\n",
    "> **Note:** You need a GPU (ideally A100 or similar) to load a 7B model at usable speed.\n",
    "\n",
    "We'll use the `transformers` library. Loading may take a while (model is ~14GB in 16-bit mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mod3-llm-load",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LLM: Qwen/Qwen3-0.6B\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install transformers accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = 'Qwen/Qwen3-0.6B'  # Or Qwen/Qwen3-8B\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    torch_dtype='auto'\n",
    ")\n",
    "MODEL_READY = True\n",
    "print(f'Loaded LLM: {model_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed32a01",
   "metadata": {},
   "source": [
    "![RAG pipeline](rag_pipeline_graphviz.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mod3-rag-pipeline-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Build chunked passage index from abstracts\n",
    "def chunk_text(text, max_chars=400):\n",
    "    text = (text or '').strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return [text[i:i+max_chars].strip() for i in range(0, len(text), max_chars)]\n",
    "\n",
    "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "chunk_texts = []\n",
    "chunk_meta = []\n",
    "for d in docs:\n",
    "    abs_text = d.get('abstract', '')\n",
    "    pieces = chunk_text(abs_text, max_chars=400)\n",
    "    for j, t in enumerate(pieces):\n",
    "        if not t:\n",
    "            continue\n",
    "        chunk_texts.append(t)\n",
    "        chunk_meta.append({'doc_id': d.get('id'), 'title': d.get('title'), 'chunk_id': j})\n",
    "\n",
    "embs = encoder.encode(chunk_texts)\n",
    "embs = np.array([v/np.linalg.norm(v) for v in embs], dtype='float32')\n",
    "index = faiss.IndexFlatIP(embs.shape[1])\n",
    "index.add(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "858cd17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved chunk indices: [39 40]\n",
      "Top-1 Retrieved text snippet: Afforestation and reforestation are popular strategies for mitigating climate change by enhancing carbon sequestration. However, the effectiveness of these effo ...\n"
     ]
    }
   ],
   "source": [
    "# Example question for climate economics\n",
    "query = \"According to recent studies, how exactly does replanting trees to replenish forests help to fight against climate change?\"\n",
    "q = encoder.encode([query])[0]\n",
    "q = (q/np.linalg.norm(q)).astype('float32')\n",
    "D, I = index.search(np.array([q]), k=2)\n",
    "retrieved_indices = I[0]\n",
    "print('Retrieved chunk indices:', retrieved_indices)\n",
    "retrieved_texts = [chunk_texts[i] for i in retrieved_indices]\n",
    "retrieved_meta = [chunk_meta[i] for i in retrieved_indices]\n",
    "print('Top-1 Retrieved text snippet:', retrieved_texts[0][:160].replace('\\n',' '), '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-compose-prompt",
   "metadata": {},
   "source": [
    "### Building the Prompt\n",
    "To maximize answer quality, prompt your LLM with clear instructions and insert the most relevant docs just before the user's question.\n",
    "A simple format is to list docs like [Document 1], [Document 2], then give the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mod3-prompt-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt (rendered):\n",
      "\n",
      "<|im_start|>system\n",
      "You are a research assistant. Ground your answer in the provided documents. Cite document numbers inline when useful. If unsure, say you don't know.<|im_end|>\n",
      "<|im_start|>user\n",
      "Context:\n",
      "[Document 1]\n",
      "Afforestation and reforestation are popular strategies for mitigating climate change by enhancing carbon sequestration. However, the effectiveness of these efforts is often self-reported by project developers, or certified through processes with limited external validation. This leads to concerns about data reliability and project integrity. In response to increasing scrutiny of voluntary carbon m\n",
      "[Document 2]\n",
      "arkets, this study presents a dataset on global afforestation and reforestation efforts compiled from primary (meta-)information and augmented with time-series satellite imagery and other secondary data. Our dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years. Since any remote sensing-based validation effort relies on the integrity of a planting site's geographic boundar\n",
      "\n",
      "Question: According to recent studies, how exactly does replanting trees to replenish forests help to fight against climate change?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build messages for chat template aware models (e.g., Qwen3)\n",
    "system_msg = (\n",
    "    \"You are a research assistant. Ground your answer in the provided documents. \"\n",
    "    \"Cite document numbers inline when useful. If unsure, say you don't know.\"\n",
    ")\n",
    "\n",
    "docs_lines = []\n",
    "for i, text in enumerate(retrieved_texts, start=1):\n",
    "    docs_lines.append(f'[Document {i}]\\n{text}\\n')\n",
    "context_block = \"\".join(docs_lines)\n",
    "user_msg = f\"Context:\\n{context_block}\\nQuestion: {query}\"\n",
    "\n",
    "messages = [\n",
    "    { 'role': 'system', 'content': system_msg },\n",
    "    { 'role': 'user',   'content': user_msg },\n",
    "]\n",
    "\n",
    "# Render with chat template\n",
    "rendered_prompt = None\n",
    "\n",
    "rendered_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print('Prompt (rendered):\\n')\n",
    "print(rendered_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-llm-generate-explain",
   "metadata": {},
   "source": [
    "### LLM: Answering with Retrieved Information\n",
    "Now, send the composed prompt to your language model.\n",
    "> This step may be slow unless you're on a GPU-ready machine, but shows the full RAG loop!\n",
    "If working on CPU or want to skip, use a smaller LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mod3-llm-generate-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Answer: system\n",
      "You are a research assistant. Ground your answer in the provided documents. Cite document numbers inline when useful. If unsure, say you don't know.\n",
      "user\n",
      "Context:\n",
      "[Document 1]\n",
      "Afforestation and reforestation are popular strategies for mitigating climate change by enhancing carbon sequestration. However, the effectiveness of these efforts is often self-reported by project developers, or certified through processes with limited external validation. This leads to concerns about data reliability and project integrity. In response to increasing scrutiny of voluntary carbon m\n",
      "[Document 2]\n",
      "arkets, this study presents a dataset on global afforestation and reforestation efforts compiled from primary (meta-)information and augmented with time-series satellite imagery and other secondary data. Our dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years. Since any remote sensing-based validation effort relies on the integrity of a planting site's geographic boundar\n",
      "\n",
      "Question: According to recent studies, how exactly does replanting trees to replenish forests help to fight against climate change?\n",
      "assistant\n",
      "<think>\n",
      "Okay, let's see. The user is asking how replanting trees helps fight against climate change based on recent studies. I need to check the provided documents to find relevant information.\n",
      "\n",
      "Looking at Document 1, it mentions that afforestation and reforestation help mitigate climate change by enhancing carbon sequestration. But it also notes that the effectiveness is often self-reported or certified with limited external validation. So, the main point here is that these efforts help in carbon sequestration, which is a key part of combating climate change.\n",
      "\n",
      "Document 2 talks about a dataset compiled from primary and secondary data, covering 1.28 million planting sites over 33 years. It also mentions that remote sensing validation relies on the site's geographic boundaries. However, the user's question is about the mechanism of replanting trees. \n",
      "\n",
      "Wait, the answer should be based on the information given. Document 1 directly states that replanting trees replenishes forests, which helps in carbon sequestration. The studies mentioned in Document 2 are about the dataset, but the user is asking about the mechanism, not the dataset. So the answer should focus on carbon sequestration as per Document 1. \n",
      "\n",
      "I need to make sure not to use any other documents. The user's question is about the mechanism, so the answer should be concise, citing Document 1. The answer should mention that replanting trees replenishes forests, which leads to carbon sequestration, thus helping combat climate change. No other documents are needed here.\n",
      "</think>\n",
      "\n",
      "According to recent studies, replanting trees to replenish forests helps combat climate change by enhancing carbon sequestration. This process increases the amount of carbon stored in the atmosphere, which mitigates global warming. Document 1 explicitly states that afforestation and reforestation enhance carbon sequestration, and Document 2 supports this by highlighting the role of such efforts in carbon storage. Thus, the mechanism is directly tied to carbon sequestration.\n"
     ]
    }
   ],
   "source": [
    "if 'MODEL_READY' in globals() and MODEL_READY:\n",
    "    input_ids = tokenizer(rendered_prompt, return_tensors='pt').input_ids.to(model.device)\n",
    "    outputs = model.generate(input_ids, max_new_tokens=512, temperature=0.2, do_sample=False)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Try to strip the prompt if possible (best-effort)\n",
    "    try:\n",
    "        start = answer.find(rendered_prompt)\n",
    "        if start != -1:\n",
    "            answer = answer[start + len(rendered_prompt):]\n",
    "    except Exception:\n",
    "        pass\n",
    "    print('\\nGenerated Answer:', answer.strip())\n",
    "else:\n",
    "    print('\\n[Skipped LLM generation] Model not loaded. Review the previous cell output for setup instructions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-try-yours",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "Modify the `query` above (in the RAG pipeline code cell) to something your document can answer -- or to something *none* of the docs cover.\n",
    "What happens? How does the retrieval affect the model's output?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mod3-answer-box",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box('In the above code in this notebook, what does the line `q = encoder.encode([query])[0]` do?', question_id='encoder_question')\n",
    "\n",
    "create_answer_box('In the above code in this notebook, what does the line `D, I = index.search(np.array([q]), k=2)` do?', question_id='index_question')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-context-limits",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note on Prompt Lengths & Context:**\n",
    "Models like Qwen3-8B support long context windows (up to 32K tokens or more), but you often need to truncate or focus your retrieved docs.\n",
    "Too much, and the model may ignore key info; too little, and you could miss relevant context.\n",
    "\n",
    "That's why retrieval *quality* is just as important as the LLM itself!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-end-wrap",
   "metadata": {},
   "source": [
    "\n",
    "Congratulations—You now have a basic, working RAG pipeline!\n",
    "In the next module, we'll explore how to improve retrieval quality and tackle more advanced scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mod3-streamlined-intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Streamlined RAG (Library-Based)\n",
    "\n",
    "The above walkthrough showed a ground-up RAG pipeline. Below is a concise version using a popular orchestration library to wire up embeddings, a vector store, a retriever, and an LLM chain.\n",
    "\n",
    "This mirrors what many teams do in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "mod3-streamlined-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vector store with LangChain (auto-chunk + FAISS) ...\n",
      "- Loaded 18 source documents\n",
      "- Created 74 chunks (chunk_size=400, overlap=40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'do_sample': True}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping Transformers model as an LLM pipeline...\n",
      "Querying streamlined RAG chain...\n",
      "\n",
      "Answer: System: You are a research assistant. Use the provided context to answer. Cite titles when helpful. If unsure, say you don't know.\n",
      "Human: Context:\n",
      "Afforestation and reforestation are popular strategies for mitigating climate change by enhancing carbon sequestration. However, the effectiveness of these efforts is often self-reported by project developers, or certified through processes with limited external validation. This leads to concerns about data reliability and project integrity. In response to increasing scrutiny of voluntary carbon\n",
      "\n",
      "increasing scrutiny of voluntary carbon markets, this study presents a dataset on global afforestation and reforestation efforts compiled from primary (meta-)information and augmented with time-series satellite imagery and other secondary data. Our dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years. Since any remote sensing-based validation effort relies on the\n",
      "\n",
      "Question: According to recent studies, how exactly does replanting trees to replenish forests help to fight against climate change? (a) It helps to replenish the forest by increasing the number of trees. (b) It helps to replenish the forest by decreasing the number of trees. (c) It helps to replenish the forest by increasing the number of trees. (d) It helps to replenish the forest by decreasing the number of trees. (e) It helps to replenish the forest by increasing the number of trees. (f) It helps to replenish the forest by decreasing the number of trees.\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l)\n",
      "\n",
      "Choices: (a) (c) (d) (e) (f) (g) (h) (i\n",
      "\n",
      "Top sources:\n",
      "- Source 1: A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts (id=2508.11349)\n",
      "- Source 2: A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts (id=2508.11349)\n"
     ]
    }
   ],
   "source": [
    "# Optional: install helpers if missing\n",
    "# Recommended: install compatible LangChain packages\n",
    "# %pip install -U \"langchain>=0.2.16\" \"langchain-core>=0.2.38\" \"langchain-community>=0.2.16\" \"langchain-huggingface>=0.0.6\" \"langchain-text-splitters>=0.2.2\" jsonpatch\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS as LCFAISS\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "print('Building vector store with LangChain (auto-chunk + FAISS) ...')\n",
    "emb = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Wrap raw records as LangChain Documents, carrying metadata\n",
    "raw_docs = []\n",
    "for d in docs:\n",
    "    text = (d.get('abstract') or '').strip()\n",
    "    if not text:\n",
    "        continue\n",
    "    md = { 'doc_id': d.get('id'), 'title': d.get('title'), 'year': d.get('year') }\n",
    "    raw_docs.append(Document(page_content=text, metadata=md))\n",
    "print(f'- Loaded {len(raw_docs)} source documents')\n",
    "\n",
    "# Split automatically with a standard text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40, separators=['\\n\\n','\\n',' ', ''])\n",
    "docs_split = splitter.split_documents(raw_docs)\n",
    "print(f'- Created {len(docs_split)} chunks (chunk_size=400, overlap=40)')\n",
    "\n",
    "# Build FAISS vector store directly from Documents\n",
    "vs = LCFAISS.from_documents(docs_split, embedding=emb)\n",
    "retriever = vs.as_retriever(search_type='similarity', search_kwargs={'k': 2})\n",
    "\n",
    "print('Wrapping Transformers model as an LLM pipeline...')\n",
    "gen = hf_pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    do_sample=False,\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=gen)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a research assistant. Use the provided context to answer. Cite titles when helpful. If unsure, say you don\\'t know.'),\n",
    "    ('human', 'Context:\\n{context}\\n\\nQuestion: {input}')\n",
    "])\n",
    "\n",
    "doc_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, doc_chain)\n",
    "\n",
    "print('Querying streamlined RAG chain...')\n",
    "# create_retrieval_chain expects the user query under key \"input\"\n",
    "res = rag_chain.invoke({'input': query})\n",
    "answer = res.get('answer') or res.get('output_text', '')\n",
    "print('\\nAnswer:', answer.strip())\n",
    "\n",
    "print('\\nTop sources:')\n",
    "for i, d in enumerate(res.get('context', []), 1):\n",
    "    md = getattr(d, 'metadata', {}) or {}\n",
    "    title = md.get('title', '')\n",
    "    doc_id = md.get('doc_id', '')\n",
    "    print(f'- Source {i}: {title[:80]} (id={doc_id})')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-rag-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
