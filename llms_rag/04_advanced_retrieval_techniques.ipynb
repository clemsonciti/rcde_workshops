{
 "cells": [
      {
        "cell_type": "markdown",
        "id": "mod4-title",
        "metadata": {},
        "source": [
          "# Module 4: Advanced Retrieval Techniques\n",
          "\n",
          "*Part of the RCD Workshops series: Retrieval-Augmented Generation (RAG) for Advanced Research Applications*\n",
          "\n",
          "---\n",
          "\n",
          "Good retrieval is essential for RAG! In this module, you'll learn about enhancements far beyond simple vector search.\n"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "mod4-overview",
        "metadata": {},
        "source": [
          "## Why go beyond the basics?\n",
          "If your pipeline can't fetch the right info, the LLM can't answer correctly. Let's look at state-of-the-art research & practical strategies to improve retrieval in RAG systems."
        ]
      },

      {
        "cell_type": "markdown",
        "id": "mod4-hybrid",
        "metadata": {},
        "source": [
          "### 4.1 Hybrid Search (Semantic + Lexical)\n",
          "\n",
          "Pure vector retrieval (using embeddings) sometimes misses exact matches for names, numbers, or domain jargon‚Äîthis is the **lexical gap**. Hybrid search combines **keyword-based retrieval** (like BM25 or even word overlap) with **vector-based retrieval**. Keyword search is precise for rare terms, vector search is good for synonyms/concepts. Together, they cover each other's weaknesses.\n",
          "\n",
          "> **Diagram placeholder:** Schematic of hybrid search: keyword engine + vector engine whose results are merged before sending to LLM.\n"
        ]
      },

      {
        "cell_type": "markdown",
        "id": "mod4-hybrid-demo-desc",
        "metadata": {},
        "source": [
          "**Demo: Let's simulate a hybrid search for a specific keyword (e.g. '3¬∞C').**\n",
          "In a real system, you could use libraries like Whoosh (BM25), or ElasticSearch, or filter vector results by keyword presence. Here we combine vector retrieval and a keyword filter in pure Python."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "mod4-hybrid-demo-code",
        "metadata": {},
        "outputs": [],
        "source": [
          "# Hybrid search demonstration\n",
          "query = \"What is the GDP loss at 3¬∞C warming?\"\n",
          "query_vec = encoder.encode([query])\n",
          "query_vec = query_vec / np.linalg.norm(query_vec)\n",
          "\n",
          "# Vector search\n",
          "D, I = index.search(query_vec, k=2)\n",
          "candidates = list(I[0])\n",
          "\n",
          "# Keyword filter: boost any document containing '3¬∞C'\n",
          "keyword = \"3¬∞C\"\n",
          "for idx, text in enumerate(docs):\n",
          "    if keyword in text:\n",
          "        if idx not in candidates:\n",
          "            candidates.append(idx)\n",
          "\n",
          "print(\"Candidates by hybrid criteria:\", candidates)\n",
          "# You can choose to rerank these based on similarity or other heuristics\n"
        ]
      },

      {
        "cell_type": "markdown",
        "id": "mod4-rerank",
        "metadata": {},
        "source": [
          "### 4.2 Reranking with Cross-Encoders\n",
          "\n",
          "Fast embedding models give a rough score. A **cross-encoder reranker** (like a small BERT or the LLM itself) looks at a (query, passage) pair together to judge relevance more accurately.\n",
          "In practice: use vector search for top-k, then rerank those with a cross-encoder. üèÖ\n",
          "\n",
          "> *Relevant models:* `cross-encoder/ms-marco-MiniLM-L-6-v2` and relatives in Hugging Face."
        ]
      },

      {
        "cell_type": "markdown",
        "id": "mod4-query-expansion",
        "metadata": {},
        "source": [
          "### 4.3 Query Expansion and Reformulation\n",
          "\n",
          "Short or vague user queries? Expand or rephrase them (by LLM prompt or classic IR) to increase your chances of hitting relevant docs.\n",
          "- E.g.: \"health effects of PM2.5\" ‚Üí add synonyms and related phrases for a richer search.\n",
          "- This could be done by classic feedback, or by prompting an LLM to \"rewrite the query\" before retrieval.\n"
        ]
      },

      {
        "cell_type": "markdown",
        "id": "mod4-domain-embs",
        "metadata": {},
        "source": [
          "### 4.4 Domain-Specific Embeddings\n",
          "\n",
          "General embedding models may miss field-specific meanings. For research, consider field-tuned embedders (e.g., SciBERT, BioBERT, SPECTER).\n",
          "\n",
          "*If your work is biomedical, legal, or patent-focused, use a specialized encoder!*"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "mod4-indexing",
        "metadata": {},
        "source": [
          "### 4.5 Efficient Indexing and Filters\n",
          "- Large corpora: use metadata (year, topic, author) to filter/scope docs before search.\n",
          "- Maintain separate indexes by category/topic if relevant.\n",
          "*This reduces irrelevant retrieval‚Äîimproving both speed and accuracy!*"
        ]
      },
      {
        "cell_type": "markdown",
        "id": "mod4-evaluate",
        "metadata": {},
        "source": [
          "### 4.6 Evaluating Your Retrieval\n",
          "- Hand-inspect retrieval for key queries\n",
          "- If labeled data is available, use recall/precision, MRR, etc.\n",
          "- Often: if LLM hallucinated, see if it retrieved the right info! If not, work on retrieval first."
        ]
      },
      {
        "cell_type": "markdown",
        "id": "mod4-pytorch-link",
        "metadata": {},
        "source": [
          "> **PyTorch connection:** If you want to train an embedder retriever for your own corpus (e.g. a Dense Passage Retriever), that's a neural ranking task handled by twin-encoder architectures. Out of scope for today, but know it connects the dots with deep learning."
        ]
      },
      {
        "cell_type": "markdown",
        "id": "mod4-reflection-prompt",
        "metadata": {},
        "source": [
          "### Try it yourself!\n",
          "What is one technique you would use to improve retrieval if the initial results are bad? (Hybrid, reranking, query expansion, etc. are all fair answers!)"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "id": "mod4-answer-box",
        "metadata": {},
        "outputs": [],
        "source": [
          "from utils import create_answer_box\n",
          "create_answer_box('üìù **Your Answer:** One technique is ...', question_id='mod4_improve_retrieval')"
        ]
      }
    ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchWorkshop",
   "language": "python",
   "name": "pytorchworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
