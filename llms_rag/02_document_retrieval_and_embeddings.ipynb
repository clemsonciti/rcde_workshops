{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- Explain embeddings and similarity at a high level.\n- Chunk and embed a small corpus with a modern sentence encoder.\n- Index vectors and run top-k similarity search (FAISS or exact NN).\n- Assess retrieval quality and iterate on chunking/top_k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: Demo Corpus\n",
    "\n",
    "We will use a tiny mixed-domain corpus (AI, Climate, Biomedical, Materials) stored in `data/demo_corpus.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport pandas as pd\n\nDATA_PATH = 'data/demo_corpus.jsonl'\ndf = pd.read_json(DATA_PATH, lines=True)\ndocs = df.to_dict('records')\nprint(f'Loaded {len(docs)} docs from {DATA_PATH}')\ndisplay(df[['id','title','year','topics']].head())\n"
  },
  {
   "cell_type": "markdown",
   "id": "9d3b1d61",
   "metadata": {},
   "source": [
    "# Module 2: Document Retrieval and Embeddings\n",
    "\n",
    "*Part of the RCD Workshops series: RAG for Research Applications*\n",
    "---\n",
    "\n",
    "In this module, we'll dive into how to fetch relevant documents for RAG, covering both \"classic\" (keyword) and modern (embedding) approaches, with hands-on practice for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cc490",
   "metadata": {},
   "source": [
    "## 2.1: From Keywords to Vectors: Why Classic Search Isn’t Enough\n",
    "\n",
    "Traditional document search relies on **keyword matching** — for example, using TF-IDF or BM25 — but this method misses synonyms and rephrasings. RAG leverages **embeddings** instead: both documents and queries are mapped to dense vectors that reflect semantic *meaning*, enabling discovery even if no words overlap.\n",
    "\n",
    "By \"dense vectors,\" we mean that each document or query is represented as a point in a high-dimensional space, where similar meanings are closer together. This allows us to find relevant documents based on their semantic content rather than just exact word matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d43c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Impacts of climate change on global economies are substantial.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE: Classic Keyword Search\n",
    "corpus = [\n",
    "    'Impacts of climate change on global economies are substantial.',\n",
    "    'Recent studies discuss economic loss due to global warming.',\n",
    "    'Embedding models let us search by meaning, not just words.'\n",
    "]\n",
    "query = 'climate economics'\n",
    "def keyword_search(query, docs):\n",
    "    return [d for d in docs if any(word.lower() in d.lower() for word in query.split())]\n",
    "keyword_search(query, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b50123",
   "metadata": {},
   "source": [
    "<img src=\"semantic_sim_venn.png\" alt=\"Semantic Similarity Venn Diagram\" width=\"600\"/>\n",
    "\n",
    "Above: Only exact (or near-exact) keyword matches will be found. Synonyms/non-obvious rephrasings are missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc1585",
   "metadata": {},
   "source": [
    "## 2.2: What Are Embeddings?\n",
    "\n",
    "Embeddings are vector representations of text such that meaningfully similar texts have vectors close together in space.\n",
    "\n",
    "Let's see a toy example using word relatives (analogy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2bea9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If vector('king') - vector('man') + vector('woman') produces something like vector('queen'), the model is capturing analogy meaning!\n"
     ]
    }
   ],
   "source": [
    "# Embedding arithmetic example (pseudocode/description)\n",
    "print(\"If vector('king') - vector('man') + vector('woman') produces something like vector('queen'), the model is capturing analogy meaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Semantic similarity concept](semantic_sim_venn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae054ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary tool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "sentences = [\n",
    "    'Large language models can learn from research papers.',\n",
    "    'AI systems use documents to answer questions.',\n",
    "    'Bananas are yellow and tasty.'\n",
    "]\n",
    "embs = model.encode(sentences)\n",
    "# Show cosine similarities\n",
    "def cosine(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "for i, s1 in enumerate(sentences):\n",
    "    for j, s2 in enumerate(sentences):\n",
    "        if i < j:\n",
    "            print(f\"Similarity('{s1}', '{s2}') = {cosine(embs[i], embs[j]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3d190",
   "metadata": {},
   "source": [
    "You should see higher similarity between topically related text, much lower for unrelated (e.g. the banana one).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581d08f",
   "metadata": {},
   "source": [
    "### Quick Check: In your own words\n",
    "Why do we use embeddings instead of plain keyword search when building a RAG system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"**Your Answer:** I think we use embeddings instead of only keywords because...\", question_id=\"mod2_why_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ba523",
   "metadata": {},
   "source": [
    "## 2.3: Preparing Documents for Retrieval: Chunking and Embedding\n",
    "\n",
    "Documents are often too long for models to process at once. We break them into chunks (by token/paragraph) before embedding.\n",
    "\n",
    "**Why chunk?**\n",
    "- Keeps each unit the right size for LLM input\n",
    "- Lets retrieval focus on topical sections — precision\n",
    "\n",
    "Let's practice chunking and embedding a custom document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6339d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manual chunking\n",
    "doc = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) augments LLMs by allowing retrieval from external sources. \\\n",
    "Chunking splits text into manageable parts; for example, splitting by paragraph.\n",
    "\n",
    "Embeddings allow searches to find relevant sections even if different words are used. Cosine similarity quantifies text closeness.\n",
    "\n",
    "Document retrieval pipelines (using tools like FAISS) depend on these steps working well together.\n",
    "\"\"\"\n",
    "chunks = [c.strip() for c in doc.split('\\n') if c.strip()]\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Chunk {i+1}: {chunk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1611da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed your chunks\n",
    "chunk_embs = model.encode(chunks)\n",
    "print(f'Each chunk embedding shape: {chunk_embs[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a3ad4",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "Type or copy/paste a short document (2-4 sentences, each about a different subtopic).\n",
    "We'll split and embed your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"**Write a mini-document (2-4 sentences, each different topic):**\", question_id=\"mod2_mini_chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732f4e1",
   "metadata": {},
   "source": [
    "## 2.4: Indexing a Small Corpus (with FAISS)\n",
    "We'll go end-to-end: encode docs → index → retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95146ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "docs = [\n",
    "    'Estimates of GDP loss from climate change include effects of weather extremes.',\n",
    "    'RAG systems combine LLMs with document retrievers for better answers.',\n",
    "    'Bananas and apples are common fruits.'\n",
    "]\n",
    "doc_embs = model.encode(docs)\n",
    "doc_embs = np.array([v/np.linalg.norm(v) for v in doc_embs])  # Normalize for cosine\n",
    "index = faiss.IndexFlatIP(doc_embs.shape[1])\n",
    "index.add(doc_embs)\n",
    "\n",
    "query = 'How does RAG use LLMs?'\n",
    "q_emb = model.encode([query])[0]\n",
    "q_emb = q_emb/np.linalg.norm(q_emb)\n",
    "D, I = index.search(np.array([q_emb]), k=2)\n",
    "print('Top result:', docs[I[0][0]], '\\nScore:', D[0][0])\n",
    "print('Second result:', docs[I[0][1]], '\\nScore:', D[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a61a11",
   "metadata": {},
   "source": [
    "> **Diagram placeholder:** Schematic of a vector index: documents as points on a sphere, query vector arrow, nearest documents circled.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5034557",
   "metadata": {},
   "source": [
    "## Quick Knowledge Check\n",
    "What would happen if you used a *very* long chunk, or failed to normalize your vectors before similarity search?\n",
    "Write a brief hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"**Your Hypothesis:**\\n- With a long chunk...\\n- If you don't normalize vectors...\", question_id=\"mod2_longchunk_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772e48b",
   "metadata": {},
   "source": [
    "# End of Module 2\n",
    "\n",
    "You've now practiced the core steps of document retrieval for RAG: classic vs. semantic search, embedding, chunking, and vector indexing.\n",
    "\n",
    "Next: We'll assemble these building blocks into a complete RAG pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms-rag-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}