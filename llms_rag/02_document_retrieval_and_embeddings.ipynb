{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3b1d61",
   "metadata": {},
   "source": [
    "# Module 2: Document Retrieval and Embeddings\n",
    "\n",
    "*Part of the RCD Workshops series: RAG for Advanced Research Applications*\n",
    "---\n",
    "\n",
    "In this module, we'll dive into how to fetch relevant documents for RAG, covering both \"classic\" (keyword) and modern (embedding) approaches, with hands-on practice for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cc490",
   "metadata": {},
   "source": [
    "## 2.1: From Keywords to Vectors: Why Classic Search Isn‚Äôt Enough\n",
    "\n",
    "Traditional document search relies on **keyword matching** ‚Äî for example, using TF-IDF or BM25 ‚Äî but this method misses synonyms and rephrasings. RAG leverages **embeddings** instead: both documents and queries are mapped to dense vectors that reflect semantic *meaning*, enabling discovery even if no words overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d43c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Classic Keyword Search\n",
    "corpus = [\n",
    "    'Impacts of climate change on global economies are substantial.',\n",
    "    'Recent studies discuss economic loss due to global warming.',\n",
    "    'Embedding models let us search by meaning, not just words.'\n",
    "]\n",
    "query = 'climate economics'\n",
    "def keyword_search(query, docs):\n",
    "    return [d for d in docs if any(word.lower() in d.lower() for word in query.split())]\n",
    "keyword_search(query, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b50123",
   "metadata": {},
   "source": [
    "> **Diagram placeholder:** Show a Venn diagram with \"Keyword Search\" and \"Semantic Search\", highlighting the overlap (recalled by keywords) versus the full \"meaning space\" captured by embeddings.\n",
    "\n",
    "Above: Only exact (or near-exact) keyword matches will be found. Synonyms/non-obvious rephrasings are missed.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc1585",
   "metadata": {},
   "source": [
    "## 2.2: What Are Embeddings?\n",
    "\n",
    "Embeddings are vector representations of text such that meaningfully similar texts have vectors close together in space.\n",
    "\n",
    "Let's see a toy example using word relatives (analogy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bea9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding arithmetic example (pseudocode/description)\n",
    "print(\"If vector('king') - vector('man') + vector('woman') produces something like vector('queen'), the model is capturing analogy meaning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f385ccc",
   "metadata": {},
   "source": [
    "> **Diagram placeholder:** 3D space showing vectors for king, man, woman, queen with arrows illustrating the analogy.\n",
    "\n",
    "To make this hands-on, let's compute *real sentence embeddings* for a few sentences and examine their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae054ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary tool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "sentences = [\n",
    "    'Large language models can learn from research papers.',\n",
    "    'AI systems use documents to answer questions.',\n",
    "    'Bananas are yellow and tasty.'\n",
    "]\n",
    "embs = model.encode(sentences)\n",
    "# Show cosine similarities\n",
    "def cosine(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "for i, s1 in enumerate(sentences):\n",
    "    for j, s2 in enumerate(sentences):\n",
    "        if i < j:\n",
    "            print(f\"Similarity('{s1}', '{s2}') = {cosine(embs[i], embs[j]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3d190",
   "metadata": {},
   "source": [
    "You should see higher similarity between topically related text, much lower for unrelated (e.g. the banana one).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581d08f",
   "metadata": {},
   "source": [
    "### Quick Check: In your own words\n",
    "Why do we use embeddings instead of plain keyword search when building a RAG system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"üìù **Your Answer:** I think we use embeddings instead of only keywords because...\", question_id=\"mod2_why_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ba523",
   "metadata": {},
   "source": [
    "## 2.3: Preparing Documents for Retrieval: Chunking and Embedding\n",
    "\n",
    "Documents are often too long for models to process at once. We break them into chunks (by token/paragraph) before embedding.\n",
    "\n",
    "**Why chunk?**\n",
    "- Keeps each unit the right size for LLM input\n",
    "- Lets retrieval focus on topical sections ‚Äî precision\n",
    "\n",
    "Let's practice chunking and embedding a custom document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6339d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manual chunking\n",
    "doc = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) augments LLMs by allowing retrieval from external sources. \\\n",
    "Chunking splits text into manageable parts; for example, splitting by paragraph.\n",
    "\n",
    "Embeddings allow searches to find relevant sections even if different words are used. Cosine similarity quantifies text closeness.\n",
    "\n",
    "Document retrieval pipelines (using tools like FAISS) depend on these steps working well together.\n",
    "\"\"\"\n",
    "chunks = [c.strip() for c in doc.split('\\n') if c.strip()]\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Chunk {i+1}: {chunk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1611da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed your chunks\n",
    "chunk_embs = model.encode(chunks)\n",
    "print(f'Each chunk embedding shape: {chunk_embs[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a3ad4",
   "metadata": {},
   "source": [
    "### Try it yourself!\n",
    "Type a short document (2-4 sentences, each about a different subtopic).\n",
    "We'll split and embed your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"‚úçÔ∏è **Write a mini-document (2-4 sentences, each different topic):**\", question_id=\"mod2_mini_chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732f4e1",
   "metadata": {},
   "source": [
    "## 2.4: Indexing a Small Corpus (with FAISS)\n",
    "We'll go end-to-end: encode docs ‚Üí index ‚Üí retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95146ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "docs = [\n",
    "    'Estimates of GDP loss from climate change include effects of weather extremes.',\n",
    "    'RAG systems combine LLMs with document retrievers for better answers.',\n",
    "    'Bananas and apples are common fruits.'\n",
    "]\n",
    "doc_embs = model.encode(docs)\n",
    "doc_embs = np.array([v/np.linalg.norm(v) for v in doc_embs])  # Normalize for cosine\n",
    "index = faiss.IndexFlatIP(doc_embs.shape[1])\n",
    "index.add(doc_embs)\n",
    "\n",
    "query = 'How does RAG use LLMs?'\n",
    "q_emb = model.encode([query])[0]\n",
    "q_emb = q_emb/np.linalg.norm(q_emb)\n",
    "D, I = index.search(np.array([q_emb]), k=2)\n",
    "print('Top result:', docs[I[0][0]], '\\nScore:', D[0][0])\n",
    "print('Second result:', docs[I[0][1]], '\\nScore:', D[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a61a11",
   "metadata": {},
   "source": [
    "> **Diagram placeholder:** Schematic of a vector index: documents as points on a sphere, query vector arrow, nearest documents circled.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5034557",
   "metadata": {},
   "source": [
    "## Quick Knowledge Check\n",
    "What would happen if you used a *very* long chunk, or failed to normalize your vectors before similarity search?\n",
    "Write a brief hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"‚úçÔ∏è **Your Hypothesis:**\\n- With a long chunk...\\n- If you don't normalize vectors...\", question_id=\"mod2_longchunk_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772e48b",
   "metadata": {},
   "source": [
    "# End of Module 2\n",
    "\n",
    "You've now practiced the core steps of document retrieval for RAG: classic vs. semantic search, embedding, chunking, and vector indexing.\n",
    "\n",
    "Next: We'll assemble these building blocks into a complete RAG pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytorchWorkshop",
   "language": "python",
   "name": "pytorchworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
