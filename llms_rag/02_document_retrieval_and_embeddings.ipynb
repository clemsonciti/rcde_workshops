{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e186c37",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- Explain embeddings and similarity at a high level.\n",
    "- Chunk and embed a small corpus with a modern sentence encoder.\n",
    "- Index vectors and run top-k similarity search (FAISS or exact NN).\n",
    "- Assess retrieval quality and iterate on chunking/top_k.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b1d61",
   "metadata": {},
   "source": [
    "# Module 2: Document Retrieval and Embeddings\n",
    "\n",
    "*Part of the RCD Workshops series: RAG for Research Applications*\n",
    "---\n",
    "\n",
    "In this module, we'll dive into how to fetch relevant documents for RAG, covering both \"classic\" (keyword) and modern (embedding) approaches, with hands-on practice for each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89cc490",
   "metadata": {},
   "source": [
    "## 2.1: From Keywords to Vectors: Why Classic Search Isn’t Enough\n",
    "\n",
    "Traditional document search relies on **keyword matching** — for example, using TF-IDF or BM25 — but this method misses synonyms and rephrasings. RAG leverages **embeddings** instead: both documents and queries are mapped to dense vectors that reflect semantic *meaning*, enabling discovery even if no words overlap.\n",
    "\n",
    "By \"dense vectors,\" we mean that each document or query is represented as a point in a high-dimensional space, where similar meanings are closer together. This allows us to find relevant documents based on their semantic content rather than just exact word matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d43c06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Impacts of climate change on global economies are substantial.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE: Classic Keyword Search\n",
    "corpus = [\n",
    "    'Impacts of climate change on global economies are substantial.',\n",
    "    'Recent studies discuss economic loss due to global warming.',\n",
    "    'Embedding models let us search by meaning, not just words.'\n",
    "]\n",
    "query = 'climate economics'\n",
    "def keyword_search(query, docs):\n",
    "    return [d for d in docs if any(word.lower() in d.lower() for word in query.split())]\n",
    "keyword_search(query, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b50123",
   "metadata": {},
   "source": [
    "<img src=\"semantic_sim_venn.png\" alt=\"Semantic Similarity Venn Diagram\" width=\"600\"/>\n",
    "\n",
    "Above: Only exact (or near-exact) keyword matches will be found. Synonyms/non-obvious rephrasings are missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cc1585",
   "metadata": {},
   "source": [
    "## 2.2: What Are Embeddings?\n",
    "\n",
    "Embeddings are vector representations of text such that meaningfully similar texts have vectors close together in space.\n",
    "\n",
    "Let's see a toy example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebe0c0",
   "metadata": {},
   "source": [
    "![Semantic similarity concept](semantic_sim_venn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae054ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0: Large language models can learn from research papers.\n",
      "Embedding: [-0.02631376 -0.05902476  0.03586608  0.03195616  0.02241782  0.07368159\n",
      " -0.06222048  0.03942321  0.06609748 -0.00331412 -0.0080848   0.04686617\n",
      "  0.01815479  0.04575907  0.05886647]...\n",
      "\n",
      "Sentence 1: AI systems use documents to answer questions.\n",
      "Embedding: [-0.03400004  0.03123258 -0.03424968  0.0382176   0.05766154  0.00407708\n",
      "  0.02707845  0.08380622  0.07094587  0.014809   -0.07037963  0.01803808\n",
      "  0.04322198 -0.00348967 -0.00045261]...\n",
      "\n",
      "Sentence 2: Bananas are yellow and tasty.\n",
      "Embedding: [-0.01373625 -0.02557316  0.03501061  0.02210407  0.01295308  0.06529091\n",
      "  0.06829672 -0.05202602 -0.00095944  0.08391878  0.00758327 -0.1111851\n",
      " -0.00839011 -0.05700076  0.06420341]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary tool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "sentences = [\n",
    "    'Large language models can learn from research papers.',\n",
    "    'AI systems use documents to answer questions.',\n",
    "    'Bananas are yellow and tasty.'\n",
    "]\n",
    "embs = model.encode(sentences)\n",
    "\n",
    "# Now we have embeddings for each sentence. Let's take a look at the first chunk of each embedding.\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"Sentence {i}: {s}\")\n",
    "    print(f\"Embedding: {embs[i][:15]}...\\n\")  # Display first 15 elements of each embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c81d325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity('Large language models can learn from research papers.', 'AI systems use documents to answer questions.') = 0.39\n",
      "Similarity('Large language models can learn from research papers.', 'Bananas are yellow and tasty.') = 0.00\n",
      "Similarity('AI systems use documents to answer questions.', 'Bananas are yellow and tasty.') = 0.02\n"
     ]
    }
   ],
   "source": [
    "# Now let's look at the cosine similarities among our documents.\n",
    "def cosine(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "for i, s1 in enumerate(sentences):\n",
    "    for j, s2 in enumerate(sentences):\n",
    "        if i < j:\n",
    "            print(f\"Similarity('{s1}', '{s2}') = {cosine(embs[i], embs[j]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3d190",
   "metadata": {},
   "source": [
    "You should see higher similarity between topically related text, much lower for unrelated (e.g. the banana one).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581d08f",
   "metadata": {},
   "source": [
    "### Quick Check: In your own words\n",
    "Why do we use embeddings instead of plain keyword search when building a RAG system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e6827a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Your Answer:** We use embeddings instead of only keywords because..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295b03dbc15442babcb027ced0feb001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='500px'), placeholder='Type your answer here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5062bf0bf13d46c6b094a8bd51e623d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b892f7b4a644a38dbfd7919b51a517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"**Your Answer:** We use embeddings instead of only keywords because...\", question_id=\"mod2_why_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ba523",
   "metadata": {},
   "source": [
    "## 2.3: Preparing Documents for Retrieval: Chunking and Embedding\n",
    "\n",
    "Documents are often too long for models to process at once. We break them into chunks (by token/paragraph) before embedding.\n",
    "\n",
    "**Why chunk?**\n",
    "- Keeps each unit the right size for LLM input\n",
    "- Lets retrieval focus on topical sections — precision\n",
    "\n",
    "Let's practice chunking and embedding a custom document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6339d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Retrieval-Augmented Generation (RAG) augments LLMs by allowing retrieval from external sources. Chunking splits text into manageable parts; for example, splitting by paragraph.\n",
      "Chunk 2: Embeddings allow searches to find relevant sections even if different words are used. Cosine similarity quantifies text closeness.\n",
      "Chunk 3: Document retrieval pipelines (using tools like FAISS) depend on these steps working well together.\n"
     ]
    }
   ],
   "source": [
    "# Example: Manual chunking\n",
    "doc = \"\"\"\n",
    "Retrieval-Augmented Generation (RAG) augments LLMs by allowing retrieval from external sources. \\\n",
    "Chunking splits text into manageable parts; for example, splitting by paragraph.\n",
    "\n",
    "Embeddings allow searches to find relevant sections even if different words are used. Cosine similarity quantifies text closeness.\n",
    "\n",
    "Document retrieval pipelines (using tools like FAISS) depend on these steps working well together.\n",
    "\"\"\"\n",
    "chunks = [c.strip() for c in doc.split('\\n') if c.strip()]\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'Chunk {i+1}: {chunk}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1611da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Retrieval-Augmented Generation (RAG) augments LLMs by allowing retrieval from external sources. Chunking splits text into manageable parts; for example, splitting by paragraph.\n",
      "Embedding: [-0.07808192  0.01271216  0.0162458   0.01324641 -0.02419303  0.0192296\n",
      " -0.02702066 -0.03190193  0.0106208  -0.02104903  0.0347135   0.04540067\n",
      "  0.0567274  -0.082671    0.02537981]...\n",
      "\n",
      "Chunk 2: Embeddings allow searches to find relevant sections even if different words are used. Cosine similarity quantifies text closeness.\n",
      "Embedding: [-0.00037822 -0.03876137 -0.02181436 -0.03538439  0.05156246  0.06276481\n",
      " -0.03221408  0.013954    0.07484718 -0.07835811  0.05879162  0.05892429\n",
      "  0.06992489  0.05355405 -0.01230072]...\n",
      "\n",
      "Chunk 3: Document retrieval pipelines (using tools like FAISS) depend on these steps working well together.\n",
      "Embedding: [-0.0423752  -0.00392205 -0.0310769  -0.00017406  0.03266594 -0.02668107\n",
      " -0.09702621  0.02626733 -0.01050339 -0.03040713  0.01340665  0.0550771\n",
      "  0.04512915  0.02616271 -0.00836009]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Embed your chunks\n",
    "chunk_embs = model.encode(chunks)\n",
    "\n",
    "# Print the first 15 elements of each chunk embedding\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n",
    "    print(f\"Embedding: {chunk_embs[i][:15]}...\\n\")  # Display first 15 elements of each embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ec63d",
   "metadata": {},
   "source": [
    "### Dataset: Demo Corpus\n",
    "\n",
    "We will use a tiny mixed-domain corpus (AI, Climate, Biomedical, Materials paper abstracts) stored in `data/demo_corpus.jsonl`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8703bc71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 18 docs from data/demo_corpus.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2508.05366</td>\n",
       "      <td>Can Language Models Critique Themselves? Inves...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[NLP, Retrieval, Language Model, Biomedical]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2508.07326</td>\n",
       "      <td>Nonparametric Reaction Coordinate Optimization...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[ML, Climate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2508.07654</td>\n",
       "      <td>MLego: Interactive and Scalable Topic Explorat...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[Databases, IR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2508.07798</td>\n",
       "      <td>Generative Inversion for Property-Targeted Mat...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[Materials, ML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2508.08140</td>\n",
       "      <td>Data-Efficient Biomedical In-Context Learning:...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[NLP, Retrieval, Language Model, Biomedical]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  year  \\\n",
       "0  2508.05366  Can Language Models Critique Themselves? Inves...  2025   \n",
       "1  2508.07326  Nonparametric Reaction Coordinate Optimization...  2025   \n",
       "2  2508.07654  MLego: Interactive and Scalable Topic Explorat...  2025   \n",
       "3  2508.07798  Generative Inversion for Property-Targeted Mat...  2025   \n",
       "4  2508.08140  Data-Efficient Biomedical In-Context Learning:...  2025   \n",
       "\n",
       "                                         topics  \n",
       "0  [NLP, Retrieval, Language Model, Biomedical]  \n",
       "1                                 [ML, Climate]  \n",
       "2                               [Databases, IR]  \n",
       "3                               [Materials, ML]  \n",
       "4  [NLP, Retrieval, Language Model, Biomedical]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'data/demo_corpus.jsonl'\n",
    "df = pd.read_json(DATA_PATH, lines=True)\n",
    "docs = df.to_dict('records')\n",
    "print(f'Loaded {len(docs)} docs from {DATA_PATH}')\n",
    "display(df[['id','title','year','topics']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732f4e1",
   "metadata": {},
   "source": [
    "## 2.4: Indexing Scientific Abstracts (with FAISS)\n",
    "We'll go end-to-end using the demo corpus of scientific abstracts: chunk abstracts → encode chunks → index → retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95146ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 score=0.429 | id=2508.13107 | All for law and law for all: Adaptive RAG Pipeline for Legal...\n",
      "   contributions demonstrate the potential of task-aware, component-level tuning to deliver legally grounded, reproducible, and cost-effective RAG systems for lega...\n",
      "\n",
      "#2 score=0.349 | id=2508.12863 | Word Meanings in Transformer Language Models...\n",
      "   serves to rule out certain \"meaning eliminativist\" hypotheses about how transformer LLMs process semantic information....\n",
      "\n",
      "#3 score=0.340 | id=2508.05366 | Can Language Models Critique Themselves? Investigating Self-...\n",
      "   Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim to enable autonomous search processes where Large Language Models (LLMs) iterativel...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build a tiny passage index from scientific abstracts with simple chunking\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def chunk_text(text, max_chars=400):\n",
    "    text = (text or '').strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    return [text[i:i+max_chars].strip() for i in range(0, len(text), max_chars)]\n",
    "\n",
    "# Prepare chunk records from the loaded demo corpus (expects df/docs from above)\n",
    "chunk_texts = []\n",
    "chunk_meta = []\n",
    "for d in docs:\n",
    "    abs_text = d.get('abstract', '')\n",
    "    pieces = chunk_text(abs_text, max_chars=400)\n",
    "    for j, t in enumerate(pieces):\n",
    "        if not t:\n",
    "            continue\n",
    "        chunk_texts.append(t)\n",
    "        chunk_meta.append({'doc_id': d.get('id'), 'title': d.get('title'), 'chunk_id': j})\n",
    "\n",
    "# Encode and normalize for cosine similarity via inner product\n",
    "embs = model.encode(chunk_texts)\n",
    "embs = np.array([v/np.linalg.norm(v) for v in embs], dtype='float32')\n",
    "index = faiss.IndexFlatIP(embs.shape[1])\n",
    "index.add(embs)\n",
    "\n",
    "# Simple demo query over abstracts\n",
    "query = 'How do RAG systems combine LLMs with retrieval?'\n",
    "q = model.encode([query])[0]\n",
    "q = (q/np.linalg.norm(q)).astype('float32')\n",
    "D, I = index.search(np.array([q]), k=3)\n",
    "for rank, (idx, score) in enumerate(zip(I[0], D[0]), start=1):\n",
    "    m = chunk_meta[idx]\n",
    "    snippet = chunk_texts[idx][:160].replace('\\n',' ')\n",
    "    print(f'#{rank} score={score:.3f} | id={m[\"doc_id\"]} | {m[\"title\"][:60]}...')\n",
    "    print(f'   {snippet}...\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a61a11",
   "metadata": {},
   "source": [
    "> **Diagram placeholder:** Schematic of a vector index: documents as points on a sphere, query vector arrow, nearest documents circled.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5034557",
   "metadata": {},
   "source": [
    "## Quick Knowledge Check\n",
    "What would happen if you used a *very* long chunk, or failed to normalize your vectors before similarity search?\n",
    "Write a brief hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a5e546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Your Hypothesis:**\n",
       "- With a long chunk...\n",
       "- If you don't normalize vectors..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2037794712e74c168ae0099ed1db4373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='500px'), placeholder='Type your answer here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb67f971271742719087f6e99ad26d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfb025e7ce54e51903a51d0018d4bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import create_answer_box\n",
    "create_answer_box(\"**Your Hypothesis:**\\n- With a long chunk...\\n- If you don't normalize vectors...\", question_id=\"mod2_longchunk_norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772e48b",
   "metadata": {},
   "source": [
    "# End of Module 2\n",
    "\n",
    "You've now practiced the core steps of document retrieval for RAG: classic vs. semantic search, embedding, chunking, and vector indexing.\n",
    "\n",
    "Next: We'll assemble these building blocks into a complete RAG pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
