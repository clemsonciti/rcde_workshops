{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92feeaf1",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- Define Retrieval-Augmented Generation (RAG) and why it matters for research.\n",
    "- Contrast RAG with fine-tuning and plain prompting; identify trade-offs.\n",
    "- Recognize common research use cases and pitfalls (stale sources, grounding).\n",
    "- Name the core components of a RAG system (retrieval, ranking, prompting, generation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef07b9d",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "*Part of the RCD Workshops series: RAG for Research Applications*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a06d1",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 What is RAG and Why Do We Need It?\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) combines a pretrained large language model (LLM) with an external data retrieval system so that answers are **grounded in up-to-date, relevant external knowledge**. Instead of relying solely on what the LLM knows, RAG-enabled systems can \"consult\" databases or document corpora to generate better, more trustworthy responses.\n",
    "\n",
    "### Motivations for RAG in Research\n",
    "- **Reduce hallucinations**: LLMs sometimes make up answers. RAG anchors model outputs using real documents.\n",
    "- **Extend knowledge**: LLMs have a training cutoff; RAG lets you search new/specialized info on demand.\n",
    "- **Enable citations & trust**: In research and academic settings, RAG allows citation of sources and provenance.\n",
    "- **Lower costs**: Augmenting a frozen LLM with retrieval is much cheaper than fine-tuning or retraining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ecd2d8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> ![Diagram Placeholder: Schematic of a RAG pipeline. Show a query box, retrieval module fetching relevant docs from a database, passing to LLM with retrieved context, and final grounded answer.](rag_pipeline_graphviz.png)\n",
    "\n",
    "> If the image is missing, use this simple schematic as a guide:\n",
    "\n",
    "```\n",
    "[User Query] -> [Retriever] -> [Top-k Docs] + [Prompt Template] -> [LLM Generator] -> [Answer + Citations]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002609d7",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Examples & Use Cases\n",
    "- **Open-domain QA**: e.g., \"What were the results of the latest climate impact study?\" Without RAG, LLM can't recall something recent; with RAG, it summarizes/quotes the real paper.\n",
    "- **Academic/enterprise chatbots**: e.g., University chatbot using RAG to cite research articles, theses, policies.\n",
    "- **LLM as a \"clerk\"**: LLM interprets, but retrieval provides the vital supporting facts/documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbe6ea",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3 RAG vs. Other Approaches\n",
    "- **Versus Fine-tuning:** Fine-tuning builds new knowledge into the model weights (very costly/slow). RAG keeps the model fixed and only augments its inputs with up-to-date evidence.\n",
    "- **Versus traditional search:** Search finds docs, but RAG finds *and reads/summarizes* them for you, so you get the answer directly.\n",
    "- **Versus plain prompting:** Without retrieval, the LLM answers from memory; with RAG, it cites and grounds responses in retrieved evidence.\n",
    "\n",
    "---\n",
    "\n",
    "### Reflection Point\n",
    "**Consider your own research context:** How might RAG address a specific challenge you face when working with literature or data in your field? What would be the primary benefit for your work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1_components",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4 Core Components of a RAG System\n",
    "- **Retrieval:** Ingest and index documents (often via chunking + embeddings) and fetch top-k candidates for a query.\n",
    "- **Ranking:** Re-rank retrieved passages (e.g., BM25 + dense, or cross-encoder rerankers) to improve relevance.\n",
    "- **Prompting:** Build an instruction + query + selected context prompt that guides the LLM to ground answers and cite sources.\n",
    "- **Generation:** Use the LLM to produce an answer constrained by the provided evidence.\n",
    "\n",
    "> Other lifecycle pieces include ingestion (chunking/overlap), indexing refresh, evaluation, and observability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1_components_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    \"Why is \\\"prompting\\\" one of the core components of a RAG system that we need to build? After all, isn't it the user who will provide the prompt?\",\n",
    "    question_id='mod1_components'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1_pitfalls",
   "metadata": {},
   "source": [
    "\n",
    "## 1.5 Common Pitfalls and Mitigations\n",
    "- **Stale sources:** Indexes drift out of date; schedule refreshes and show source timestamps in answers.\n",
    "- **Irrelevant retrieval:** Poor embeddings or queries lead to noise; tune chunking, add query expansion, and apply reranking.\n",
    "- **Over/under-chunking:** Too large misses specifics; too small loses context; use overlap and validate chunk sizes.\n",
    "- **Context overflow:** Excess context gets truncated; limit k, compress or summarize context, and instruct concise citation.\n",
    "- **Prompt injection/untrusted content:** Retrieved text may contain instructions; sanitize inputs and instruct the model to ignore instructions from context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1_pitfalls_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    \"You are certain that your RAG source documents are good, but you're still getting terrible results. What could be the problem?\",\n",
    "    question_id='mod1_pitfalls'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80829bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**How might RAG be relevant to your research?**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697858d209f843ed8705eba98fae98dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='500px'), placeholder='Type your answer here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321d0a9dfeee4d229bc2ca096b6c1c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826727749e9c42eca5539e8538de6cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import create_answer_box\n",
    "\n",
    "create_answer_box(\n",
    "    '**How might RAG be relevant to your research?**',\n",
    "    question_id='mod1_advantage_rag'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e152c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Wrap-Up\n",
    "\n",
    "You now understand what RAG is, why it's important for scientific/academic research, and how it compares with other ways of expanding a language model's knowledge.\n",
    "\n",
    "**Next:** In the following module, you'll get hands-on with document retrieval and see how modern vector-based retrieval (using embeddings) powers RAG systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cde33f-6599-4faf-bf6e-c290306e0ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
