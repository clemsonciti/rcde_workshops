{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1abb179",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Attention is all you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a3aea3-c7a4-448c-a337-0a82e81c503a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c34ac7-4bf5-45b5-bdd6-786b9ac8c19a",
   "metadata": {},
   "source": [
    "Make sure you have the [dataset.py](https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/dataset.py) and [utils.py](https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/utils.py) in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23134e4c-9dad-48ab-872f-ffe71e77975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PubMedDataset\n",
    "from utils import train, test, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b1006-69a3-43f1-96a9-0c52e4a77a40",
   "metadata": {},
   "source": [
    "## A little bit of history\n",
    "\n",
    "In the beginning was the Markov model:\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/arlm_markov.png?raw=true\" alt=\"autoregressive markov chain\" width=\"800\"/>\n",
    "\n",
    "Then came recurrent neural networks (GRU, LSTM, ...):\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/rnn_lstm.png?raw=true\" alt=\"rnn and lstm\" width=\"800\"/>\n",
    "\n",
    "Then came LSTMs with something called attention:\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/lstm_attention.png?raw=true\" alt=\"rnn and lstm\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7fa7d-39f6-42bf-891a-5370cf0fb798",
   "metadata": {},
   "source": [
    "If you are feeling a little overwhelmed by this picture, you are not alone. In fact, exactly that feeling produced the title for the 2017 paper [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762):\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/attn_all_you_need.png?raw=true\" alt=\"rnn and lstm\" width=\"800\"/>\n",
    "\n",
    "And this is what their model architecture looks like: \n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/encoder_decoder_transformer.png?raw=true\" alt=\"transformer encoder decoder\" height=\"600\"/>\n",
    "\n",
    "However, they were focused on text translation which benefits from having a separate encoder/decoder. For language modeling, we only need the first half. The picture simplifies to:  \n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/transformer_decoder.png?raw=true\" alt=\"transformer decoder only for language modeling\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c80881f",
   "metadata": {},
   "source": [
    "Now this is starting to be a little less intimidating. \n",
    "\n",
    "Let's take stock of what we need to figure out: \n",
    "1. Input embedding\n",
    "2. Position encoding\n",
    "3. The circle plus thing\n",
    "3. Masked Multi-Head atttention (MMHA)\n",
    "4. Connections going around the MMHA\n",
    "5. Add and Norm\n",
    "6. Fully Connected\n",
    "8. Linear and Softmax\n",
    "\n",
    "The heart of the transformer is the \"Masked Multi-Head attention\" step. All of the other oparations act at the single-token level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993bcbf7-795e-44fe-bc39-2405698595e1",
   "metadata": {},
   "source": [
    "## Masked Multi-Head Attention\n",
    "\n",
    "### What is attention?\n",
    "\n",
    "Attention selects information from a set of entries based on a query. To perform this operation we need to define:\n",
    "* $Q$: the query, represented by a numeric vector. The query specifies what kind of information should be given more attention.\n",
    "* $K$: the keys, also vectors. Each entry in the set has a key. We compare the query with each key to see how much attention the entry should be given.\n",
    "* $V$: the value, also usually a vector. This represents the information associated with each entry that we are retrieving. \n",
    "* $f(Q, K_i) = \\alpha_i$: the \"comparison\" or \"compatibility\" function. This function compares $Q$ with $K_i$, the key for entry $i$. The function returns the attention logit $\\alpha_i$. \n",
    "\n",
    "The attention scores are computed from the attention logits with the softmax operation: \n",
    "$$\n",
    "a_i = \\frac{\\exp{(\\alpha_i)}}{\\sum_{j=1}^L\\exp{(\\alpha_j)}}\n",
    "$$\n",
    "In pytorch, we will simply do `a = alpha.softmax(dim=-1)`.\n",
    "\n",
    "Let's work out a simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae1db4d-3ffe-45b0-bd93-e8e17f825369",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.tensor([1., 0.])\n",
    "\n",
    "values = torch.tensor([0., \n",
    "                       1., \n",
    "                       0.])\n",
    "\n",
    "keys = torch.tensor([\n",
    "    [0.95, 0.05],  # goes with value 0.\n",
    "    [0.1, 0.9],  # goes with value 1. \n",
    "    [0.8, 0.2]    # goes with value 0.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9169b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha values: tensor([0.9500, 0.1000, 0.8000])\n",
      "attention values: tensor([0.4370, 0.1868, 0.3762])\n",
      "Result: 0.18679720163345337\n"
     ]
    }
   ],
   "source": [
    "# for our comparison function, let's just use the dot product\n",
    "alpha = keys @ query\n",
    "print(\"alpha values:\", alpha)\n",
    "\n",
    "# now compute the normalized attention scores\n",
    "attn = alpha.softmax(dim=-1)\n",
    "print(\"attention values:\", attn)\n",
    "\n",
    "# now use the attention scores to aggregate the values\n",
    "result = values @ attn\n",
    "print(\"Result:\", result.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc170f-a44f-4daf-86fe-e24f51e47f15",
   "metadata": {},
   "source": [
    "Because the query vector was more like the vectors with value `0.`, our result ended up closer to `0.`\n",
    "\n",
    "Check to see the result when using the query `[0., 1.]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ce883-f34b-4f2c-a96e-0a7fb82f32bb",
   "metadata": {},
   "source": [
    "### Masked Attention for autoregressive language models\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/arlm.png?raw=true\" alt=\"autoregressive lm\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5834c-2174-49ce-9e11-94ce761df31a",
   "metadata": {},
   "source": [
    "Consider the figure above. In order to make a good prediction for token 4, we need to adaptively combine the information from tokens 1, 2, and 3. Let's use attention to do this. Here's how we define Q, K, and, V:\n",
    "* $Q_3 = W_Q h_3$, where $h_3$ is the embedding for token 3 and $W_Q$ is an `embed_dim x embed_dim` projection matrix. \n",
    "* $K_{i\\leq3} = W_K h_i$ where $W_K$ is an `embed_dim x embed_dim` matrix.\n",
    "* $V_{i\\leq3} = W_V h_i$ where $W_V$ is an `embed_dim x embed_dim` matrix. \n",
    "* $\\alpha_{i,3} = \\frac{Q_3\\cdot K_i}{\\sqrt{|Q_3|}}$ where $|Q_3|$ is the number of elements in $Q_3$.\n",
    "\n",
    "We then use softmax to normalize the attention logits yeilding the attention scores $a_{i,3},\\, i\\leq3$. The output of the attention block is then \n",
    "$$\n",
    "h^{(\\rm out)}_3 = \\sum_{i=1}^3 a_{i,3}V_{i} \n",
    "$$\n",
    "\n",
    "We're now ready to implement this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd9e5a2-ab5e-44c6-9dd6-2423cca14403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 462, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, max_tokens, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_tokens = max_tokens\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.scale_factor = embed_dim**-0.5\n",
    "        \n",
    "        # q,k,v\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)  # W_Q @ h_i\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)  # W_K @ h_i\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)  # W_V @ h_i\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # final projection\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim, bias=False),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # autoregressive mask\n",
    "        self.register_buffer(\n",
    "            \"ar_mask\",\n",
    "            torch.tril(torch.ones(max_tokens, max_tokens)).unsqueeze(0)\n",
    "        )  \n",
    "        # self.ar_mask.shape == (1, L, L)\n",
    "        # for each batch, we need to select the sub-matrix\n",
    "        # of size (1, L_batch, L_batch) where L_batch<=L\n",
    "        # is the sequence length for the batch.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (N, L_batch, embed_dim)\n",
    "        L_batch = x.size(1)\n",
    "        \n",
    "        # qkv\n",
    "        q = self.query(x) # (N, L_batch, embed_dim)\n",
    "        k = self.key(x) # (N, L_batch, embed_dim)\n",
    "        v = self.value(x) # (N, L_batch, embed_dim)\n",
    "        \n",
    "        # scaled dot-product attention\n",
    "        # we use einstein summation approach to avoid \n",
    "        # complicated reshape then permute operations\n",
    "        alpha = torch.einsum(\"Nie,Nje->Nij\", q, k) * self.scale_factor\n",
    "        alpha = self.attn_dropout(alpha)\n",
    "        # alpha.shape = (N, L_batch, L_batch)\n",
    "        # the 1st L_batch dim indexes the query token, \n",
    "        # the 2nd indexes the key/val token\n",
    "        \n",
    "        # autoregressive masking\n",
    "        mask = self.ar_mask[:, :L_batch, :L_batch] # (1, L_batch, L_batch)\n",
    "        alpha = alpha.masked_fill(mask==0, float(\"-inf\"))  # why does this work? \n",
    "        \n",
    "        # normalized attention scores\n",
    "        attn = alpha.softmax(-1)  # N, L_batch, L_batch\n",
    "        \n",
    "        # aggregate\n",
    "        v_agg = torch.einsum(\"Nij,Nje->Nie\", attn, v)\n",
    "        h_out = self.proj(v_agg)\n",
    "        \n",
    "        return h_out # (N, L_batch, embed_dim)\n",
    "\n",
    "h = torch.randn(3, 462, 32)  # (N, L_batch, embed_dim)\n",
    "ma = MaskedAttention(embed_dim=32, max_tokens=512, dropout_rate=0.1)\n",
    "h_out = ma(h)  # expect (3, 462, 32)\n",
    "h_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92177a51-4819-4fba-a473-8ba3bc09d659",
   "metadata": {},
   "source": [
    "### Multi-head Masked Attention\n",
    "Now we deal with the \"Multi-head\" part. The logic here is that using a single attention score to aggregate an entire token embedding may not have enough resolution. Perhaps their are two somewhat independent parts of the embedding that need to be attended to under different circumstances. Multi-head attention addresses this issue. Conceptually, we break up the embedding vector into `num_heads` smaller embedding vectors and then perform the same attention mechanism as above independently for each sub-vector. We then concatenate the resulting sub-vectors before projecting. \n",
    "\n",
    "Once we've understood the single-head case, the multi-head case is not very difficult to implement. Copy-paste the MaskedAttention class and modify it to incorporate multiple heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f93812-5d4d-4418-a1f6-006e03a58b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 462, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadMaskedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_tokens, dropout_rate):\n",
    "        super().__init__()        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_tokens = max_tokens\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisble by num_heads\"\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale_factor = self.head_dim**-0.5  # now we scale based on head size\n",
    "        \n",
    "        # q,k,v\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)  # W_Q @ h_i\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)  # W_K @ h_i\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)  # W_V @ h_i\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # final projection\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim, bias=False),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # autoregressive mask\n",
    "        # we need one extra dimension for the head\n",
    "        self.register_buffer(\n",
    "            \"ar_mask\",\n",
    "            torch.tril(torch.ones(max_tokens, max_tokens)).unsqueeze(0).unsqueeze(0)\n",
    "        )  \n",
    "        # self.ar_mask.shape == (1, 1, L, L)\n",
    "        # for each batch, we need to select the sub-matrix\n",
    "        # of size (1, 1, L_batch, L_batch) where L_batch<=L\n",
    "        # is the sequence length for the batch.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (N, L_batch, embed_dim)\n",
    "        L_batch = x.size(1)\n",
    "        \n",
    "        # qkv\n",
    "        q = self.query(x) # (N, L_batch, num_heads * head_dim)\n",
    "        k = self.key(x) # (N, L_batch, num_heads * head_dim)\n",
    "        v = self.value(x) # (N, L_batch, num_heads * head_dim)\n",
    "        \n",
    "        # reshape to isolate head embedding\n",
    "        q,k,v = [vec.view(-1, L_batch, self.num_heads, self.head_dim) for vec in (q,k,v)]\n",
    "        # vec.shape == (N, L_batch, num_heads, head_dim)\n",
    "        \n",
    "        # scaled dot-product attention\n",
    "        # we use einstein summation approach to avoid \n",
    "        # complicated reshape then permute operations\n",
    "        alpha = torch.einsum(\"Nihe,Njhe->Nhij\", q, k) * self.scale_factor\n",
    "        alpha = self.attn_dropout(alpha)\n",
    "        # alpha.shape = (N, num_heads, L_batch, L_batch)\n",
    "        # the 1st L_batch dim indexes the query token, \n",
    "        # the 2nd indexes the key/val token\n",
    "        \n",
    "        # autoregressive masking\n",
    "        mask = self.ar_mask[:, :, :L_batch, :L_batch] # (1, 1, L_batch, L_batch)\n",
    "        alpha = alpha.masked_fill(mask==0, float(\"-inf\")) \n",
    "        \n",
    "        # normalized attention scores\n",
    "        attn = alpha.softmax(-1)  # N, num_heads, L_batch, L_batch\n",
    "        \n",
    "        # aggregate\n",
    "        v_agg = torch.einsum(\"Nhij,Njhe->Nihe\", attn, v)  # (N,L_batch,num_heads,head_dim)\n",
    "        \n",
    "        # reshape to concat the heads (view won't work)\n",
    "        v_agg = v_agg.reshape(-1, L_batch, self.embed_dim) # (N, L_batch, embed_dim)\n",
    "        h_out = self.proj(v_agg)\n",
    "        \n",
    "        return h_out # (N, L_batch, embed_dim)\n",
    "\n",
    "h = torch.randn(3, 462, 32)  # (N, L_batch, embed_dim)\n",
    "ma = MultiHeadMaskedAttention(embed_dim=32, num_heads=4, max_tokens=512, dropout_rate=0.1)\n",
    "h_out = ma(h)  # expect (3, 462, 32)\n",
    "h_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0481c01-b824-47dc-8dd0-097501e8820c",
   "metadata": {},
   "source": [
    "### The Transformer\n",
    "Now that we've tackled multi-head masked attention, the rest is easy. All other operations act at the individual token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a8e206-be12-4865-87a5-b422471b6bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_tokens, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lay_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.lay_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadMaskedAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads = num_heads,\n",
    "            max_tokens=max_tokens, \n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4*embed_dim),  # the factor of 4 comes from the original GPT paper.\n",
    "            nn.GELU(),  # like relu but a smooth\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.lay_norm1(x + self.attn(x))\n",
    "        z = self.lay_norm2(z + self.feed_forward(z))\n",
    "\n",
    "        return z\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, max_tokens, num_blocks, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_tokens = max_tokens\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # embeddings\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_tokens, embed_dim)\n",
    "\n",
    "        # sequence of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, max_tokens, dropout_rate) \n",
    "            for i in range(num_blocks)])\n",
    "\n",
    "        # output linear layer\n",
    "        self.fout = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (N, L)\n",
    "        # mask.shape = (N, L)\n",
    "        L = x.shape[-1]\n",
    "        pos = torch.arange(0, L, device=x.device, dtype=torch.long)\n",
    "\n",
    "        # embeddings\n",
    "        tok_embedding = self.tok_embed(x)  # (N, L, embed_dim)\n",
    "        pos_embedding = self.pos_embed(pos)  # (L, embed_dim)\n",
    "        embedding = tok_embedding + pos_embedding  # (N, L, embed_dim)\n",
    "\n",
    "        # transformer blocks\n",
    "        h = self.blocks(embedding)\n",
    "\n",
    "        # output\n",
    "        logits = self.fout(h)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def numpar(self):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b82809-8755-4688-ad1e-a6ca7e601093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/dane2/.cache/huggingface/datasets/text/default-cadbbf8acc2e2b5a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2413a440874c82a71815da6b0a7ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "dataset = PubMedDataset(\"/project/rcde/datasets/pubmed/mesh_50k/splits/\")\n",
    "dl_train = dataset.get_dataloader('train', batch_size=3)\n",
    "batch = next(iter(dl_train))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6be8af-170f-410b-b971-4d0a388772ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embed): Embedding(28996, 64)\n",
      "  (pos_embed): Embedding(512, 64)\n",
      "  (blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (lay_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (lay_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiHeadMaskedAttention(\n",
      "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (lay_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (lay_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiHeadMaskedAttention(\n",
      "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (lay_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (lay_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiHeadMaskedAttention(\n",
      "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (proj): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (1): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (feed_forward): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fout): Linear(in_features=64, out_features=28996, bias=True)\n",
      ")\n",
      "Trainable parameters:  3922436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 463, 28996])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    vocab_size = dataset.tokenizer.vocab_size,\n",
    "    embed_dim = 64, \n",
    "    num_heads = 8,\n",
    "    max_tokens = 512, \n",
    "    num_blocks = 3, \n",
    "    dropout_rate = 0.1\n",
    ")\n",
    "print(model)\n",
    "print(\"Trainable parameters: \", model.numpar())\n",
    "model(batch['input_ids']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42dc20-1cd2-46a9-877b-0cf55c305bef",
   "metadata": {},
   "source": [
    "## Time to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d5bc80-df00-496a-a1a0-7bf78374a39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training settings\n",
    "num_epochs=20\n",
    "batch_size=64\n",
    "learning_rate=0.002  # We could get better performance by using a learning rate scheduler\n",
    "\n",
    "# model settings\n",
    "embed_dim = 128 # gpt-1 uses 768. We have a much smaller dataset.\n",
    "num_heads = 4  # gpt uses 12 size 64 heads.\n",
    "max_tokens = 512 # gpt-1 uses 512\n",
    "dropout_rate = 0.2 # gpt-1 uses 0.1\n",
    "num_blocks = 6 # gpt-1 uses 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c952d8-1155-4ab5-b255-b3fc9823507d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/dane2/.cache/huggingface/datasets/text/default-cadbbf8acc2e2b5a/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96aceb0a13e43a28392117d2f62993d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 8704068\n"
     ]
    }
   ],
   "source": [
    "# reinitialize dataset for good measure\n",
    "dataset = PubMedDataset(\"/project/rcde/datasets/pubmed/mesh_50k/splits/\", max_tokens=max_tokens)\n",
    "vocab_size = dataset.tokenizer.vocab_size\n",
    "\n",
    "# train/test dataloaders\n",
    "dl_train = dataset.get_dataloader('train', batch_size=batch_size, num_workers=20)\n",
    "dl_test = dataset.get_dataloader('test', batch_size=batch_size, num_workers=20)\n",
    "\n",
    "# reinitialize the model on gpu\n",
    "model = Transformer(\n",
    "    vocab_size = dataset.tokenizer.vocab_size,\n",
    "    embed_dim = embed_dim, \n",
    "    num_heads = num_heads,\n",
    "    max_tokens = max_tokens, \n",
    "    num_blocks = num_blocks, \n",
    "    dropout_rate = dropout_rate\n",
    ").to('cuda')\n",
    "\n",
    "print(\"Trainable parameters:\", model.numpar())\n",
    "\n",
    "# create the pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398be384-8306-4556-aa52-7b882aa75108",
   "metadata": {},
   "source": [
    "This is going to take a while, and we only have 45k training samples (63MB) and a tiny model (yes 8.7 million is tiny by today's LLM standards). GPT-3 has about 175 billion parameters and 45 TB of text data. That's 22 thousand times more model and 700 thousand times more data... Be glad we don't have to train that! Nevertheless, the basic architecture is very similar to what we wrote down above. \n",
    "\n",
    "While it trains, try looking at your gpu utilization (for example `nvidia-smi -l 3`) and cpu utilization (`top` or `htop`). Can you identify the bottleneck in the training pipeline? How would we remedy this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a49abbc-be99-4540-a915-cded42f96a47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START EPOCH 1\n",
      "TRAINING\n",
      "Batch 0 training loss: 10.452584266662598\n",
      "Batch 80 training loss: 6.374979019165039\n",
      "Batch 160 training loss: 5.691380023956299\n",
      "Batch 240 training loss: 5.352369785308838\n",
      "Batch 320 training loss: 5.182412147521973\n",
      "Batch 400 training loss: 5.0503997802734375\n",
      "Batch 480 training loss: 5.087331771850586\n",
      "Batch 560 training loss: 4.811542987823486\n",
      "Batch 640 training loss: 4.880934238433838\n",
      "TESTING\n",
      "Batch 0 testing loss: 4.585996150970459\n",
      "Batch 20 testing loss: 4.726027965545654\n",
      "Batch 40 testing loss: 4.576559066772461\n",
      "Batch 60 testing loss: 4.589410781860352\n",
      "------------------------------\n",
      "START EPOCH 2\n",
      "TRAINING\n",
      "Batch 0 training loss: 4.719789028167725\n",
      "Batch 80 training loss: 4.812595367431641\n",
      "Batch 160 training loss: 4.71852970123291\n",
      "Batch 240 training loss: 4.6425676345825195\n",
      "Batch 320 training loss: 4.589133262634277\n",
      "Batch 400 training loss: 4.54886531829834\n",
      "Batch 480 training loss: 4.641766548156738\n",
      "Batch 560 training loss: 4.420822620391846\n",
      "Batch 640 training loss: 4.512775421142578\n",
      "TESTING\n",
      "Batch 0 testing loss: 4.2399516105651855\n",
      "Batch 20 testing loss: 4.3824143409729\n",
      "Batch 40 testing loss: 4.222767353057861\n",
      "Batch 60 testing loss: 4.243021011352539\n",
      "------------------------------\n",
      "START EPOCH 3\n",
      "TRAINING\n",
      "Batch 0 training loss: 4.383571624755859\n",
      "Batch 80 training loss: 4.506415843963623\n",
      "Batch 160 training loss: 4.425286769866943\n",
      "Batch 240 training loss: 4.360123157501221\n",
      "Batch 320 training loss: 4.346638202667236\n",
      "Batch 400 training loss: 4.298503875732422\n",
      "Batch 480 training loss: 4.398643493652344\n",
      "Batch 560 training loss: 4.1932454109191895\n",
      "Batch 640 training loss: 4.302890300750732\n",
      "TESTING\n",
      "Batch 0 testing loss: 4.011663913726807\n",
      "Batch 20 testing loss: 4.155764579772949\n",
      "Batch 40 testing loss: 3.9985527992248535\n",
      "Batch 60 testing loss: 4.008823394775391\n",
      "------------------------------\n",
      "START EPOCH 4\n",
      "TRAINING\n",
      "Batch 0 training loss: 4.17204475402832\n",
      "Batch 80 training loss: 4.281955242156982\n",
      "Batch 160 training loss: 4.221310615539551\n",
      "Batch 240 training loss: 4.170859336853027\n",
      "Batch 320 training loss: 4.154685974121094\n",
      "Batch 400 training loss: 4.136913299560547\n",
      "Batch 480 training loss: 4.245109558105469\n",
      "Batch 560 training loss: 4.040589332580566\n",
      "Batch 640 training loss: 4.153753280639648\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.8661282062530518\n",
      "Batch 20 testing loss: 3.99613356590271\n",
      "Batch 40 testing loss: 3.840102434158325\n",
      "Batch 60 testing loss: 3.8525760173797607\n",
      "------------------------------\n",
      "START EPOCH 5\n",
      "TRAINING\n",
      "Batch 0 training loss: 4.013996601104736\n",
      "Batch 80 training loss: 4.165761470794678\n",
      "Batch 160 training loss: 4.087644100189209\n",
      "Batch 240 training loss: 4.045799255371094\n",
      "Batch 320 training loss: 4.020554065704346\n",
      "Batch 400 training loss: 4.015940189361572\n",
      "Batch 480 training loss: 4.129086494445801\n",
      "Batch 560 training loss: 3.9176104068756104\n",
      "Batch 640 training loss: 4.046818733215332\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.755732536315918\n",
      "Batch 20 testing loss: 3.8752472400665283\n",
      "Batch 40 testing loss: 3.735603094100952\n",
      "Batch 60 testing loss: 3.746253252029419\n",
      "------------------------------\n",
      "START EPOCH 6\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.915494680404663\n",
      "Batch 80 training loss: 4.053531646728516\n",
      "Batch 160 training loss: 3.9983108043670654\n",
      "Batch 240 training loss: 3.944861650466919\n",
      "Batch 320 training loss: 3.927967071533203\n",
      "Batch 400 training loss: 3.9284825325012207\n",
      "Batch 480 training loss: 4.031927585601807\n",
      "Batch 560 training loss: 3.8456199169158936\n",
      "Batch 640 training loss: 3.956512689590454\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.6882715225219727\n",
      "Batch 20 testing loss: 3.8003017902374268\n",
      "Batch 40 testing loss: 3.661961793899536\n",
      "Batch 60 testing loss: 3.6671717166900635\n",
      "------------------------------\n",
      "START EPOCH 7\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.834089994430542\n",
      "Batch 80 training loss: 3.977390766143799\n",
      "Batch 160 training loss: 3.925405263900757\n",
      "Batch 240 training loss: 3.8695054054260254\n",
      "Batch 320 training loss: 3.854212760925293\n",
      "Batch 400 training loss: 3.8631300926208496\n",
      "Batch 480 training loss: 3.9682087898254395\n",
      "Batch 560 training loss: 3.77004075050354\n",
      "Batch 640 training loss: 3.8823935985565186\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.621049404144287\n",
      "Batch 20 testing loss: 3.7294516563415527\n",
      "Batch 40 testing loss: 3.5902976989746094\n",
      "Batch 60 testing loss: 3.599853754043579\n",
      "------------------------------\n",
      "START EPOCH 8\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.778795003890991\n",
      "Batch 80 training loss: 3.9082093238830566\n",
      "Batch 160 training loss: 3.8752939701080322\n",
      "Batch 240 training loss: 3.798201560974121\n",
      "Batch 320 training loss: 3.793893575668335\n",
      "Batch 400 training loss: 3.8117570877075195\n",
      "Batch 480 training loss: 3.9035396575927734\n",
      "Batch 560 training loss: 3.710036039352417\n",
      "Batch 640 training loss: 3.8467564582824707\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.5694069862365723\n",
      "Batch 20 testing loss: 3.67524790763855\n",
      "Batch 40 testing loss: 3.542551040649414\n",
      "Batch 60 testing loss: 3.537353754043579\n",
      "------------------------------\n",
      "START EPOCH 9\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.711329698562622\n",
      "Batch 80 training loss: 3.833411455154419\n",
      "Batch 160 training loss: 3.81878399848938\n",
      "Batch 240 training loss: 3.751685857772827\n",
      "Batch 320 training loss: 3.7343690395355225\n",
      "Batch 400 training loss: 3.759854316711426\n",
      "Batch 480 training loss: 3.8445470333099365\n",
      "Batch 560 training loss: 3.6745879650115967\n",
      "Batch 640 training loss: 3.808680772781372\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.542301893234253\n",
      "Batch 20 testing loss: 3.6386985778808594\n",
      "Batch 40 testing loss: 3.509641408920288\n",
      "Batch 60 testing loss: 3.513662338256836\n",
      "------------------------------\n",
      "START EPOCH 10\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.6756978034973145\n",
      "Batch 80 training loss: 3.803558349609375\n",
      "Batch 160 training loss: 3.7809062004089355\n",
      "Batch 240 training loss: 3.715864658355713\n",
      "Batch 320 training loss: 3.705570697784424\n",
      "Batch 400 training loss: 3.7197763919830322\n",
      "Batch 480 training loss: 3.815206527709961\n",
      "Batch 560 training loss: 3.6370961666107178\n",
      "Batch 640 training loss: 3.752600908279419\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.5052151679992676\n",
      "Batch 20 testing loss: 3.6101551055908203\n",
      "Batch 40 testing loss: 3.470839500427246\n",
      "Batch 60 testing loss: 3.479055881500244\n",
      "------------------------------\n",
      "START EPOCH 11\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.629218339920044\n",
      "Batch 80 training loss: 3.7521986961364746\n",
      "Batch 160 training loss: 3.7358386516571045\n",
      "Batch 240 training loss: 3.6734073162078857\n",
      "Batch 320 training loss: 3.6772537231445312\n",
      "Batch 400 training loss: 3.690865993499756\n",
      "Batch 480 training loss: 3.7839596271514893\n",
      "Batch 560 training loss: 3.6087210178375244\n",
      "Batch 640 training loss: 3.7202019691467285\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.47998046875\n",
      "Batch 20 testing loss: 3.5763354301452637\n",
      "Batch 40 testing loss: 3.441972494125366\n",
      "Batch 60 testing loss: 3.451063394546509\n",
      "------------------------------\n",
      "START EPOCH 12\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.614640951156616\n",
      "Batch 80 training loss: 3.7264034748077393\n",
      "Batch 160 training loss: 3.704191207885742\n",
      "Batch 240 training loss: 3.6371068954467773\n",
      "Batch 320 training loss: 3.6355886459350586\n",
      "Batch 400 training loss: 3.656683921813965\n",
      "Batch 480 training loss: 3.7488291263580322\n",
      "Batch 560 training loss: 3.571959972381592\n",
      "Batch 640 training loss: 3.6997549533843994\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.4594802856445312\n",
      "Batch 20 testing loss: 3.5531997680664062\n",
      "Batch 40 testing loss: 3.4273688793182373\n",
      "Batch 60 testing loss: 3.4279913902282715\n",
      "------------------------------\n",
      "START EPOCH 13\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.577592372894287\n",
      "Batch 80 training loss: 3.6883506774902344\n",
      "Batch 160 training loss: 3.67146635055542\n",
      "Batch 240 training loss: 3.6163558959960938\n",
      "Batch 320 training loss: 3.610748529434204\n",
      "Batch 400 training loss: 3.6240313053131104\n",
      "Batch 480 training loss: 3.716013193130493\n",
      "Batch 560 training loss: 3.552309036254883\n",
      "Batch 640 training loss: 3.669013738632202\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.4420790672302246\n",
      "Batch 20 testing loss: 3.541376829147339\n",
      "Batch 40 testing loss: 3.4125959873199463\n",
      "Batch 60 testing loss: 3.4093074798583984\n",
      "------------------------------\n",
      "START EPOCH 14\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.560422420501709\n",
      "Batch 80 training loss: 3.6768457889556885\n",
      "Batch 160 training loss: 3.638218402862549\n",
      "Batch 240 training loss: 3.5907909870147705\n",
      "Batch 320 training loss: 3.5863687992095947\n",
      "Batch 400 training loss: 3.60388445854187\n",
      "Batch 480 training loss: 3.6895956993103027\n",
      "Batch 560 training loss: 3.5222744941711426\n",
      "Batch 640 training loss: 3.653254747390747\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.425461769104004\n",
      "Batch 20 testing loss: 3.5140018463134766\n",
      "Batch 40 testing loss: 3.38629412651062\n",
      "Batch 60 testing loss: 3.3906784057617188\n",
      "------------------------------\n",
      "START EPOCH 15\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.535149574279785\n",
      "Batch 80 training loss: 3.6382691860198975\n",
      "Batch 160 training loss: 3.617363929748535\n",
      "Batch 240 training loss: 3.5583314895629883\n",
      "Batch 320 training loss: 3.569516897201538\n",
      "Batch 400 training loss: 3.571824312210083\n",
      "Batch 480 training loss: 3.6581544876098633\n",
      "Batch 560 training loss: 3.495903491973877\n",
      "Batch 640 training loss: 3.620234489440918\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.405750036239624\n",
      "Batch 20 testing loss: 3.4966983795166016\n",
      "Batch 40 testing loss: 3.3656790256500244\n",
      "Batch 60 testing loss: 3.3756566047668457\n",
      "------------------------------\n",
      "START EPOCH 16\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.5067262649536133\n",
      "Batch 80 training loss: 3.6259729862213135\n",
      "Batch 160 training loss: 3.609950065612793\n",
      "Batch 240 training loss: 3.5440521240234375\n",
      "Batch 320 training loss: 3.5391523838043213\n",
      "Batch 400 training loss: 3.5536675453186035\n",
      "Batch 480 training loss: 3.6511363983154297\n",
      "Batch 560 training loss: 3.4821293354034424\n",
      "Batch 640 training loss: 3.5934128761291504\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.3949966430664062\n",
      "Batch 20 testing loss: 3.486539125442505\n",
      "Batch 40 testing loss: 3.360231399536133\n",
      "Batch 60 testing loss: 3.364014148712158\n",
      "------------------------------\n",
      "START EPOCH 17\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.484517812728882\n",
      "Batch 80 training loss: 3.597951650619507\n",
      "Batch 160 training loss: 3.5910627841949463\n",
      "Batch 240 training loss: 3.5222134590148926\n",
      "Batch 320 training loss: 3.526491641998291\n",
      "Batch 400 training loss: 3.532900333404541\n",
      "Batch 480 training loss: 3.612983226776123\n",
      "Batch 560 training loss: 3.4576520919799805\n",
      "Batch 640 training loss: 3.5748281478881836\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.3752410411834717\n",
      "Batch 20 testing loss: 3.4622974395751953\n",
      "Batch 40 testing loss: 3.332735300064087\n",
      "Batch 60 testing loss: 3.3503453731536865\n",
      "------------------------------\n",
      "START EPOCH 18\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.464963436126709\n",
      "Batch 80 training loss: 3.5874645709991455\n",
      "Batch 160 training loss: 3.5614945888519287\n",
      "Batch 240 training loss: 3.503298282623291\n",
      "Batch 320 training loss: 3.5087859630584717\n",
      "Batch 400 training loss: 3.5220634937286377\n",
      "Batch 480 training loss: 3.606145143508911\n",
      "Batch 560 training loss: 3.4522624015808105\n",
      "Batch 640 training loss: 3.561788320541382\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.365344285964966\n",
      "Batch 20 testing loss: 3.4563546180725098\n",
      "Batch 40 testing loss: 3.325873613357544\n",
      "Batch 60 testing loss: 3.3404791355133057\n",
      "------------------------------\n",
      "START EPOCH 19\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.4553885459899902\n",
      "Batch 80 training loss: 3.5692543983459473\n",
      "Batch 160 training loss: 3.547701120376587\n",
      "Batch 240 training loss: 3.4801671504974365\n",
      "Batch 320 training loss: 3.4948878288269043\n",
      "Batch 400 training loss: 3.500943422317505\n",
      "Batch 480 training loss: 3.5880343914031982\n",
      "Batch 560 training loss: 3.4192421436309814\n",
      "Batch 640 training loss: 3.553492307662964\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.354518175125122\n",
      "Batch 20 testing loss: 3.4489073753356934\n",
      "Batch 40 testing loss: 3.3195173740386963\n",
      "Batch 60 testing loss: 3.3344483375549316\n",
      "------------------------------\n",
      "START EPOCH 20\n",
      "TRAINING\n",
      "Batch 0 training loss: 3.443753242492676\n",
      "Batch 80 training loss: 3.557410955429077\n",
      "Batch 160 training loss: 3.5269641876220703\n",
      "Batch 240 training loss: 3.470795154571533\n",
      "Batch 320 training loss: 3.4830808639526367\n",
      "Batch 400 training loss: 3.4765262603759766\n",
      "Batch 480 training loss: 3.574180841445923\n",
      "Batch 560 training loss: 3.4021897315979004\n",
      "Batch 640 training loss: 3.5335121154785156\n",
      "TESTING\n",
      "Batch 0 testing loss: 3.346216917037964\n",
      "Batch 20 testing loss: 3.4404871463775635\n",
      "Batch 40 testing loss: 3.3077878952026367\n",
      "Batch 60 testing loss: 3.3251307010650635\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"START EPOCH {epoch+1}\")\n",
    "    print(\"TRAINING\")\n",
    "    train(model, dl_train, optimizer, reporting_interval=80)\n",
    "    print(\"TESTING\")\n",
    "    test(model, dl_test, reporting_interval=20)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90adc4c-3734-44e9-8bdc-fef139ac0504",
   "metadata": {},
   "source": [
    "## Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "194f9869-7d49-4822-b1a8-1b9e4fd2a664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] we compared the prevalence of diagnosis ( immunoglobulin, alpha, alpha -, and beta ) collaborating cases for cancer in the redhermal meningoencephalogic patients in patients with diabetes mellitus and current mellitus. among these',\n",
       " '[CLS] we compared the prevalence of microbial protein use in 22 patients with human gastric cancer ( mbm ) ; the incidence of secretory cell - like chemotactic factor ( egf ) within the number of diseases due to variation of the adherence to dairy',\n",
       " '[CLS] we compared the prevalence of acquired bovine myocarditis in swine to compare the incidence of postoperative mesenteric surgery repair. patient clinically suspected to resolve the clinical course in the first duration of the first trimester during have also yielded only at least']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"We compared the prevalence of\", # organ-specific autoantibodies in a group of Helicobacter...\"\n",
    "    \"We compared the prevalence of\",\n",
    "    \"We compared the prevalence of\",\n",
    "]\n",
    "\n",
    "prompt_ids = dataset.tokenizer(prompts, return_tensors='pt')['input_ids']\n",
    "\n",
    "# trim off unwanted [SEP] tokens which act like our special end-of-sequence token.\n",
    "prompt_ids = prompt_ids[:,:-1]\n",
    "\n",
    "# generate ids\n",
    "gen_ids = generate(model.to('cpu'), prompt_ids, 50)\n",
    "dataset.decode_batch(gen_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4a371e-234b-412b-adec-cd24dd1d958d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] the quick brown fox corner yeast, taady, witn, rycnach1 - mediated dna polymerase or photomid px. px microarrays assay ( pmg1 ) together with limited dna - coupled interfaces.',\n",
       " '[CLS] the quick brown foxsera peptide ( wt ) produced in sp and a sulfated rice ( gtp ) 22 mrs in which lancet strain and the karyotypic spanish - transformed pancreatic resistance caused by v20 mouse',\n",
       " '[CLS] the quick brown fox apomortoma approach to first dipstick short periods, to enable leaf premature renal cell membranes are commonly used due to vascularization resulting in tissue damage in prolonged periods and becomes more serious, but with generally no low energy supply, remains']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"The quick brown fox\", # jumps over the lazy dog\n",
    "    \"The quick brown fox\",\n",
    "    \"The quick brown fox\",\n",
    "]\n",
    "\n",
    "prompt_ids = dataset.tokenizer(prompts, return_tensors='pt')['input_ids']\n",
    "\n",
    "# trim off unwanted [SEP] tokens which act like our special end-of-sequence token.\n",
    "prompt_ids = prompt_ids[:,:-1]\n",
    "\n",
    "# generate ids\n",
    "gen_ids = generate(model, prompt_ids, 50)\n",
    "dataset.decode_batch(gen_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec8209-1363-490a-a67c-212e6955f196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
