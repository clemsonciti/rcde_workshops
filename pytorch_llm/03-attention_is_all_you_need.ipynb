{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1abb179",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Attention is all you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3aea3-c7a4-448c-a337-0a82e81c503a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c34ac7-4bf5-45b5-bdd6-786b9ac8c19a",
   "metadata": {},
   "source": [
    "Make sure you have the [dataset.py](https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/dataset.py) and [utils.py](https://raw.githubusercontent.com/clemsonciti/rcde_workshops/master/pytorch_llm/utils.py) in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23134e4c-9dad-48ab-872f-ffe71e77975c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset import PubMedDataset\n",
    "from utils import train, test, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b1006-69a3-43f1-96a9-0c52e4a77a40",
   "metadata": {},
   "source": [
    "## A little bit of history\n",
    "\n",
    "In the beginning was the Markov model:\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/arlm_markov.png?raw=true\" alt=\"autoregressive markov chain\" width=\"800\"/>\n",
    "\n",
    "Then came recurrent neural networks (GRU, LSTM, ...):\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/rnn_lstm.png?raw=true\" alt=\"rnn and lstm\" width=\"800\"/>\n",
    "\n",
    "Then came LSTMs with something called attention:\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/lstm_attention.png?raw=true\" alt=\"rnn and lstm\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e7fa7d-39f6-42bf-891a-5370cf0fb798",
   "metadata": {},
   "source": [
    "If you are feeling a little overwhelmed by this picture, you are not alone. In fact, exactly that feeling produced the title for the 2017 paper [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762):\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/attn_all_you_need.png?raw=true\" alt=\"rnn and lstm\" width=\"800\"/>\n",
    "\n",
    "And this is what their model architecture looks like: \n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/encoder_decoder_transformer.png?raw=true\" alt=\"transformer encoder decoder\" height=\"600\"/>\n",
    "\n",
    "However, they were focused on text translation which benefits from having a separate encoder/decoder. For language modeling, we only need the first half. The picture simplifies to:  \n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/transformer_decoder.png?raw=true\" alt=\"transformer decoder only for language modeling\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c80881f",
   "metadata": {},
   "source": [
    "Now this is starting to be a little less intimidating. \n",
    "\n",
    "Let's take stock of what we need to figure out: \n",
    "1. Input embedding\n",
    "2. Position encoding\n",
    "3. The circle plus thing\n",
    "3. Masked Multi-Head atttention (MMHA)\n",
    "4. Connections going around the MMHA\n",
    "5. Add and Norm\n",
    "6. Fully Connected\n",
    "8. Linear and Softmax\n",
    "\n",
    "The heart of the transformer is the \"Masked Multi-Head attention\" step. All of the other operations act at the single-token level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993bcbf7-795e-44fe-bc39-2405698595e1",
   "metadata": {},
   "source": [
    "## Masked Multi-Head Attention\n",
    "\n",
    "### What is attention?\n",
    "\n",
    "Attention selects information from a set of entries based on a query. To perform this operation we need to define:\n",
    "* $Q$: the query, represented by a numeric vector. The query specifies what kind of information should be given more attention.\n",
    "* $K$: the keys, also vectors. Each entry in the set has a key. We compare the query with each key to see how much attention the entry should be given.\n",
    "* $V$: the value, also usually a vector. This represents the information associated with each entry that we are retrieving. \n",
    "* $f(Q, K_i) = \\alpha_i$: the \"comparison\" or \"compatibility\" function. This function compares $Q$ with $K_i$, the key for entry $i$. The function returns the attention logit $\\alpha_i$. \n",
    "\n",
    "The attention scores are computed from the attention logits with the softmax operation: \n",
    "$$\n",
    "a_i = \\frac{\\exp{(\\alpha_i)}}{\\sum_{j=1}^L\\exp{(\\alpha_j)}}\n",
    "$$\n",
    "In pytorch, we will simply do `a = alpha.softmax(dim=-1)`.\n",
    "\n",
    "Let's work out a simple example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae1db4d-3ffe-45b0-bd93-e8e17f825369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = torch.tensor([1., 0.])\n",
    "\n",
    "values = torch.tensor([0., \n",
    "                       1., \n",
    "                       0.])\n",
    "\n",
    "keys = torch.tensor([\n",
    "    [0.95, 0.05],  # goes with value 0.\n",
    "    [0.1, 0.9],  # goes with value 1. \n",
    "    [0.8, 0.2]    # goes with value 0.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9169b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for our comparison function, let's just use the dot product\n",
    "alpha = keys @ query\n",
    "print(\"alpha values:\", alpha)\n",
    "\n",
    "# now compute the normalized attention scores\n",
    "attn = alpha.softmax(dim=-1)\n",
    "print(\"attention values:\", attn)\n",
    "\n",
    "# now use the attention scores to aggregate the values\n",
    "result = values @ attn\n",
    "print(\"Result:\", result.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc170f-a44f-4daf-86fe-e24f51e47f15",
   "metadata": {},
   "source": [
    "Because the query vector was more like the vectors with value `0.`, our result ended up closer to `0.`\n",
    "\n",
    "Check to see the result when using the query `[0., 1.]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ce883-f34b-4f2c-a96e-0a7fb82f32bb",
   "metadata": {},
   "source": [
    "### Masked Attention for autoregressive language models\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/blob/master/pytorch_llm/figs/arlm.png?raw=true\" alt=\"autoregressive lm\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5834c-2174-49ce-9e11-94ce761df31a",
   "metadata": {},
   "source": [
    "Consider the figure above. In order to make a good prediction for token 4, we need to adaptively combine the information from tokens 1, 2, and 3. Let's use attention to do this. Here's how we define Q, K, and, V:\n",
    "* $Q_3 = W_Q h_3$, where $h_3$ is the embedding for token 3 and $W_Q$ is an `embed_dim x embed_dim` projection matrix. \n",
    "* $K_{i\\leq3} = W_K h_i$ where $W_K$ is an `embed_dim x embed_dim` matrix.\n",
    "* $V_{i\\leq3} = W_V h_i$ where $W_V$ is an `embed_dim x embed_dim` matrix. \n",
    "* $\\alpha_{i,3} = \\frac{Q_3\\cdot K_i}{\\sqrt{|Q_3|}}$ where $|Q_3|$ is the number of elements in $Q_3$.\n",
    "\n",
    "We then use softmax to normalize the attention logits yeilding the attention scores $a_{i,3},\\, i\\leq3$. The output of the attention block is then \n",
    "$$\n",
    "h^{(\\rm out)}_3 = \\sum_{i=1}^3 a_{i,3}V_{i} \n",
    "$$\n",
    "\n",
    "We're now ready to implement this in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9e5a2-ab5e-44c6-9dd6-2423cca14403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, max_tokens, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_tokens = max_tokens\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.scale_factor = embed_dim**-0.5\n",
    "        \n",
    "        # q,k,v\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)  # W_Q @ h_i\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)  # W_K @ h_i\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)  # W_V @ h_i\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # final projection\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim, bias=False),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # autoregressive mask\n",
    "        self.register_buffer(\n",
    "            \"ar_mask\",\n",
    "            torch.tril(torch.ones(max_tokens, max_tokens)).unsqueeze(0)\n",
    "        )  \n",
    "        # self.ar_mask.shape == (1, L, L)\n",
    "        # for each batch, we need to select the sub-matrix\n",
    "        # of size (1, L_batch, L_batch) where L_batch<=L\n",
    "        # is the sequence length for the batch.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (N, L_batch, embed_dim)\n",
    "        L_batch = x.size(1)\n",
    "        \n",
    "        # qkv\n",
    "        q = self.query(x) # (N, L_batch, embed_dim)\n",
    "        k = self.key(x) # (N, L_batch, embed_dim)\n",
    "        v = self.value(x) # (N, L_batch, embed_dim)\n",
    "        \n",
    "        # scaled dot-product attention\n",
    "        # we use einstein summation approach to avoid \n",
    "        # complicated reshape then permute operations\n",
    "        alpha = torch.einsum(\"Nie,Nje->Nij\", q, k) * self.scale_factor\n",
    "        alpha = self.attn_dropout(alpha)\n",
    "        # alpha.shape = (N, L_batch, L_batch)\n",
    "        # the 1st L_batch dim indexes the query token, \n",
    "        # the 2nd indexes the key/val token\n",
    "        \n",
    "        # autoregressive masking\n",
    "        mask = self.ar_mask[:, :L_batch, :L_batch] # (1, L_batch, L_batch)\n",
    "        alpha = alpha.masked_fill(mask==0, float(\"-inf\"))  # why does this work? \n",
    "        \n",
    "        # normalized attention scores\n",
    "        attn = alpha.softmax(-1)  # N, L_batch, L_batch\n",
    "        \n",
    "        # aggregate\n",
    "        v_agg = torch.einsum(\"Nij,Nje->Nie\", attn, v)\n",
    "        h_out = self.proj(v_agg)\n",
    "        \n",
    "        return h_out # (N, L_batch, embed_dim)\n",
    "\n",
    "h = torch.randn(3, 462, 32)  # (N, L_batch, embed_dim)\n",
    "ma = MaskedAttention(embed_dim=32, max_tokens=512, dropout_rate=0.1)\n",
    "h_out = ma(h)  # expect (3, 462, 32)\n",
    "h_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92177a51-4819-4fba-a473-8ba3bc09d659",
   "metadata": {},
   "source": [
    "### Multi-head Masked Attention\n",
    "Now we deal with the \"Multi-head\" part. The logic here is that using a single attention score to aggregate an entire token embedding may not have enough resolution. Perhaps there are two somewhat independent parts of the embedding that need to be attended to under different circumstances. Multi-head attention addresses this issue. Conceptually, we break up the embedding vector into `num_heads` smaller embedding vectors and then perform the same attention mechanism as above independently for each sub-vector. We then concatenate the resulting sub-vectors before projecting. \n",
    "\n",
    "Once we've understood the single-head case, the multi-head case is not very difficult to implement. Copy-paste the MaskedAttention class and modify it to incorporate multiple heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f93812-5d4d-4418-a1f6-006e03a58b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadMaskedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_tokens, dropout_rate):\n",
    "        super().__init__()        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_tokens = max_tokens\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisble by num_heads\"\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale_factor = self.head_dim**-0.5  # now we scale based on head size\n",
    "        \n",
    "        # q,k,v\n",
    "        self.query = nn.Linear(embed_dim, embed_dim, bias=False)  # W_Q @ h_i\n",
    "        self.key = nn.Linear(embed_dim, embed_dim, bias=False)  # W_K @ h_i\n",
    "        self.value = nn.Linear(embed_dim, embed_dim, bias=False)  # W_V @ h_i\n",
    "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # final projection\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim, bias=False),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # autoregressive mask\n",
    "        # we need one extra dimension for the head\n",
    "        self.register_buffer(\n",
    "            \"ar_mask\",\n",
    "            torch.tril(torch.ones(max_tokens, max_tokens)).unsqueeze(0).unsqueeze(0)\n",
    "        )  \n",
    "        # self.ar_mask.shape == (1, 1, L, L)\n",
    "        # for each batch, we need to select the sub-matrix\n",
    "        # of size (1, 1, L_batch, L_batch) where L_batch<=L\n",
    "        # is the sequence length for the batch.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (N, L_batch, embed_dim)\n",
    "        L_batch = x.size(1)\n",
    "        \n",
    "        # qkv\n",
    "        q = self.query(x) # (N, L_batch, num_heads * head_dim)\n",
    "        k = self.key(x) # (N, L_batch, num_heads * head_dim)\n",
    "        v = self.value(x) # (N, L_batch, num_heads * head_dim)\n",
    "        \n",
    "        # reshape to isolate head embedding\n",
    "        q,k,v = [vec.view(-1, L_batch, self.num_heads, self.head_dim) for vec in (q,k,v)]\n",
    "        # vec.shape == (N, L_batch, num_heads, head_dim)\n",
    "        \n",
    "        # scaled dot-product attention\n",
    "        # we use einstein summation approach to avoid \n",
    "        # complicated reshape then permute operations\n",
    "        alpha = torch.einsum(\"Nihe,Njhe->Nhij\", q, k) * self.scale_factor\n",
    "        alpha = self.attn_dropout(alpha)\n",
    "        # alpha.shape = (N, num_heads, L_batch, L_batch)\n",
    "        # the 1st L_batch dim indexes the query token, \n",
    "        # the 2nd indexes the key/val token\n",
    "        \n",
    "        # autoregressive masking\n",
    "        mask = self.ar_mask[:, :, :L_batch, :L_batch] # (1, 1, L_batch, L_batch)\n",
    "        alpha = alpha.masked_fill(mask==0, float(\"-inf\")) \n",
    "        \n",
    "        # normalized attention scores\n",
    "        attn = alpha.softmax(-1)  # N, num_heads, L_batch, L_batch\n",
    "        \n",
    "        # aggregate\n",
    "        v_agg = torch.einsum(\"Nhij,Njhe->Nihe\", attn, v)  # (N,L_batch,num_heads,head_dim)\n",
    "        \n",
    "        # reshape to concat the heads (view won't work)\n",
    "        v_agg = v_agg.reshape(-1, L_batch, self.embed_dim) # (N, L_batch, embed_dim)\n",
    "        h_out = self.proj(v_agg)\n",
    "        \n",
    "        return h_out # (N, L_batch, embed_dim)\n",
    "\n",
    "h = torch.randn(3, 462, 32)  # (N, L_batch, embed_dim)\n",
    "ma = MultiHeadMaskedAttention(embed_dim=32, num_heads=4, max_tokens=512, dropout_rate=0.1)\n",
    "h_out = ma(h)  # expect (3, 462, 32)\n",
    "h_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0481c01-b824-47dc-8dd0-097501e8820c",
   "metadata": {},
   "source": [
    "### The Transformer\n",
    "Now that we've tackled multi-head masked attention, the rest is easy. All other operations act at the individual token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8e206-be12-4865-87a5-b422471b6bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, max_tokens, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lay_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.lay_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadMaskedAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads = num_heads,\n",
    "            max_tokens=max_tokens, \n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4*embed_dim),  # the factor of 4 comes from the original GPT paper.\n",
    "            nn.GELU(),  # like relu but a smooth\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.lay_norm1(x + self.attn(x))\n",
    "        z = self.lay_norm2(z + self.feed_forward(z))\n",
    "\n",
    "        return z\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, max_tokens, num_blocks, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.max_tokens = max_tokens\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # embeddings\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_tokens, embed_dim)\n",
    "\n",
    "        # sequence of transformer blocks\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, max_tokens, dropout_rate) \n",
    "            for i in range(num_blocks)])\n",
    "\n",
    "        # output linear layer\n",
    "        self.fout = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = (N, L)\n",
    "        # mask.shape = (N, L)\n",
    "        L = x.shape[-1]\n",
    "        pos = torch.arange(0, L, device=x.device, dtype=torch.long)\n",
    "\n",
    "        # embeddings\n",
    "        tok_embedding = self.tok_embed(x)  # (N, L, embed_dim)\n",
    "        pos_embedding = self.pos_embed(pos)  # (L, embed_dim)\n",
    "        embedding = tok_embedding + pos_embedding  # (N, L, embed_dim)\n",
    "\n",
    "        # transformer blocks\n",
    "        h = self.blocks(embedding)\n",
    "\n",
    "        # output\n",
    "        logits = self.fout(h)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def numpar(self):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b82809-8755-4688-ad1e-a6ca7e601093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's test it\n",
    "dataset = PubMedDataset(\"/project/rcde/datasets/pubmed/mesh_50k/splits/\")\n",
    "dl_train = dataset.get_dataloader('train', batch_size=3)\n",
    "batch = next(iter(dl_train))\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6be8af-170f-410b-b971-4d0a388772ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size = dataset.tokenizer.vocab_size,\n",
    "    embed_dim = 64, \n",
    "    num_heads = 8,\n",
    "    max_tokens = 512, \n",
    "    num_blocks = 3, \n",
    "    dropout_rate = 0.1\n",
    ")\n",
    "print(model)\n",
    "print(\"Trainable parameters: \", model.numpar())\n",
    "model(batch['input_ids']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf42dc20-1cd2-46a9-877b-0cf55c305bef",
   "metadata": {},
   "source": [
    "## Time to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5bc80-df00-496a-a1a0-7bf78374a39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training settings\n",
    "num_epochs=20\n",
    "batch_size=64\n",
    "learning_rate=0.002  # We could get better performance by using a learning rate scheduler\n",
    "\n",
    "# model settings\n",
    "embed_dim = 128 # gpt-1 uses 768. We have a much smaller dataset.\n",
    "num_heads = 4  # gpt uses 12 size 64 heads.\n",
    "max_tokens = 512 # gpt-1 uses 512\n",
    "dropout_rate = 0.2 # gpt-1 uses 0.1\n",
    "num_blocks = 6 # gpt-1 uses 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c952d8-1155-4ab5-b255-b3fc9823507d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reinitialize dataset for good measure\n",
    "dataset = PubMedDataset(\"/project/rcde/datasets/pubmed/mesh_50k/splits/\", max_tokens=max_tokens)\n",
    "vocab_size = dataset.tokenizer.vocab_size\n",
    "\n",
    "# train/test dataloaders\n",
    "dl_train = dataset.get_dataloader('train', batch_size=batch_size, num_workers=20)\n",
    "dl_test = dataset.get_dataloader('test', batch_size=batch_size, num_workers=20)\n",
    "\n",
    "# reinitialize the model on gpu\n",
    "model = Transformer(\n",
    "    vocab_size = dataset.tokenizer.vocab_size,\n",
    "    embed_dim = embed_dim, \n",
    "    num_heads = num_heads,\n",
    "    max_tokens = max_tokens, \n",
    "    num_blocks = num_blocks, \n",
    "    dropout_rate = dropout_rate\n",
    ").to('cuda')\n",
    "\n",
    "print(\"Trainable parameters:\", model.numpar())\n",
    "\n",
    "# create the pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398be384-8306-4556-aa52-7b882aa75108",
   "metadata": {},
   "source": [
    "This is going to take a while, and we only have 45k training samples (63MB) and a tiny model (yes 8.7 million is tiny by today's LLM standards). GPT-3 has about 175 billion parameters and 45 TB of text data. That's 22 thousand times more model and 700 thousand times more data... Be glad we don't have to train that! Nevertheless, the basic architecture is very similar to what we wrote down above. \n",
    "\n",
    "While it trains, try looking at your gpu utilization (for example `nvidia-smi -l 3`) and cpu utilization (`top` or `htop`). Can you identify the bottleneck in the training pipeline? How would we remedy this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49abbc-be99-4540-a915-cded42f96a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"START EPOCH {epoch+1}\")\n",
    "    print(\"TRAINING\")\n",
    "    train(model, dl_train, optimizer, reporting_interval=80)\n",
    "    print(\"TESTING\")\n",
    "    test(model, dl_test, reporting_interval=20)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90adc4c-3734-44e9-8bdc-fef139ac0504",
   "metadata": {},
   "source": [
    "## Generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f9869-7d49-4822-b1a8-1b9e4fd2a664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"We compared the prevalence of\", # organ-specific autoantibodies in a group of Helicobacter...\"\n",
    "    \"We compared the prevalence of\",\n",
    "    \"We compared the prevalence of\",\n",
    "]\n",
    "\n",
    "prompt_ids = dataset.tokenizer(prompts, return_tensors='pt')['input_ids']\n",
    "\n",
    "# trim off unwanted [SEP] tokens which act like our special end-of-sequence token.\n",
    "prompt_ids = prompt_ids[:,:-1]\n",
    "\n",
    "# generate ids\n",
    "gen_ids = generate(model.to('cpu'), prompt_ids, 50)\n",
    "dataset.decode_batch(gen_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a371e-234b-412b-adec-cd24dd1d958d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The pytorch llm workshop was\",\n",
    "    \"The pytorch llm workshop was\",\n",
    "    \"The pytorch llm workshop was\",\n",
    "]\n",
    "\n",
    "prompt_ids = dataset.tokenizer(prompts, return_tensors='pt')['input_ids']\n",
    "\n",
    "# trim off unwanted [SEP] tokens which act like our special end-of-sequence token.\n",
    "prompt_ids = prompt_ids[:,:-1]\n",
    "\n",
    "# generate ids\n",
    "gen_ids = generate(model, prompt_ids, 50)\n",
    "dataset.decode_batch(gen_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AttentionWorkshop",
   "language": "python",
   "name": "attentionworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
