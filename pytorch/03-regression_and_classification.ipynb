{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79133f6c-817f-4c0e-b823-db59d14d472f",
   "metadata": {},
   "source": [
    "# Regression and Classification with Fully Connected Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f847921a-205f-41ee-b214-b4d0ff89eb53",
   "metadata": {},
   "source": [
    "Deep learning is a large, developing field with many sub-communities, a constant stream of new developments, and unlimited application areas. Despite this complexity, most deep learning techniques share a relatively small set of algorithmic building blocks. The purpose of this notebook is to gain familiarity with some of these core concepts. Much of the intuition you develop here is applicable to the neural networks making headline news from the likes of OpenAI. To focus this notebook, we will consider the two main types of supervised machine learning:\n",
    "1. Regression: estimation of continuous quantities\n",
    "2. Classification: estimation of categories or discrete quantities\n",
    "\n",
    "We will create \"deep learning\" models for regression and classification. We will see the power of neural networks as well as the pitfalls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e169d958-1c9c-44de-987b-c90a49ca9a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# libraries we will need\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.distributions import bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178cfcb0-8204-4efe-889e-93c50299d3d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tasks to solve in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6cc8a-80cc-4197-ab1d-796071e635a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d7b76aff-7c4f-4691-ad0f-fe8e11fabf43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make some fake regression data\n",
    "n_samples_reg = 100\n",
    "x_reg = 1.5*torch.randn(n_samples_reg, 1)\n",
    "y_reg = 2 * x_reg + 3.0*torch.sin(3*x_reg) + 1 + 2 * torch.randn(n_samples_reg, 1)\n",
    "y_reg = y_reg.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc9643-7da4-4bce-9550-0092d6aca44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x_reg, y_reg, 'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668c2596-b76e-4f6b-aa9d-e6369d8c27cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c319c99-a519-41b1-b63e-7ed123f0a7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fake classification data\n",
    "n_samples_clf = 200\n",
    "x_clf = 2.5*torch.randn(n_samples_clf, 2)\n",
    "d = torch.sqrt( x_clf[:, 0]**2 +  x_clf[:, 1]**2 )\n",
    "y_clf = (torch.sin(d) > 0) & (d<torch.pi)\n",
    "\n",
    "# swap some labels near the boundary\n",
    "width = 0.7\n",
    "pgivd = 0.3 * width**2 / ((d - torch.pi)**2 + width**2)\n",
    "swaps = bernoulli.Bernoulli(pgivd).sample().type(torch.bool)\n",
    "y_clf[swaps] = ~y_clf[swaps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255e9aa-74e8-4fc7-9556-434cfe567a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x_clf[y_clf, 0], x_clf[y_clf,1], 'o', label='positive')\n",
    "plt.plot(x_clf[~y_clf, 0], x_clf[~y_clf,1], 'o', label='negative')\n",
    "plt.gca().set_aspect(1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ba720-2d66-4027-b15d-674f807ff0a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Simplest \"Deep Learning\" Model: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e9bf1-1e6b-4717-bab5-3e9d6f534f97",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initializing our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66244fda-aab2-4cc5-b9ea-5076426a07cc",
   "metadata": {},
   "source": [
    "Good old fashioned linear model: $y = wx + b$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1730298a-d352-426a-9cb4-a4d879e02bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pytorch's `nn` module has lots of neural network building blocks\n",
    "# nn.Linear is what we need for y=wx+b\n",
    "reg_model = nn.Linear(in_features = 1, out_features = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb68a1e-73a7-40e7-9d7f-352444b7ab1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p>What do you think the arguments `in_features` and `out_features` mean? When might they not be 1?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057e5c5-69cc-4fde-b1d2-473ed3aad0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pytorch randomly initializes the w and b\n",
    "reg_model.weight, reg_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeb4b6-01ed-4e32-bc68-b73e8fef5cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at some predictions\n",
    "with torch.no_grad():\n",
    "    y_reg_init = reg_model(x_reg)\n",
    "    \n",
    "y_reg_init[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8691cf50-675d-46b5-876a-caf2e5afa925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the predictions before training the model\n",
    "plt.plot(x_reg, y_reg, 'o', label='Actual targets')\n",
    "plt.plot(x_reg, y_reg_init, 'o', label='Predicted targets')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "_ = plt.title(\"This model sucks.\\nWe better fit it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8572362-7c2b-4fe1-acd5-bafa43513d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Anatomy of a training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae6fe3-7f23-47c9-a3e1-b973c42b70f2",
   "metadata": {},
   "source": [
    "Basic idea: \n",
    "* Start with random parameter values\n",
    "* Define a loss/cost/objective measure to optimize\n",
    "* Use training data to evaluate the loss\n",
    "* Update the model to reduce the loss (use gradient descent)\n",
    "* Repeat until converged\n",
    "\n",
    "Basic weight update formula: \n",
    "$\n",
    "w_{i+1} = w_i - \\mathrm{lr}\\times \\left.\\frac{\\partial L}{\\partial w}\\right|_{w=w_i}\n",
    "$\n",
    "\n",
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p>Why the minus sign in front of the second term?</p>\n",
    "</div>\n",
    "\n",
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/raw/master/pytorch/fig_grad_descent.png\" alt=\"grad descent\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007792f-c65e-4025-8017-edaac378f5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-initialize our model each time we run this cell\n",
    "# otherwise the model picks up from where it left off\n",
    "reg_model = nn.Linear(in_features = 1, out_features = 1)\n",
    "\n",
    "# the torch `optim` module helps us fit models\n",
    "import torch.optim as optim\n",
    "\n",
    "# create our optimizer object and tell it about the parameters in our model\n",
    "# the optimizer will be responsible for updating these parameters during training\n",
    "optimizer = optim.SGD(reg_model.parameters(), lr=0.01)\n",
    "\n",
    "# how many times to update the model based on the available data\n",
    "num_epochs = 100\n",
    "\n",
    "# setting up a colormap for ordered colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "colors = plt.cm.Greens(np.linspace(0.3, 1, num_epochs // 20))\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # make sure gradients are set to zero on all parameters\n",
    "    # by default, gradients accumulate\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # \"forward pass\"\n",
    "    y_hat = reg_model(x_reg).squeeze()\n",
    "    \n",
    "    # measure the loss\n",
    "    # this is the mean squared error\n",
    "    loss = torch.mean((y_hat - y_reg)**2)\n",
    "    # can use pytorch built-in: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
    "    \n",
    "    # \"backward pass\"\n",
    "    # that is, compute gradient of loss wrt all parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # parameter updates\n",
    "    # use the basic weight update formula given above (with slight modifications)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"Epoch {i+1}. MSE Loss = {loss:0.3f}\")\n",
    "        with torch.no_grad():\n",
    "            y_reg_init = reg_model(x_reg)\n",
    "        \n",
    "        ix = x_reg.argsort(axis=0).squeeze()\n",
    "        plt.plot(x_reg.squeeze()[ix], y_reg_init.squeeze()[ix], '-', color=colors[i // 20], label=i, alpha=1)\n",
    "\n",
    "\n",
    "plt.plot(x_reg, y_reg, 'ok', label=\"Actual data\")\n",
    "plt.grid()\n",
    "_ = plt.legend(title='Epoch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# re-initialize our model each time we run this cell\n",
    "# otherwise the model picks up from where it left off\n",
    "reg_model = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "# create our optimizer object and tell it about the parameters in our model\n",
    "# the optimizer will be responsible for updating these parameters during training\n",
    "optimizer = optim.SGD(reg_model.parameters(), lr=0.01)\n",
    "\n",
    "# how many times to update the model based on the available data\n",
    "num_epochs = 100\n",
    "\n",
    "# setup the figure for animation\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot the initial data points\n",
    "ax.plot(x_reg, y_reg, 'ok', label=\"Actual data\")\n",
    "line, = ax.plot([], [], 'g-', label=\"Regression Line\")\n",
    "epoch_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# function to initialize the animation\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    epoch_text.set_text('')\n",
    "    return line, epoch_text\n",
    "\n",
    "# function to update the plot during each frame of the animation\n",
    "def update(epoch):\n",
    "    # make sure gradients are set to zero on all parameters\n",
    "    # by default, gradients accumulate\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # \"forward pass\"\n",
    "    y_hat = reg_model(x_reg).squeeze()\n",
    "    \n",
    "    # measure the loss\n",
    "    # this is the mean squared error\n",
    "    loss = torch.mean((y_hat - y_reg)**2)\n",
    "    \n",
    "    # \"backward pass\"\n",
    "    # that is, compute gradient of loss wrt all parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # parameter updates\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}. MSE Loss = {loss:0.3f}\")\n",
    "    \n",
    "    # update the regression line\n",
    "    with torch.no_grad():\n",
    "        y_reg_init = reg_model(x_reg)\n",
    "        ix = x_reg.argsort(axis=0).squeeze()\n",
    "        line.set_data(x_reg.squeeze()[ix], y_reg_init.squeeze()[ix])\n",
    "    \n",
    "    # update the epoch text\n",
    "    epoch_text.set_text(f'Epoch: {epoch+1}')\n",
    "    \n",
    "    return line, epoch_text\n",
    "\n",
    "# create the animation\n",
    "ani = FuncAnimation(fig, update, frames=num_epochs, init_func=init, blit=True)\n",
    "\n",
    "# display the animation in Jupyter\n",
    "plt.close(fig)  # prevent static image from displaying\n",
    "HTML(ani.to_jshtml())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154ffa7-3dc9-4b94-92e9-04030a9466d9",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> What shortcomings does this model have?  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb2bef-5ed6-4462-87bf-fefbed501818",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">🔥 IMPORTANT CONCEPT 🔥</p>\n",
    "<p> Model bias is a type of error that occurs when your model doesn't have enough flexibility to represent the real-world data! </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cee99-4645-4a8e-9135-e566538c8aff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3a43c-c10d-4980-aa30-594710f907ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A slight modification to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a107c3-66fc-4112-825a-717bf969cb1c",
   "metadata": {},
   "source": [
    "We need to modify our model because the outputs are true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76db551-ebe3-4f03-a1f9-b4f2b60d946c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_clf.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e281b3-401d-4c35-8b9f-2d477ddbc7d3",
   "metadata": {},
   "source": [
    "While training the model, we cannot simply threshold the model outputs (e.g. call outputs greater than 0 'True'). Instead, we can have our model output a continuous number between 0 and 1. Use sigmoid to transform the output of a linear model to the range (0,1):\n",
    "$$\n",
    "p(y=1 | \\vec{x}) = \\mathrm{sigmoid}(\\vec w \\cdot \\vec x + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fdba60-0320-4a94-9f69-3335362e93b5",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> Why doesn't thresholding work while training?  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80d846-48a8-461c-a045-7a603cc1766a",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">🔥 IMPORTANT CONCEPT 🔥</p>\n",
    "<p> In order to use gradient descent, neural networks must be differentiable.  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68231e56-ae59-4b6e-ae0b-266eb7a2c597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what is sigmoid?\n",
    "x_plt = torch.linspace(-10,10, 100)\n",
    "y_plt = torch.sigmoid(x_plt)\n",
    "plt.axvline(0, color='gray')\n",
    "plt.plot(x_plt, y_plt)\n",
    "plt.grid()\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('output')\n",
    "plt.gcf().set_size_inches(5,2.5)\n",
    "_ = plt.title('Sigmoid Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83ec51",
   "metadata": {},
   "source": [
    "The sigmoid function is defined as:\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a19978fd-5789-422d-a0b5-178e78ac1c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# proposed new model for classification\n",
    "# use nn.Sequential to string operations together\n",
    "model_clf = nn.Sequential(\n",
    "    nn.Linear(2, 1), # notice: we now have two inputs\n",
    "    nn.Sigmoid()  # sigmoid squishes real numbers into (0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d2554-dd7c-4eba-85df-cd33fffe708c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what do the outputs look like?\n",
    "with torch.no_grad():\n",
    "    print(model_clf(x_clf)[:10])\n",
    "# notice how they all lie between 0 and 1\n",
    "    \n",
    "# we loosely interpret these numbers as\n",
    "# \"the probability that y=1 given the provided value of x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d65b00f-64f8-42d4-a128-0de8f25a5e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turning probabilities into \"decisions\"\n",
    "# apply a threshold\n",
    "with torch.no_grad():\n",
    "    print(model_clf(x_clf)[:10] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1adf1-0ef4-4bb6-bc8c-93549dfb1e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's see how this model does before any training\n",
    "\n",
    "# Code to plot the decision regions\n",
    "x_clf.min(), x_clf.max()\n",
    "xx, yy = mg = torch.meshgrid(\n",
    "    torch.linspace(x_clf[:,0].min(), x_clf[:,0].max(), 100),\n",
    "    torch.linspace(x_clf[:,1].min(), x_clf[:,1].max(), 100)\n",
    ")\n",
    "grid_pts = torch.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "with torch.no_grad():\n",
    "    # use a 0.5 decision threshold\n",
    "    grid_preds = model_clf(grid_pts).squeeze()>0.5\n",
    "\n",
    "grid_preds = grid_preds.reshape(xx.shape)\n",
    "\n",
    "plt.gcf().set_size_inches(7,7)\n",
    "plt.plot(x_clf[y_clf, 0], x_clf[y_clf,1], 'o', label='positive')\n",
    "plt.plot(x_clf[~y_clf, 0], x_clf[~y_clf,1], 'o', label='negative')\n",
    "plt.gca().set_aspect(1)\n",
    "plt.grid()\n",
    "plt.contourf(xx, yy, grid_preds, cmap=plt.cm.RdBu, alpha=0.4)\n",
    "plt.legend()\n",
    "_ = plt.title(\"Decision boundary is not good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6996e94-f804-4595-b11b-ebcbbcc15e13",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1d2661-d20b-4baa-afdb-be7a1daf7d96",
   "metadata": {},
   "source": [
    "The main difference between regression and classification is the loss function. For regression, we used mean squared error. For classification, we will use cross-entropy loss. This loss formula encourages the model to output low values for $p(y=1|x)$ when the class is 0 and high values when the class is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ae385-84b5-4cbf-869a-2f9069755914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-initialize our model each time we run this cell\n",
    "# otherwise the model picks up from where it left off\n",
    "model_clf = nn.Sequential(\n",
    "    nn.Linear(2, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# create our optimizer object and tell it about the parameters in our model\n",
    "optimizer = optim.SGD(model_clf.parameters(), lr=0.01)\n",
    "\n",
    "# torch needs the class labels to be zeros and ones, not False/True\n",
    "y_clf_int = y_clf.type(torch.int16)\n",
    "\n",
    "# how many times to update the model based on the available data\n",
    "num_epochs = 1000\n",
    "for i in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_hat = model_clf(x_clf).squeeze()\n",
    "    \n",
    "    # measure the goodness of fit\n",
    "    # need to use a different loss function here\n",
    "    # this is the \"cross entropy loss\"\n",
    "    loss = -y_clf_int * torch.log(y_hat) - (1-y_clf_int)*torch.log(1-y_hat)\n",
    "    loss = loss.mean()\n",
    "    # pytorch built-in: https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html\n",
    "    \n",
    "    # update the model\n",
    "    loss.backward() # gradient computation\n",
    "    optimizer.step()  # weight updates\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i+1}. MSE Loss = {loss:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc4f27-46f7-4d2f-b66f-5b6a449342bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # use a 0.5 decision threshold\n",
    "    grid_preds = model_clf(grid_pts).squeeze()>0.5\n",
    "\n",
    "grid_preds = grid_preds.reshape(xx.shape)\n",
    "\n",
    "plt.gcf().set_size_inches(7,7)\n",
    "plt.plot(x_clf[y_clf, 0], x_clf[y_clf,1], 'o', label='positive')\n",
    "plt.plot(x_clf[~y_clf, 0], x_clf[~y_clf,1], 'o', label='negative')\n",
    "plt.gca().set_aspect(1)\n",
    "plt.grid()\n",
    "plt.contourf(xx, yy, grid_preds, cmap=plt.cm.RdBu, alpha=0.4)\n",
    "plt.legend()\n",
    "_ = plt.title(\"Decision boundary is still not good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd8832-8656-4933-ad40-390f49c538cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p>  Why did this experiment fail? Any ideas what we could do to make it work?  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829cda7-cb59-457d-a232-f1836149269f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Summing up linear models\n",
    "\n",
    "Sometimes linear models can be a good approximation to our data. Sometimes not. \n",
    "\n",
    "Linear models tend to be _biased_ in the statistical sense of the word. That is, they enforce linearity in the form of linear regression lines or linear decision boundaries. The linear model will fail to the degree that the real-world data is non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b61a4a-ae5b-450c-9ff6-e469a5edf92a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting non-linear with fully-connected neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db7689-9bc5-44df-9fb3-d796d7f2546a",
   "metadata": {},
   "source": [
    "We can compose simple math operations together to represent very complex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e98501-d552-46ee-9fa2-369a244614b6",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/clemsonciti/rcde_workshops/raw/master/pytorch/fig_ann.png\" alt=\"grad descent\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7de1e-994c-4987-8351-966a4ed51140",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d864637-e344-4b85-a037-8c2009c162a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_test_regression_model(model, num_epochs=1000, reporting_interval=100):\n",
    "\n",
    "    # create our optimizer object and tell it about the parameters in our model\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # how many times to update the model based on the available data\n",
    "    for i in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(x_reg).squeeze()\n",
    "\n",
    "        # measure the goodness of fit\n",
    "        loss = torch.mean((y_hat - y_reg)**2)\n",
    "\n",
    "        # update the model\n",
    "        loss.backward() # gradient computation\n",
    "        optimizer.step()  # weight updates\n",
    "\n",
    "        if i % reporting_interval == 0:\n",
    "            print(f\"Epoch {i+1}. MSE Loss = {loss:0.3f}\")\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        y_reg_init = model(x_reg)\n",
    "    plt.plot(x_reg.cpu(), y_reg.cpu(), 'o', label='Actual targets')\n",
    "    plt.plot(x_reg.cpu(), y_reg_init.cpu(), 'x', label='Predicted targets')\n",
    "    plt.grid()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "25b36cc0-0bae-4d5f-aa46-c251413b5d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_test_classification_model(model, num_epochs=1000, reporting_interval=100):\n",
    "\n",
    "    # create our optimizer object and tell it about the parameters in our model\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # how many times to update the model based on the available data\n",
    "    for i in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(x_clf).squeeze()\n",
    "\n",
    "        # measure the goodness of fit\n",
    "        # need to use a different loss function here\n",
    "        loss = -y_clf_int * torch.log(y_hat) - (1-y_clf_int)*torch.log(1-y_hat)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # update the model\n",
    "        loss.backward() # gradient computation\n",
    "        optimizer.step()  # weight updates\n",
    "    \n",
    "        if i % reporting_interval == 0:\n",
    "            print(f\"Epoch {i+1}. MSE Loss = {loss:0.3f}\")   \n",
    "            \n",
    "    with torch.no_grad():\n",
    "        grid_preds = model(grid_pts).squeeze()>0.5\n",
    "\n",
    "    grid_preds = grid_preds.reshape(xx.shape)\n",
    "\n",
    "    plt.gcf().set_size_inches(7,7)\n",
    "    plt.plot(x_clf[y_clf, 0], x_clf[y_clf,1], 'o', label='positive')\n",
    "    plt.plot(x_clf[~y_clf, 0], x_clf[~y_clf,1], 'o', label='negative')\n",
    "    plt.gca().set_aspect(1)\n",
    "    plt.grid()\n",
    "    plt.contourf(xx, yy, grid_preds, cmap=plt.cm.RdBu, alpha=0.4)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18615604-59bd-4740-8c29-4874aee9f4d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2878156-fbfa-4e0c-addb-f6fd9ffcc32c",
   "metadata": {},
   "source": [
    "What about nested linear models? Might this do something interesting? \n",
    "$$\n",
    "y = w_1(w_0 x + b_0) + b_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b84f19-c5ae-4540-8783-3fc575955258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can build more complex models by stringing together more operations inside nn.Sequential:\n",
    "model =  nn.Sequential(\n",
    "    nn.Linear(in_features = 1, out_features = 1),\n",
    "    nn.Linear(1,1)\n",
    ")\n",
    "\n",
    "train_and_test_regression_model(model)\n",
    "_ = plt.title(\"But wait, that's still just a line?!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94312d3b-fddc-4bbc-ba00-407d84ecbdf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> Why is it still just a line? How can we fix it?  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293b590c-43f2-49d4-a5f4-0b394250169f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need a non-linearity between the two linear operations\n",
    "model =  nn.Sequential(\n",
    "    nn.Linear(in_features = 1, out_features = 1),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1,1)\n",
    ")\n",
    "# this model does not reduce to a linear operation\n",
    "\n",
    "train_and_test_regression_model(model)\n",
    "_ = plt.title(\"Non-linearity! But not enough.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa3e43-5744-4a28-a5d0-cad9ffc43b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's really crank up the number of hidden nodes\n",
    "model =  nn.Sequential(\n",
    "    nn.Linear(in_features = 1, out_features = 30),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(30,30),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(30,30),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(30,30),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(30,30),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(30,30),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(30,1)\n",
    ")\n",
    "\n",
    "train_and_test_regression_model(model, num_epochs=50000, reporting_interval=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f759fc-91d1-4841-93fd-be48a3b0896d",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> What is wrong with this picture?  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91c429-2728-43dc-90c9-343c13c0be95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_grid = torch.linspace(x_reg.min(), x_reg.max(), 200)[:,None]\n",
    "    y_reg_grid = model(x_grid)\n",
    "    y_reg_init = model(x_reg)\n",
    "plt.plot(x_reg.cpu(), y_reg.cpu(), 'o', label='Actual targets')\n",
    "plt.plot(x_reg.cpu(), y_reg_init.cpu(), 'x', label='Predicted targets')\n",
    "plt.plot(x_grid.cpu(), y_reg_grid.cpu(), 'k-', label='Predicted')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f036bd-c5be-47c1-a494-c4dc1252413a",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> Where is overfitting most severe? How might having more data alleviate the problem?  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f90cb0-0a58-4b20-bb04-3359f4c2cac2",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">🔥 IMPORTANT CONCEPT 🔥</p>\n",
    "<p> Overfitting is a type of error that occurs when your model memorizes the training samples and fails to generalize to unseen data. This usually ocurrs when the model has excess capacity.  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1aad6f-b05b-4c5f-bc0b-bab70733d19a",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">😅 EXCERCISE 😅</p>\n",
    "<p> Using the code cell below, design and test a model that gets it \"just right\". </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "82b27829-0696-49c0-905b-28321ff839d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = your model goes here\n",
    "\n",
    "# train_and_test_regression_model(model, num_epochs=10000, reporting_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e86f7c-fe45-4e00-b0ec-58778526a338",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8f736b-c741-49a7-90bf-0e739783e664",
   "metadata": {},
   "source": [
    "Let's apply the same non-linear treatment to the case of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a351ba9-6b6e-4026-b920-933211533808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "    nn.Linear(in_features = 2, out_features = 2),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "train_and_test_classification_model(model, 10000, 1000)\n",
    "_ = plt.title(\"Not non-linear enough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94380290-ab95-4f8f-8e74-795ec789f696",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> What would overfitting look like in this type of graph?  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49889624-0c42-4c71-aed5-dc765ea3070d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "    nn.Linear(in_features = 2, out_features = 50),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(50,50),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(50,50),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(50,50),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(50,50),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(50,50),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(50,1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "train_and_test_classification_model(model, 30000, 3000)\n",
    "_ = plt.title(\"What went wrong here?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c53f2-c0c1-4d41-b14c-47293fa8e3ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> What's wrong with this picture? </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4f529-4de8-425c-9942-b0970994794a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">😅 EXCERCISE 😅</p>\n",
    "<p> Using the code cell below, design and test a model that gets it \"just right\". </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6a0c86c-2c7b-4bef-b3a4-d00237e6a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = your model goes here\n",
    "\n",
    "# train_and_test_classification_model(model, 20000, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5b287a-98fe-40ba-90c8-2c7583fb59c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Summing up fully connected neural networks\n",
    "* Fully connected neural networks can represent highly non-linear functions\n",
    "* We can learn good functions through gradient descent\n",
    "* Overfitting is a big problem\n",
    "\n",
    "These concepts apply to nearly any neural network trained with gradient descent from the smallest fully connected net to GPT-4. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Workshop",
   "language": "python",
   "name": "pytorchworkshop"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
