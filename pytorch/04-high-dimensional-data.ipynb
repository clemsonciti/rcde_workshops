{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8557333-2a13-496e-8bb0-9d396376aa45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# High Dimensional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52645648-1e24-40fd-933b-09380b1ecf68",
   "metadata": {},
   "source": [
    "For high-dimensional data, it is not possible to get a dense sampling of the data space. Consider the following analogy. Imagine we are measuring the temperatures of spaces with growing numbers of dimensions. We have sensors that measure reliably within a \"volume\" 1mm in extent. Outside of that volume, the temperature may change, so we need lots of sensors to map the temperature through the space. \n",
    "* 1D space: The total space is 1 m. You are given 100 sensors; each 1mm. The most you could fill up is 1/10th of the space\n",
    "* 2D space: The total space is 1 m^2. You are given 100 sensors; each 1mm^2. The most you could fill up is 100mm^2 / (1000 mm)^2 = 1/10000th of the space. To cover 1/10th of the space, you would need 100,000 sensors!\n",
    "* 3D space: The total space is 1 m^3. You are given 100 sensors; each 1mm^3. The most you could fill up is 100 mm^3 / (1000 mm)^3 = 1/10^7th of the space. To cover 1/10th of the space, you would need 100,000,000 sensors! \n",
    "\n",
    "You see where this is going. This is called THE CURSE OF DIMENSIONALITY.\n",
    "\n",
    "In deep learning, we often work in data spaces with hundreds or thousands of dimensions, so... are we out of luck?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e169d958-1c9c-44de-987b-c90a49ca9a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# libraries we will need\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.distributions import bernoulli\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c87626-44a5-4f12-a271-a543e84e1ff7",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">ðŸ”¥ IMPORTANT CONCEPT ðŸ”¥</p>\n",
    "<p> The CURSE OF DIMENSIONALITY refers to the fact that in high dimensional space, it is impossible to get dense coverage of the data space. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a72193-5355-448e-9c69-1d5ac707e2f0",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> Think back to the overfit regression model in the previous notebook. How might the curse of dimensionality make the overfitting problem worse? Why are deep neural networks especially problematic in this regard?  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29e2f0-c657-484c-89da-b1573d5e9abb",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> How might THE CURSE OF DIMENSIONALITY pose a problem for real-world tasks like computer vision for autonomous driving? </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0385a628-89cd-4007-8061-869291cc4573",
   "metadata": {},
   "source": [
    "## Fake high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a4aec681-9774-4914-8fd1-cde23b2aa44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6561ae7a-20a0-4f2d-9d41-475af3cd8ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this dataset is a lot bigger, so we're going to switch over to gpu\n",
    "# pytorch makes this easy\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9dc65-44aa-45f6-8777-a5aad8f83f72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make some non-linear, fake data\n",
    "n_samples = 100000\n",
    "n_test = 20000\n",
    "n_dims = 1000\n",
    "\n",
    "w1 = torch.randn(n_dims, 128)\n",
    "w2 = torch.randn(128)\n",
    "b1 = torch.randn(128)\n",
    "b2 = torch.randn(1)\n",
    "noise = 1 * torch.randn(n_samples)\n",
    "\n",
    "X = torch.randn(n_samples, n_dims)\n",
    "y = torch.tanh(X @ w1 + b1) @ w2 + b2 + noise\n",
    "\n",
    "# move to gpu:\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cfece6-5a99-40b8-9e7d-ec90a1799bf3",
   "metadata": {},
   "source": [
    "## Splitting into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915029d-51b9-4ae0-aff7-a344ef5e573a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train/test split\n",
    "X_train, X_test = X[:-n_test], X[-n_test:]\n",
    "y_train, y_test = y[:-n_test], y[-n_test:]\n",
    "\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04cd5d3-3cd6-4042-94b3-b3026edbbe95",
   "metadata": {},
   "source": [
    "## Fitting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b36618a7-f861-4263-9b7f-13f04fda822f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function for training/evaluation\n",
    "def train_and_test_highdim(model, num_epochs=1000, reporting_interval=100):\n",
    "\n",
    "    # create our optimizer object and tell it about the parameters in our model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # how many times to update the model based on the available data\n",
    "    epoch_ls = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model(X_train).squeeze()\n",
    "        loss = torch.mean((y_hat - y_train)**2)\n",
    "\n",
    "        # update the model\n",
    "        loss.backward() # gradient computation\n",
    "        optimizer.step()  # weight updates\n",
    "\n",
    "        if i % reporting_interval == 0:\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            # turn off gradient tracking for this step:\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(X_test).squeeze()\n",
    "                test_loss = torch.mean((y_hat - y_test)**2)\n",
    "                \n",
    "            print(f\"Epoch {str(i+1).zfill(5)}\", end='\\r')\n",
    "            \n",
    "            epoch_ls.append(i)\n",
    "            train_losses.append(loss.detach().item())\n",
    "            test_losses.append(test_loss.item())\n",
    "            \n",
    "    plt.plot(epoch_ls, train_losses, label='train')\n",
    "    plt.plot(epoch_ls, test_losses, label='test')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab2bd4-b83d-49ae-8848-e46af59edb66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This architecture is intentionally designed to overfit\n",
    "model =  nn.Sequential(\n",
    "    nn.Linear(in_features = n_dims, out_features = 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128,128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128,64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64,50),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(50,1)\n",
    ")\n",
    "\n",
    "# move model to gpu:\n",
    "model = model.to(device)\n",
    "\n",
    "train_and_test_highdim(model, 2000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69673f02-af2f-4ce9-b7a5-287fdef4c6f2",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> Why does the plot look like that? </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273f81c-2d09-4922-aa3e-a5fa026ad904",
   "metadata": {},
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">Question</p>\n",
    "<p> Does this plot suggest a way that we could mitigate overfitting? </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dcb2e9-c830-4939-9a71-9e1415a3d050",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"admonition-main-idea admonition\">\n",
    "<p class=\"admonition-title\">ðŸ˜… EXCERCISE ðŸ˜…</p>\n",
    "<p> Using the code cell below, designed and test a biased model. How do the training curves differ? </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "33f6bfea-9935-4c48-8ddd-8b1a7772c1d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = your biased model goes here\n",
    "\n",
    "#model = model.to(device)\n",
    "\n",
    "# train_and_test_highdim(model, 2000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a1020-07eb-4733-87be-476ae035d33b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summing up high dimensional data\n",
    "* Overfitting problems become much worse for high-dimensional data due to the curse of dimensionality\n",
    "* To test for overfitting, it is critical to evaluate your model on different data than you used to train your model\n",
    "* Much of the technical progress in deep learning over the last decade can be viewed as better ways to prevent overfitting.\n",
    "\n",
    "The significance of the challenge of overfitting for deep learning cannot be overstated!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch Workshop",
   "language": "python",
   "name": "pytorchworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
