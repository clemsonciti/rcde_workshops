{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n40xD862nhd"
   },
   "source": [
    "# Computer Vision and Convolutional Neural Networks\n",
    "\n",
    "In 2012, Alex Krizhevsky et al. competed in the [ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575). Their solution, based on a deep _Convolutional Neural Network_ (CNN) beat the runner up by more than 10% ([their paper](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)). Prior to this result, many thought that it was infeasable to train such deep models due to the large number parameters that need to be optimized and the associated high computational cost. Krizhevsky et al. were among the first to user GPUs to speed things up. This landmark result kicked off a wave of innovation that continues to this day. \n",
    "\n",
    "Convolutional Neural Networks are named thus because of the _convolution_ operation which plays a primary role in their design. You can take a look at [these slides](https://docs.google.com/presentation/d/17Uygrr1g6qBMpXOwcfZf584PkW2LTZoXgX1mqsK6syE/edit?usp=sharing) to get a sense for what the convolution operator does. Convolution is a great example of building _inductive biases_ into the network architecture. In this case, the inductive priors are _locality_ and _translation invariance_. These assumptions lead to weight sharing which significantly improves the parameter efficiency of our model relative to the fully connected neural networks that we've seen so far. \n",
    "\n",
    "In this notebook, we apply CNNs to the classification of handwritten characters (letters and numbers) using the [EMNIST dataset](https://www.westernsydney.edu.au/icns/reproducible_research/publication_support_materials/emnist).  The EMNIST dataset has the digits 0 through 9 and a total of 37 upper and lower-case latin characters (see Figure) for a total of 47 classes. The latin characters make EMNIST a more difficult classification task than the more famous MNIST dataset which only contains digits. We will work through a variety of architecture design considerations, will train the model for each case, and learn to reason about how architecture choices impact the performance characteristics of the model.\n",
    "\n",
    "![Balanced EMNIST Dataset](https://www.researchgate.net/publication/329391937/figure/fig3/AS:700515201605633@1544027247072/Visualization-of-EMNIST-balanced-dataset-10-Fig2.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NGHwURr8or8"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tjB9tDRT2DLi"
   },
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "# set random seed for reproducibility\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about Palmetto storage, see the [onboarding training](https://docs.rcd.clemson.edu/palmetto/onboarding/) and the documentation for [Palmetto storage](https://docs.rcd.clemson.edu/palmetto/storage/store/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yeE9oK-h9Jjw",
    "outputId": "b92fa5d1-86a2-4be6-c5eb-b5b9d99e2175"
   },
   "outputs": [],
   "source": [
    "# save locations\n",
    "# best not to use your home directory for reading/writing large temporary files\n",
    "# /scratch is much faster!\n",
    "data_dir = f\"/scratch/{os.environ['USER']}/data\"\n",
    "model_path = f\"/scratch/{os.environ['USER']}/model.pt\"\n",
    "\n",
    "# Model and Training\n",
    "batch_size=64 #input batch size for training (default: 64)\n",
    "test_batch_size=1000 #input batch size for testing (default: 1000)\n",
    "num_workers=9 # parallel data loading to speed things up\n",
    "lr=1.0 # learning rate \n",
    "gamma=0.7 # Learning rate step gamma (default: 0.7)\n",
    "no_cuda=False #disables CUDA training (default: False)\n",
    "seed=42 #random seed (default: 42)\n",
    "log_interval=10 #how many batches to wait before logging training status (default: 10)\n",
    "save_model=False #save the trained model (default: False)\n",
    "\n",
    "# additional derived settings\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "a594b0a61f9c471f927d1e84bb6b660e",
      "7907db13bf594b6188ee1321502e01f3",
      "430518b6f83343e0b5e22d32b348d605",
      "4b3178c079fb4eb3800e3b8311caf412",
      "21179021fdd249dca874a2cb3215d3e9",
      "3f00f27414ed4d80ace97ee0bcea32d6",
      "87f7464c25c44cde8088e97126f4190e",
      "b765f713ef5440ea85c5f12540ebba75",
      "674919a7e1d14f738a6d70ad66a89f4b",
      "60c4d3e66e2146cdb1ed443d133ae9ec",
      "c7cb437d14a24ecd9551c8f9516577c8"
     ]
    },
    "id": "klIYys9h8sUN",
    "outputId": "7a98a546-4d78-45d1-f711-09cf658b7955"
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "# Pytorch provides a number of pre-defined dataset classes\n",
    "# EMNIST is one of them! Pytorch will automatically download the data.\n",
    "# It will only download if the data is not already present.\n",
    "data_train = datasets.EMNIST(data_dir, split='balanced', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                 ]))\n",
    "\n",
    "data_test = datasets.EMNIST(data_dir, split='balanced', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ]))\n",
    "\n",
    "# define pytorch dataloaders for training and testing\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(data_test, batch_size=test_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# save a test batch for later testing\n",
    "image_gen = iter(test_loader)\n",
    "test_img, test_trg = next(image_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJ0iyHFj-LUG",
    "outputId": "24d1bbaf-8f0d-450a-ff2b-df553af2f2e5"
   },
   "outputs": [],
   "source": [
    "print(\"Training dataset:\", train_loader.dataset)\n",
    "print(\"Testing dataset:\", test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "fBOgyUZwUVIB",
    "outputId": "642cb012-b018-4cd2-e517-f9a16c17db1e"
   },
   "outputs": [],
   "source": [
    "# Look at some examples\n",
    "n_images = 16\n",
    "rand_imgs = next(iter(test_loader))\n",
    "ints = torch.randint(0,len(rand_imgs[0]), (n_images,))\n",
    "\n",
    "fig, ax_arr = plt.subplots(4, 4)\n",
    "ax_arr = ax_arr.flatten()\n",
    "\n",
    "for n, ix in enumerate(ints):\n",
    "    img = rand_imgs[0][ix]\n",
    "    ax_arr[n].imshow(img[0].detach().cpu().numpy().T, cmap='Greys')\n",
    "    ax_arr[n].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AR0lyfnEuPsw"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Question**: In addition to the larger number of classes in EMNIST compared to MNIST, why might it be more difficult to accurately classify images from the EMNIST set?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7g4s_TT-3ZeF"
   },
   "outputs": [],
   "source": [
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('\\r\\tTrain epoch {}: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()), end='')\n",
    "            \n",
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\rTest epoch {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        epoch,\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "def train_and_test(model, save_name=model_path, epochs=5):\n",
    "    # Train the linear model\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader, epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "\n",
    "def get_n_params(model): return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfeJK7_EF0Jf"
   },
   "source": [
    "# Classification architectures\n",
    "We will explore a range of classification architectures starting with a linear classifier and scaling up to a several-layer convolutional neural network. Each architecture can be described in two steps (see figure below):\n",
    "1. Feature extraction -- converting the raw pixel information into a _feature vector_\n",
    "2. Classification -- converting the feature vector into predictions about the character in the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy7U5jeCFmRN"
   },
   "source": [
    "![CNN diagram](https://miro.medium.com/max/1400/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9AIybygGyud"
   },
   "source": [
    "Most of the secret sauce of CNNs takes place in the feature extraction step. We will define a generic classification architecture that takes as input the sequence of operations used for feature extraction. We will use progressively more complex feature extraction pipelines. For the classification step, we will always use a single linear layer, but the number of inputs will depend on the size of the feature vector produced by the feature extractor. \n",
    "\n",
    "The class defined below implements this generic classifier class in pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XU9unsjPHGWe"
   },
   "outputs": [],
   "source": [
    "# Define the generic EMNIST classification architecture\n",
    "# The `feature_extractor` argument takes images as input and produces feature vectors.\n",
    "# These vectors can be of any length. \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        # feature encoder\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        # classifier\n",
    "        # LazyLinear allows input size to be inferred automatically \n",
    "        self.classifier = nn.LazyLinear(47) # 47 classes in EMNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x) # Flatten -> x_i\n",
    "        x = self.classifier(x) # Matrix multiply -> c_m^0 + sum(W_mi*x_i)\n",
    "        x = F.softmax(x, dim=1) # apply the softmax function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EectRKIank05"
   },
   "source": [
    "# Linear Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9-aG-j7Hm81"
   },
   "source": [
    "A linear classifier uses the original pixels as the features for classification. We simply need to unravel the 2D image into a vector. This is accomplished using `nn.Flatten()`. \n",
    "\n",
    "Note: you can safely ignore the message `UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
    "  warnings.warn('Lazy modules are a new feature under heavy development`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QaTveymj-iF4",
    "outputId": "ea4dc50a-d213-49e1-822b-975d52be6e43"
   },
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define the feature extractor\n",
    "# for linear model, we just flatten the original image into a vector\n",
    "feature_extractor = nn.Flatten()\n",
    "\n",
    "# Create the model\n",
    "model = Classifier(feature_extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "8vSFTMcbH-cI",
    "outputId": "6bfb0ed5-85b1-457a-ab5d-d29712188822"
   },
   "outputs": [],
   "source": [
    "# Display model and number of parameters \n",
    "# Run one batch through the model to initialize the lazy linear layer\n",
    "# this is necessary to get accurate parameter counts\n",
    "with torch.no_grad():\n",
    "    model(test_img.to(device)) \n",
    "\n",
    "display(model)\n",
    "\n",
    "print(\"Number of parameters in linear model:\", get_n_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D9tp1GYxAvJ"
   },
   "source": [
    "## Train and test the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZONf9JTx7kAL",
    "outputId": "581e8034-772f-4e89-c557-90c2c5407609"
   },
   "outputs": [],
   "source": [
    "train_and_test(model)\n",
    "\n",
    "# by default, `train_and_test` trains for 5 epochs\n",
    "# you can adjust this using the epochs argument, like\n",
    "# train_and_test(model, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ci5OzdbdozRI"
   },
   "source": [
    "---\n",
    "\n",
    "**Question**: Think of a naive performance baseline for this task. How well does it compare with that performance? Do you find this suprising given that this is just a linear model?  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-uG4JFzoM8p"
   },
   "source": [
    "# CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYRtyOHnM8TF"
   },
   "source": [
    "### One convolution layer\n",
    "We now make a more interesting feature extractor using convolution. The feature extractor has the following steps: \n",
    "1. _convolution_ (`nn.Conv2d`) over the input image.\n",
    "2. _batch normalization_ (`nn.BatchNorm2d`) rescales the feature maps. This helps the network converge more quickly. You can see the definition [here](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html).\n",
    "3. _max pooling_ (`nn.MaxPool2d`) takes the maximum pixel from each 2x2 block resulting in an image that has half the size in each dimension. \n",
    "4. _ReLU_ (`nn.ReLU`) zeroes out negative values in the feature maps. See figure:\n",
    "\n",
    "![ReLU](https://www.researchgate.net/profile/Hossam-H-Sultan/publication/333411007/figure/fig7/AS:766785846525952@1559827400204/ReLU-activation-function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SG3nRRxHtbF"
   },
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define the feature extractor\n",
    "feature_extractor = nn.Sequential(\n",
    "    # convolution block:\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "    nn.LazyBatchNorm2d(),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # flatten just as with the linear classifier\n",
    "    nn.Flatten()\n",
    ")\n",
    "# Create the model\n",
    "model = Classifier(feature_extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "Cz8bql81KXSQ",
    "outputId": "7cc6cc2b-4681-4184-94b2-ea182c03bc32"
   },
   "outputs": [],
   "source": [
    "# @title Display model and number of parameters \n",
    "# Run one batch through the model to initialize the lazy linear layer\n",
    "# this is necessary to get accurate parameter counts\n",
    "with torch.no_grad():\n",
    "    model(test_img.to(device)) \n",
    "\n",
    "display(model)\n",
    "\n",
    "print(\"Number of parameters in linear model:\", get_n_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfdS6w6vog1V"
   },
   "source": [
    "(Less than 36,895 for linear model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "**Question**: Why does the CNN have fewer parameters than the linear model?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZYspFPRrI2t",
    "outputId": "eca388b3-8e4d-45de-fa1d-a3118d232fb9"
   },
   "outputs": [],
   "source": [
    "train_and_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaNhkI-nMg-4"
   },
   "source": [
    "---\n",
    "\n",
    "**Question**: Why does the CNN beat the linear model after 5 epochs despite having fewer parameters?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature maps:** let's look at the internal representations of the images produced by the 4 convolution kernel. In the picture below, the colored grids on the left are representations of the convolution kernel matrices. The values of these kernel matrices were \"learned\" during the training of the network. The image on the right is the feature map produced by convolving the kernel matrix with the original image. Bright areas correspond to high numerical values; dark areas correspond to low numerical values. Rerun the cell to see the feature maps for different images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681
    },
    "id": "xHvF95xthS0Y",
    "outputId": "908fbc8e-f4c0-497c-ff34-badfd22e0ab0"
   },
   "outputs": [],
   "source": [
    "imgs, labels = next(image_gen)\n",
    "test_img_plot = imgs[6,0]\n",
    "\n",
    "kern = model.feature_extractor[0]\n",
    "\n",
    "# Generate feature maps\n",
    "fmaps_1 = kern(test_img_plot[None, None].cuda()).detach().cpu().numpy()[0]\n",
    "for K, fmap in zip(kern.weight, fmaps_1):\n",
    "    # plot kernel\n",
    "    plt.subplot(1,2,1)\n",
    "    mat = K.clone().detach().cpu().numpy()[0]\n",
    "    plt.imshow(mat.T)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # plot feature map\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(fmap.T, cmap='Greys')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXtrbRCL9_Sd"
   },
   "source": [
    "# A \"wider\" model\n",
    "The \"width\" of a CNN refers to how many feature maps are used in the convolution layers. Wide layers have many convolution kernels leading to many feature maps. In the model above, we had 4 convolution kernels (see `out_channels=4` in the `nn.Conv2d` layer in the feature extractor). Interpreting these kernels as feature extractors, it's plausible that we could improve the performance of the model by increasing the number of kernels.\n",
    "\n",
    "The network below is the same as the previous one except that it uses twice as many kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3vuPy-u_vzI"
   },
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define the feature extractor\n",
    "feature_extractor = nn.Sequential(\n",
    "    # convolution block:\n",
    "    nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, bias=False),\n",
    "    nn.LazyBatchNorm2d(),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # flatten just as with the linear classifier\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "model = Classifier(feature_extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "P8XHRva5AIkF",
    "outputId": "f9369081-fec2-4f33-e5f1-4ce9f12218b0"
   },
   "outputs": [],
   "source": [
    "# @title Display model and number of parameters \n",
    "# Run one batch through the model to initialize the lazy linear layer\n",
    "# this is necessary to get accurate parameter counts\n",
    "with torch.no_grad():\n",
    "    model(test_img.to(device)) \n",
    "\n",
    "display(model)\n",
    "\n",
    "print(\"Number of parameters in linear model:\", get_n_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjhbsKpIBn3c"
   },
   "source": [
    "Notice that doubling the number of channels nearly doubled the number of free parameters in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6Vp4OjJAMb6",
    "outputId": "2870ab48-a87d-42d2-8e9b-ad98cdbe28b9"
   },
   "outputs": [],
   "source": [
    "train_and_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9Mv_jrGASFu"
   },
   "source": [
    "---\n",
    "\n",
    "**Question**: How does the performance with 8 kernels compare to the performance with 4 kernels? Why do you think this might be the case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IueniS6Gyrx"
   },
   "source": [
    "# A \"deeper\" model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80REJwRrhvV4"
   },
   "source": [
    "The \"depth\" of a CNN refers to how many convolution layers are used in the network. The feature maps from one convolution step are used as the input for the next convolution. The previous network had a single convolution step. We will now try adding more convolution layers and see how it effects performance. To isolate the effect of having more layers, we will hold the width at 4 kernels like our first CNN.\n",
    "\n",
    "---\n",
    "\n",
    "**Question**: Why might it be helpful to add more convolutional layers to the CNN? Can you contrast the effect of adding more depth to the effect of greater width?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fxqecWJM_2U"
   },
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define the encoder\n",
    "# we repeat the convolution block several times\n",
    "feature_extractor = nn.Sequential(\n",
    "    # block 1\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "    nn.LazyBatchNorm2d(),\n",
    "    nn.ReLU(),\n",
    "    # 2\n",
    "    nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "    nn.LazyBatchNorm2d(),\n",
    "    nn.ReLU(),\n",
    "    # 3\n",
    "    nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "    nn.LazyBatchNorm2d(),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # flatten just as with the linear classifier\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "model = Classifier(feature_extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "bitGRh8qNX8p",
    "outputId": "bd7e9dfe-3863-49f0-ed02-d3ef59786997"
   },
   "outputs": [],
   "source": [
    "# @title Display model and number of parameters \n",
    "# Run one batch through the model to initialize the lazy linear layer\n",
    "# this is necessary to get accurate parameter counts\n",
    "with torch.no_grad():\n",
    "    model(test_img.to(device)) \n",
    "\n",
    "display(model)\n",
    "\n",
    "print(\"Number of parameters in linear model:\", get_n_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWZzzoW1B5l-"
   },
   "source": [
    "---\n",
    "\n",
    "**Question**: The number of parameters is actually _less_ than our single layer model. We added lots of convolution kernels, so how can that be?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5pc2RTi9UfX",
    "outputId": "5b3f17e2-ff65-41e7-96de-f04d39df8f73"
   },
   "outputs": [],
   "source": [
    "train_and_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSd76lvkOKXa"
   },
   "source": [
    "---\n",
    "\n",
    "**Question**: You should find that the deeper model performed similarly to the wider model despite having many fewer parameters. Why do you think increasing the depth of the model was more parameter efficient than increasing the width?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu9yXu8cC8Uv"
   },
   "source": [
    "# Your turn! \n",
    "Modify the feature extractor below and see how good of a score you can obtain on EMNIST. Some ideas: \n",
    "* Change the number of channels in the convolution layers. Note: the `in_channels` argument to `nn.Conv2d` must match the `out_channels` argument from a previous layer. The input image has 1 channel since the images are grayscale.\n",
    "* Change the `kernel_size`, which is the size of the kernel matrix used in convolution. \n",
    "* Change the number of convolution blocks in the network.\n",
    "* Try [different activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).\n",
    "* Add or remove max pooling layers between convolution blocks\n",
    "* Use a greater number of epochs in the `train_and_test` function\n",
    "* Advanced: can you figure out how to add \"residual\"/\"skip\" connections? See [this paper](https://arxiv.org/abs/1512.03385). \n",
    "\n",
    "There are many, many possible combinations that you could try, and you can't explore them all. Try to think of a few experiments to try and record what you learn. If you want to go down the rabbit hole of trying many things, feel free, but that's not expected for this assignment.\n",
    "\n",
    "Try to beat my score of ~80% resulting from using the \"deeper model\" above trained for 13 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d944NmW_C-wh"
   },
   "outputs": [],
   "source": [
    "# set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# define the encoder\n",
    "# we repeat the convolution block several times\n",
    "feature_extractor = nn.Sequential(\n",
    "    # block 1\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "    nn.LazyBatchNorm2d(),\n",
    "    nn.ReLU(),\n",
    "    # 2\n",
    "    nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "    nn.LazyBatchNorm2d(),\n",
    "    nn.ReLU(),\n",
    "    # 3\n",
    "    nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "    nn.LazyBatchNorm2d(),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    # flatten just as with the linear classifier\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "# Create the model\n",
    "model = Classifier(feature_extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2g5zHcroEqra",
    "outputId": "c420f942-c41c-4422-fa16-98dcf95f11f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_and_test(model, epochs=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "PytorchWorkshop",
   "language": "python",
   "name": "pytorchworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21179021fdd249dca874a2cb3215d3e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f00f27414ed4d80ace97ee0bcea32d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "430518b6f83343e0b5e22d32b348d605": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b765f713ef5440ea85c5f12540ebba75",
      "max": 561753746,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_674919a7e1d14f738a6d70ad66a89f4b",
      "value": 561753746
     }
    },
    "4b3178c079fb4eb3800e3b8311caf412": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60c4d3e66e2146cdb1ed443d133ae9ec",
      "placeholder": "​",
      "style": "IPY_MODEL_c7cb437d14a24ecd9551c8f9516577c8",
      "value": " 561753746/561753746 [00:37&lt;00:00, 15679465.54it/s]"
     }
    },
    "60c4d3e66e2146cdb1ed443d133ae9ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "674919a7e1d14f738a6d70ad66a89f4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7907db13bf594b6188ee1321502e01f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f00f27414ed4d80ace97ee0bcea32d6",
      "placeholder": "​",
      "style": "IPY_MODEL_87f7464c25c44cde8088e97126f4190e",
      "value": "100%"
     }
    },
    "87f7464c25c44cde8088e97126f4190e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a594b0a61f9c471f927d1e84bb6b660e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7907db13bf594b6188ee1321502e01f3",
       "IPY_MODEL_430518b6f83343e0b5e22d32b348d605",
       "IPY_MODEL_4b3178c079fb4eb3800e3b8311caf412"
      ],
      "layout": "IPY_MODEL_21179021fdd249dca874a2cb3215d3e9"
     }
    },
    "b765f713ef5440ea85c5f12540ebba75": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7cb437d14a24ecd9551c8f9516577c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
